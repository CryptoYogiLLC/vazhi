{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Test Existing VAZHI Models\n",
    "\n",
    "Before creating new training runs, test which existing models still work.\n",
    "\n",
    "**Models to Test:**\n",
    "1. `CryptoYogi/vazhi-qwen3-lora-best` - Best from v0.8 cycle training\n",
    "2. `CryptoYogi/qwen3-0.6b-vazhi` - Latest (v3.3 - likely broken)\n",
    "3. `CryptoYogi/vazhi-qwen3-lora-cycle-6` - Last cycle checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "def test_model(model_name, is_lora=True):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {model_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    try:\n",
    "        if is_lora:\n",
    "            # Load base model + LoRA adapter\n",
    "            base_model = \"Qwen/Qwen3-0.6B\"\n",
    "            print(f\"Loading base: {base_model}\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(f\"Loading LoRA: {model_name}\")\n",
    "            model = PeftModel.from_pretrained(model, model_name)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "        else:\n",
    "            # Load merged model directly\n",
    "            print(f\"Loading merged model: {model_name}\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        \n",
    "        model.config.use_cache = True\n",
    "        \n",
    "        # Test prompts\n",
    "        test_prompts = [\n",
    "            \"வணக்கம்\",\n",
    "            \"தமிழ்நாட்டின் தலைநகரம் என்ன?\",\n",
    "            \"2+2 என்ன?\",\n",
    "        ]\n",
    "        \n",
    "        system_prompt = \"நீங்கள் VAZHI (வழி), தமிழ் மக்களுக்கான AI உதவியாளர்.\"\n",
    "        \n",
    "        for prompt in test_prompts:\n",
    "            full_prompt = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "            \n",
    "            inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "            \n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "            if \"<|im_start|>assistant\" in response:\n",
    "                response = response.split(\"<|im_start|>assistant\")[-1]\n",
    "                response = response.split(\"<|im_end|>\")[0].strip()\n",
    "            \n",
    "            print(f\"Q: {prompt}\")\n",
    "            print(f\"A: {response[:200]}\")\n",
    "            print(\"-\" * 40)\n",
    "        \n",
    "        # Check if output looks valid\n",
    "        if \"system\" in response.lower() or len(response) < 5:\n",
    "            print(\"\\n❌ MODEL APPEARS BROKEN\")\n",
    "        elif any(ord(c) > 0x0B00 and ord(c) < 0x0BFF for c in response):\n",
    "            print(\"\\n✅ MODEL PRODUCES TAMIL OUTPUT\")\n",
    "        else:\n",
    "            print(\"\\n⚠️ MODEL OUTPUT MAY BE PROBLEMATIC\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {e}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vazhi-qwen3-lora-best (v0.8 best checkpoint)\n",
    "test_model(\"CryptoYogi/vazhi-qwen3-lora-best\", is_lora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vazhi-qwen3-lora-cycle-6 (last cycle)\n",
    "test_model(\"CryptoYogi/vazhi-qwen3-lora-cycle-6\", is_lora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test qwen3-0.6b-vazhi (latest merged - likely broken)\n",
    "test_model(\"CryptoYogi/qwen3-0.6b-vazhi\", is_lora=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Based on the tests above:\n",
    "\n",
    "| Model | Status | Notes |\n",
    "|-------|--------|-------|\n",
    "| vazhi-qwen3-lora-best | ? | Best from v0.8 |\n",
    "| vazhi-qwen3-lora-cycle-6 | ? | Last cycle |\n",
    "| qwen3-0.6b-vazhi | ? | Latest (v3.3) |\n",
    "\n",
    "**Next Steps:**\n",
    "- If lora-best works: Continue training from there\n",
    "- If all broken: Use v3.4 notebook with Qwen3-0.6B-Base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
