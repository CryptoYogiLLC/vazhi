{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI Dataset Factory v4.1 — 3-Stage Data Pipeline\n",
    "\n",
    "**Pipeline:** Retrieve → Curate → Compose\n",
    "\n",
    "```\n",
    "┌─ Stage 1: RETRIEVE (CPU, ~30 min) ──────────────────────────────┐\n",
    "│ IndicAlign (9 subsets) + tamil-orca + GSM8K_TAMIL + local        │\n",
    "│ → Upload raw to HF: CryptoYogi/vazhi-raw-tamil-qa-v1            │\n",
    "├─ Stage 2: CURATE (CPU then GPU, ~3-5 hours) ───────────────────┤\n",
    "│ Pass 1 (CPU): lang-id → tamil% → empties → MinHash dedup        │\n",
    "│ Pass 2 (GPU): perplexity + IndicSBERT on candidate subset       │\n",
    "│ → Upload curated to HF: CryptoYogi/vazhi-curated-tamil-qa-v1    │\n",
    "├─ Stage 3: COMPOSE (CPU, ~5 min) ────────────────────────────────┤\n",
    "│ Filter (quality + dedup + toxicity + token-length ≤ 2048)        │\n",
    "│ → ChatML conversion → absolute count targets → stratified split  │\n",
    "│ → Upload final SFT to HF: CryptoYogi/vazhi-tamil-sft-v4_1      │\n",
    "└──────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key changes from v4.0:**\n",
    "- **Broad retrieval** — ALL available Tamil Q&A from every source (~520K+), no artificial caps\n",
    "- **ML-based curation** — fasttext lang-id, MinHash dedup, perplexity scoring, semantic clustering\n",
    "- **Absolute count targets** — no percentage-based anchoring that caused cascading downsampling\n",
    "- **max_seq_length=2048** — stops the 74% domain pack rejection caused by 1024 window\n",
    "- **Safety routing** — Toxic_Matrix/HHRLHF_T refusal pairs routed to safety bucket, not filtered\n",
    "- **HF checkpointing** — each stage uploads to HF before continuing\n",
    "\n",
    "**Run on:** Kaggle P100 (GPU needed for perplexity + embeddings in Stage 2)\n",
    "\n",
    "**Output:** `CryptoYogi/vazhi-tamil-sft-v4_1` with `train` and `validation` splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Config & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets huggingface_hub fasttext-wheel text-dedup sentence-transformers hdbscan\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import hashlib\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# === CONFIG ===\n",
    "VERSION = \"4.1\"\n",
    "RAW_DATASET = \"CryptoYogi/vazhi-raw-tamil-qa-v1\"\n",
    "CURATED_DATASET = \"CryptoYogi/vazhi-curated-tamil-qa-v1\"\n",
    "OUTPUT_DATASET = \"CryptoYogi/vazhi-tamil-sft-v4_1\"\n",
    "DAPT_MODEL = \"CryptoYogi/qwen3-0.6b-tamil-v1_1\"\n",
    "TOKENIZER_MODEL = \"Qwen/Qwen3-0.6B\"\n",
    "REPO_URL = \"https://github.com/CryptoYogiLLC/vazhi.git\"\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "SFT_MAX_SEQ_LENGTH = 2048  # Training window — stops domain pack rejection\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"\\u0ba8\\u0bc0\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd VAZHI (\\u0bb5\\u0bb4\\u0bbf), \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd \\u0bae\\u0b95\\u0bcd\\u0b95\\u0bb3\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bbe\\u0ba9 AI \\u0b89\\u0ba4\\u0bb5\\u0bbf\\u0baf\\u0bbe\\u0bb3\\u0bb0\\u0bcd. \"\n",
    "    \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bbf\\u0bb2\\u0bcd \\u0ba4\\u0bc6\\u0bb3\\u0bbf\\u0bb5\\u0bbe\\u0b95\\u0bb5\\u0bc1\\u0bae\\u0bcd \\u0b89\\u0ba4\\u0bb5\\u0bbf\\u0baf\\u0bbe\\u0b95\\u0bb5\\u0bc1\\u0bae\\u0bcd \\u0baa\\u0ba4\\u0bbf\\u0bb2\\u0bb3\\u0bbf\\u0baf\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd. \"\n",
    "    '\\u0ba4\\u0bc6\\u0bb0\\u0bbf\\u0baf\\u0bbe\\u0bb5\\u0bbf\\u0b9f\\u0bcd\\u0b9f\\u0bbe\\u0bb2\\u0bcd \"\\u0ba4\\u0bc6\\u0bb0\\u0bbf\\u0baf\\u0bb5\\u0bbf\\u0bb2\\u0bcd\\u0bb2\\u0bc8\" \\u0b8e\\u0ba9\\u0bcd\\u0bb1\\u0bc1 \\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd.'\n",
    ")\n",
    "\n",
    "# Absolute count targets per source bucket (no percentage anchoring)\n",
    "BUCKET_TARGETS = {\n",
    "    \"vazhi_packs\":    {\"min\": 1500, \"target\": 2500, \"max\": 3000},\n",
    "    \"handcrafted\":    {\"min\": 100,  \"target\": 147,  \"max\": 200},\n",
    "    \"general\":        {\"min\": 250,  \"target\": 350,  \"max\": 500},\n",
    "    \"indicalign\":     {\"min\": 2000, \"target\": 4000, \"max\": 6000},\n",
    "    \"tamil_orca\":     {\"min\": 1000, \"target\": 2000, \"max\": 3000},\n",
    "    \"gsm8k_tamil\":    {\"min\": 200,  \"target\": 400,  \"max\": 600},\n",
    "    \"safety\":         {\"min\": 200,  \"target\": 500,  \"max\": 1000},\n",
    "}\n",
    "\n",
    "# Source priority for dedup (higher = keep)\n",
    "SOURCE_PRIORITY = {\n",
    "    \"vazhi_packs\": 10, \"handcrafted\": 10,\n",
    "    \"general\": 5,\n",
    "    \"indicalign\": 2, \"tamil_orca\": 2, \"gsm8k_tamil\": 2,\n",
    "}\n",
    "\n",
    "print(f\"\\u2705 Config loaded: Dataset Factory v{VERSION}\")\n",
    "print(f\"   Raw output: {RAW_DATASET}\")\n",
    "print(f\"   Curated output: {CURATED_DATASET}\")\n",
    "print(f\"   Final SFT output: {OUTPUT_DATASET}\")\n",
    "print(f\"   max_seq_length: {SFT_MAX_SEQ_LENGTH}\")\n",
    "print(f\"\\n   Bucket targets (absolute counts):\")\n",
    "target_total = sum(v[\"target\"] for v in BUCKET_TARGETS.values())\n",
    "for name, cfg in BUCKET_TARGETS.items():\n",
    "    print(f\"     {name}: {cfg['target']} (range {cfg['min']}-{cfg['max']})\")\n",
    "print(f\"   Expected total: ~{target_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace login\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "    login(token=hf_token)\n",
    "    print(\"\\u2705 Logged in via Kaggle secrets\")\n",
    "except Exception:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        hf_token = userdata.get('HF_TOKEN')\n",
    "        login(token=hf_token)\n",
    "        print(\"\\u2705 Logged in via Colab secrets\")\n",
    "    except Exception:\n",
    "        login()\n",
    "        print(\"\\u2705 Logged in interactively\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 1: RETRIEVE\n",
    "\n",
    "Pull ALL available Tamil Q&A from every open-licensed source. Minimal filtering (only reject empty/null). Store with source metadata for downstream curation.\n",
    "\n",
    "**Output:** `CryptoYogi/vazhi-raw-tamil-qa-v1` on HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone only data/ and vazhi-packs/ from the public repo (sparse checkout)\n",
    "REPO_DIR = Path(\"/kaggle/working/vazhi\")\n",
    "if not REPO_DIR.exists():\n",
    "    print(f\"Cloning {REPO_URL} (sparse: data/ + vazhi-packs/)...\")\n",
    "    subprocess.run(\n",
    "        [\"git\", \"clone\", \"--depth\", \"1\", \"--filter=blob:none\", \"--sparse\", REPO_URL, str(REPO_DIR)],\n",
    "        check=True,\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\"git\", \"sparse-checkout\", \"set\", \"data/\", \"vazhi-packs/\"],\n",
    "        cwd=str(REPO_DIR),\n",
    "        check=True,\n",
    "    )\n",
    "    print(f\"\\u2705 Cloned to {REPO_DIR}\")\n",
    "else:\n",
    "    subprocess.run([\"git\", \"pull\"], cwd=str(REPO_DIR), check=True)\n",
    "    print(f\"\\u2705 Repo already at {REPO_DIR}, pulled latest\")\n",
    "\n",
    "SOURCES_DIR = REPO_DIR / \"data\" / \"sources\"\n",
    "LEGACY_DIR = REPO_DIR / \"data\" / \"LEGACY\"\n",
    "PACKS_DIR = SOURCES_DIR / \"sft\" / \"vazhi-packs\"\n",
    "HANDCRAFTED_DIR = SOURCES_DIR / \"sft\" / \"handcrafted\"\n",
    "\n",
    "for label, d in [(\"sources/sft/vazhi-packs\", PACKS_DIR), (\"sources/sft/handcrafted\", HANDCRAFTED_DIR), (\"LEGACY\", LEGACY_DIR)]:\n",
    "    assert d.exists(), f\"Missing: {d}\"\n",
    "    print(f\"  \\u2705 {label}: {d}\")\n",
    "print(f\"\\n\\u2705 All source data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions (reused from v4.0 + new additions)\n",
    "\n",
    "def to_chatml(instruction, output, system_prompt=None):\n",
    "    \"\"\"Convert instruction/output pair to strict ChatML format.\"\"\"\n",
    "    sp = system_prompt or SYSTEM_PROMPT\n",
    "    return (\n",
    "        f\"<|im_start|>system\\n{sp}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{instruction}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n{output}<|im_end|>\"\n",
    "    )\n",
    "\n",
    "\n",
    "CHATML_PATTERN = re.compile(\n",
    "    r'<\\|im_start\\|>system\\n.+?<\\|im_end\\|>\\n'\n",
    "    r'<\\|im_start\\|>user\\n(.+?)<\\|im_end\\|>\\n'\n",
    "    r'<\\|im_start\\|>assistant\\n(.+?)<\\|im_end\\|>',\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "\n",
    "def validate_chatml_strict(text):\n",
    "    \"\"\"Validate a sample has proper ChatML with non-empty user AND assistant.\"\"\"\n",
    "    match = CHATML_PATTERN.search(text)\n",
    "    if not match:\n",
    "        return False, \"no ChatML structure found\"\n",
    "    user_content = match.group(1).strip()\n",
    "    assistant_content = match.group(2).strip()\n",
    "    if len(user_content) < 2:\n",
    "        return False, \"empty user content\"\n",
    "    if len(assistant_content) < 2:\n",
    "        return False, \"empty assistant content\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "\n",
    "def count_tamil_chars(text):\n",
    "    \"\"\"Count Tamil Unicode characters (U+0B80 to U+0BFF).\"\"\"\n",
    "    return sum(1 for c in text if '\\u0b80' <= c <= '\\u0bff')\n",
    "\n",
    "\n",
    "def tamil_char_pct(text):\n",
    "    \"\"\"Get Tamil character percentage (0.0-1.0).\"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    return count_tamil_chars(text) / len(text)\n",
    "\n",
    "\n",
    "def is_verbatim_kural_qa(question, answer):\n",
    "    \"\"\"Reject Q&As that ask for exact verse text.\"\"\"\n",
    "    verbatim_patterns = [\n",
    "        r'\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*\\d+\\s*(\\u0b8e\\u0ba9\\u0bcd\\u0ba9|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd|\\u0b8e\\u0bb4\\u0bc1\\u0ba4\\u0bbf\\s*\\u0b95\\u0bbe\\u0b9f\\u0bcd\\u0b9f\\u0bc1|\\u0b95\\u0bc2\\u0bb1\\u0bc1\\u0b95)',\n",
    "        r'(first|\\u0bae\\u0bc1\\u0ba4\\u0bb2\\u0bcd)\\s*\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*(\\u0b8e\\u0ba9\\u0bcd\\u0ba9|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd)',\n",
    "        r'\\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bbf\\u0ba9\\u0bcd\\s+\\u0bae\\u0bc1\\u0ba4\\u0bb2\\u0bcd\\s+\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd',\n",
    "        r'\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*[\\d]+(?:\\s*\\u0b90)?\\s*\\u0b8e\\u0bb4\\u0bc1\\u0ba4\\u0bbf',\n",
    "    ]\n",
    "    for pat in verbatim_patterns:\n",
    "        if re.search(pat, question, re.IGNORECASE):\n",
    "            return True\n",
    "    if len(answer) < 200 and \"\\n\" in answer and not any(\n",
    "        w in answer for w in [\"\\u0bb5\\u0bbf\\u0bb3\\u0b95\\u0bcd\\u0b95\\u0bae\\u0bcd\", \"\\u0baa\\u0bca\\u0bb0\\u0bc1\\u0bb3\\u0bcd\", \"\\u0b85\\u0bb0\\u0bcd\\u0ba4\\u0bcd\\u0ba4\\u0bae\\u0bcd\"]\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Self-test\n",
    "good = to_chatml(\"test question\", \"test answer\")\n",
    "valid, reason = validate_chatml_strict(good)\n",
    "assert valid, f\"Self-test failed: {reason}\"\n",
    "assert 0.0 <= tamil_char_pct(\"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd test\") <= 1.0\n",
    "print(\"\\u2705 Helper functions defined and self-tested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1A: Load IndicAlign — ALL 9 Tamil subsets (no cap)\n",
    "# Stream each completely, extract tam_Taml pairs, tag with source metadata.\n",
    "\n",
    "INDICALIGN_SUBSETS = [\n",
    "    \"Dolly_T\", \"WikiHow\", \"Wiki_Conv\", \"OpenAssistant_T\",\n",
    "    \"Anudesh\", \"HHRLHF_T\", \"Indic_ShareLlama\", \"Wiki_Chat\", \"Toxic_Matrix\",\n",
    "]\n",
    "\n",
    "raw_samples = []  # Global accumulator for all Stage 1 sources\n",
    "\n",
    "print(\"Loading IndicAlign subsets (streaming all)...\")\n",
    "for subset in INDICALIGN_SUBSETS:\n",
    "    try:\n",
    "        ds = load_dataset(\n",
    "            \"ai4bharat/indic-align\",\n",
    "            subset,\n",
    "            split=\"train\",\n",
    "            streaming=True,\n",
    "        )\n",
    "\n",
    "        subset_count = 0\n",
    "        batch = []\n",
    "\n",
    "        for item in ds:\n",
    "            pairs = item.get(\"tam_Taml\", [])\n",
    "            if not pairs or not isinstance(pairs, (list, tuple)):\n",
    "                continue\n",
    "\n",
    "            for pair in pairs:\n",
    "                if not isinstance(pair, (list, tuple)) or len(pair) < 2:\n",
    "                    continue\n",
    "\n",
    "                instruction = str(pair[0]).strip() if pair[0] else \"\"\n",
    "                output = str(pair[1]).strip() if pair[1] else \"\"\n",
    "\n",
    "                # Minimal filter: reject empty/null only\n",
    "                if not instruction or not output:\n",
    "                    continue\n",
    "\n",
    "                batch.append({\n",
    "                    \"instruction\": instruction,\n",
    "                    \"output\": output,\n",
    "                    \"source\": \"indicalign\",\n",
    "                    \"subset\": subset,\n",
    "                })\n",
    "                subset_count += 1\n",
    "\n",
    "            # Flush batch every 10K to manage memory (for Wiki_Chat ~198K)\n",
    "            if len(batch) >= 10000:\n",
    "                raw_samples.extend(batch)\n",
    "                batch = []\n",
    "\n",
    "        raw_samples.extend(batch)\n",
    "        print(f\"  {subset}: {subset_count:,} pairs\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  {subset}: FAILED - {e}\")\n",
    "\n",
    "indicalign_total = sum(1 for s in raw_samples if s[\"source\"] == \"indicalign\")\n",
    "print(f\"\\n\\u2705 IndicAlign total: {indicalign_total:,} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1B: Load tamil-orca (~97K samples)\n",
    "# Schema: Instruction, Query, Answer — combine Instruction+Query into instruction field.\n",
    "\n",
    "print(\"Loading tamil-orca (streaming)...\")\n",
    "try:\n",
    "    ds = load_dataset(\"azharmo/tamil-orca\", split=\"train\", streaming=True)\n",
    "    orca_count = 0\n",
    "    batch = []\n",
    "\n",
    "    for item in ds:\n",
    "        instruction_part = str(item.get(\"Instruction\", \"\")).strip()\n",
    "        query_part = str(item.get(\"Query\", \"\")).strip()\n",
    "        answer = str(item.get(\"Answer\", \"\")).strip()\n",
    "\n",
    "        # Combine Instruction + Query for the instruction field\n",
    "        if instruction_part and query_part:\n",
    "            instruction = f\"{instruction_part}\\n{query_part}\"\n",
    "        elif query_part:\n",
    "            instruction = query_part\n",
    "        elif instruction_part:\n",
    "            instruction = instruction_part\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if not answer:\n",
    "            continue\n",
    "\n",
    "        batch.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"output\": answer,\n",
    "            \"source\": \"tamil_orca\",\n",
    "            \"subset\": \"\",\n",
    "        })\n",
    "        orca_count += 1\n",
    "\n",
    "        if len(batch) >= 10000:\n",
    "            raw_samples.extend(batch)\n",
    "            batch = []\n",
    "\n",
    "    raw_samples.extend(batch)\n",
    "    print(f\"\\u2705 tamil-orca: {orca_count:,} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"\\u274c tamil-orca FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1C: Load GSM8K_TAMIL (~8.8K math samples)\n",
    "# Schema: question, answer\n",
    "\n",
    "print(\"Loading GSM8K_TAMIL (streaming)...\")\n",
    "try:\n",
    "    ds = load_dataset(\"Vishal0407/GSM8K_TAMIL\", split=\"train\", streaming=True)\n",
    "    gsm_count = 0\n",
    "\n",
    "    for item in ds:\n",
    "        question = str(item.get(\"question\", \"\")).strip()\n",
    "        answer = str(item.get(\"answer\", \"\")).strip()\n",
    "\n",
    "        if not question or not answer:\n",
    "            continue\n",
    "\n",
    "        raw_samples.append({\n",
    "            \"instruction\": question,\n",
    "            \"output\": answer,\n",
    "            \"source\": \"gsm8k_tamil\",\n",
    "            \"subset\": \"\",\n",
    "        })\n",
    "        gsm_count += 1\n",
    "\n",
    "    print(f\"\\u2705 GSM8K_TAMIL: {gsm_count:,} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"\\u274c GSM8K_TAMIL FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1D: Load local sources (vazhi-packs, handcrafted, general from LEGACY)\n",
    "\n",
    "# --- vazhi-packs ---\n",
    "print(\"Loading vazhi-packs...\")\n",
    "packs_count = 0\n",
    "for pack_file in sorted(PACKS_DIR.glob(\"*.json\")):\n",
    "    with open(pack_file, encoding=\"utf-8\") as f:\n",
    "        pairs = json.load(f)\n",
    "    file_count = 0\n",
    "    for pair in pairs:\n",
    "        instruction = pair.get(\"instruction\", \"\").strip()\n",
    "        output = pair.get(\"output\", \"\").strip()\n",
    "        if not instruction or not output:\n",
    "            continue\n",
    "        raw_samples.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"output\": output,\n",
    "            \"source\": \"vazhi_packs\",\n",
    "            \"subset\": pack_file.stem,\n",
    "        })\n",
    "        file_count += 1\n",
    "        packs_count += 1\n",
    "    print(f\"  {pack_file.stem}: {file_count}\")\n",
    "print(f\"\\u2705 vazhi-packs total: {packs_count:,}\")\n",
    "\n",
    "# --- handcrafted ---\n",
    "print(\"\\nLoading handcrafted...\")\n",
    "hc_count = 0\n",
    "for hc_file in sorted(HANDCRAFTED_DIR.glob(\"*.json\")):\n",
    "    with open(hc_file, encoding=\"utf-8\") as f:\n",
    "        items = json.load(f)\n",
    "    file_count = 0\n",
    "    for item in items:\n",
    "        instruction = item.get(\"instruction\", \"\").strip()\n",
    "        output = item.get(\"output\", \"\").strip()\n",
    "        if not instruction or not output:\n",
    "            continue\n",
    "        raw_samples.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"output\": output,\n",
    "            \"source\": \"handcrafted\",\n",
    "            \"subset\": hc_file.stem,\n",
    "        })\n",
    "        file_count += 1\n",
    "        hc_count += 1\n",
    "    print(f\"  {hc_file.stem}: {file_count}\")\n",
    "print(f\"\\u2705 handcrafted total: {hc_count:,}\")\n",
    "\n",
    "# --- general (LEGACY) ---\n",
    "print(\"\\nLoading general (LEGACY)...\")\n",
    "GENERAL_FILES = [\n",
    "    \"06_health.json\", \"09_weather.json\", \"10_shopping.json\",\n",
    "    \"12_daily_routines.json\", \"13_emotions.json\",\n",
    "    \"14_chennai_dialect.json\", \"15_madurai_dialect.json\",\n",
    "    \"16_kongu_dialect.json\", \"31_malaysia_dialect.json\",\n",
    "    \"03_numbers_time.json\",\n",
    "]\n",
    "gen_count = 0\n",
    "for fname in GENERAL_FILES:\n",
    "    fpath = LEGACY_DIR / fname\n",
    "    if not fpath.exists():\n",
    "        print(f\"  {fname}: NOT FOUND, skipping\")\n",
    "        continue\n",
    "    with open(fpath, encoding=\"utf-8\") as f:\n",
    "        items = json.load(f)\n",
    "    file_count = 0\n",
    "    for item in items:\n",
    "        instruction = item.get(\"instruction\", item.get(\"question\", \"\")).strip()\n",
    "        output = item.get(\"output\", item.get(\"answer\", \"\")).strip()\n",
    "        if not instruction or not output:\n",
    "            continue\n",
    "        raw_samples.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"output\": output,\n",
    "            \"source\": \"general\",\n",
    "            \"subset\": fname.replace(\".json\", \"\"),\n",
    "        })\n",
    "        file_count += 1\n",
    "        gen_count += 1\n",
    "    print(f\"  {fname}: {file_count}\")\n",
    "print(f\"\\u2705 general total: {gen_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1E: Schema normalization — compute tamil_pct and char_length for all samples\n",
    "\n",
    "print(f\"Normalizing {len(raw_samples):,} samples...\")\n",
    "for s in raw_samples:\n",
    "    combined = s[\"instruction\"] + s[\"output\"]\n",
    "    s[\"char_length\"] = len(combined)\n",
    "    s[\"tamil_pct\"] = round(tamil_char_pct(combined), 4)\n",
    "\n",
    "print(f\"\\u2705 Schema normalization complete\")\n",
    "print(f\"   Fields: {list(raw_samples[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1F: Stats, spot-check, and upload raw dataset to HF\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"STAGE 1 COMPLETE: {len(raw_samples):,} raw samples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Per-source counts\n",
    "source_counts = Counter(s[\"source\"] for s in raw_samples)\n",
    "print(\"\\nPer-source counts:\")\n",
    "for src, count in source_counts.most_common():\n",
    "    print(f\"  {src}: {count:,}\")\n",
    "\n",
    "# Per-subset counts for IndicAlign\n",
    "subset_counts = Counter(s[\"subset\"] for s in raw_samples if s[\"source\"] == \"indicalign\")\n",
    "print(\"\\nIndicAlign subset breakdown:\")\n",
    "for sub, count in subset_counts.most_common():\n",
    "    print(f\"  {sub}: {count:,}\")\n",
    "\n",
    "# Length distribution\n",
    "lengths = [s[\"char_length\"] for s in raw_samples]\n",
    "print(f\"\\nLength stats: min={min(lengths)}, median={sorted(lengths)[len(lengths)//2]}, max={max(lengths)}, mean={sum(lengths)/len(lengths):.0f}\")\n",
    "\n",
    "# Tamil % distribution\n",
    "tamil_pcts = [s[\"tamil_pct\"] for s in raw_samples]\n",
    "print(f\"Tamil% stats: min={min(tamil_pcts):.2f}, median={sorted(tamil_pcts)[len(tamil_pcts)//2]:.2f}, mean={sum(tamil_pcts)/len(tamil_pcts):.2f}\")\n",
    "\n",
    "# Spot-check: 3 random samples from each source\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Spot-check (3 per source):\")\n",
    "print(\"=\" * 60)\n",
    "for src in sorted(source_counts.keys()):\n",
    "    src_samples = [s for s in raw_samples if s[\"source\"] == src]\n",
    "    picks = random.sample(src_samples, min(3, len(src_samples)))\n",
    "    print(f\"\\n[{src.upper()}]\")\n",
    "    for p in picks:\n",
    "        print(f\"  Q: {p['instruction'][:80]}...\")\n",
    "        print(f\"  A: {p['output'][:80]}...\")\n",
    "        print(f\"  tamil%={p['tamil_pct']:.2f} len={p['char_length']}\")\n",
    "\n",
    "# Upload to HuggingFace\n",
    "print(f\"\\nUploading {len(raw_samples):,} raw samples to {RAW_DATASET}...\")\n",
    "raw_ds = Dataset.from_list(raw_samples)\n",
    "api = HfApi()\n",
    "api.create_repo(RAW_DATASET, repo_type=\"dataset\", exist_ok=True)\n",
    "raw_ds.push_to_hub(RAW_DATASET)\n",
    "print(f\"\\u2705 Raw dataset uploaded: https://huggingface.co/datasets/{RAW_DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 2: CURATE\n",
    "\n",
    "Two-pass curation: cheap CPU filters first to reduce the pool, then GPU-based scoring on the surviving candidates.\n",
    "\n",
    "**Input:** In-memory raw dataset (also backed up on HF from Stage 1)  \n",
    "**Output:** `CryptoYogi/vazhi-curated-tamil-qa-v1` on HuggingFace\n",
    "\n",
    "### Pass 1 (CPU): Language detection → heuristics → dedup → toxicity  \n",
    "### Pass 2 (GPU): Perplexity scoring → semantic categorization → composite quality score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Pass 1A: Language Detection with fasttext lid.176.bin\n",
    "\n",
    "import fasttext\n",
    "import urllib.request\n",
    "\n",
    "LID_MODEL_PATH = \"/kaggle/working/lid.176.bin\"\n",
    "if not os.path.exists(LID_MODEL_PATH):\n",
    "    print(\"Downloading fasttext language ID model (126MB)...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\",\n",
    "        LID_MODEL_PATH,\n",
    "    )\n",
    "lid_model = fasttext.load_model(LID_MODEL_PATH)\n",
    "print(\"\\u2705 fasttext lid.176.bin loaded\")\n",
    "\n",
    "\n",
    "def detect_lang(text):\n",
    "    \"\"\"Detect language using fasttext. Returns (lang_code, confidence).\"\"\"\n",
    "    clean = text.replace(\"\\n\", \" \").strip()\n",
    "    if not clean:\n",
    "        return \"unknown\", 0.0\n",
    "    labels, scores = lid_model.predict(clean, k=1)\n",
    "    return labels[0].replace(\"__label__\", \"\"), float(scores[0])\n",
    "\n",
    "\n",
    "# Run language detection on all samples\n",
    "print(f\"Running language detection on {len(raw_samples):,} samples...\")\n",
    "lang_dropped = 0\n",
    "for i, s in enumerate(raw_samples):\n",
    "    combined = s[\"instruction\"] + \" \" + s[\"output\"]\n",
    "    lang_id, lang_conf = detect_lang(combined)\n",
    "    s[\"lang_id\"] = lang_id\n",
    "    s[\"lang_confidence\"] = round(lang_conf, 4)\n",
    "    if i % 50000 == 0 and i > 0:\n",
    "        print(f\"  ...{i:,} / {len(raw_samples):,}\")\n",
    "\n",
    "# Apply threshold: keep only Tamil with confidence >= 0.6\n",
    "before = len(raw_samples)\n",
    "candidates = [s for s in raw_samples if s[\"lang_id\"] == \"ta\" and s[\"lang_confidence\"] >= 0.6]\n",
    "lang_dropped = before - len(candidates)\n",
    "\n",
    "print(f\"\\n\\u2705 Language detection complete\")\n",
    "print(f\"   Before: {before:,}\")\n",
    "print(f\"   Dropped (non-Tamil or low confidence): {lang_dropped:,}\")\n",
    "print(f\"   Remaining: {len(candidates):,}\")\n",
    "\n",
    "# Show lang distribution of dropped samples\n",
    "non_tamil = [s for s in raw_samples if s[\"lang_id\"] != \"ta\" or s[\"lang_confidence\"] < 0.6]\n",
    "dropped_langs = Counter(s[\"lang_id\"] for s in non_tamil)\n",
    "print(f\"   Dropped language distribution: {dropped_langs.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Pass 1B: Quality Heuristics + Format Sanity\n",
    "\n",
    "def repetition_score(text):\n",
    "    \"\"\"Most-common-token ratio. High = repetitive/garbage.\"\"\"\n",
    "    toks = text.split()\n",
    "    if len(toks) < 5:\n",
    "        return 0.0\n",
    "    most = Counter(toks).most_common(1)[0][1] / len(toks)\n",
    "    return most\n",
    "\n",
    "\n",
    "def is_sane(instruction, output):\n",
    "    \"\"\"Format sanity — catches template corruption.\"\"\"\n",
    "    if \"<think>\" in output:\n",
    "        return False\n",
    "    if output.strip() == instruction.strip():\n",
    "        return False\n",
    "    if \"data:image\" in output:\n",
    "        return False\n",
    "    if \"<|im_start|>system\" in output:\n",
    "        return False\n",
    "    if \"systemsystem\" in output.lower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def is_echo(instruction, output):\n",
    "    \"\"\"Check if answer is >80% copy of instruction.\"\"\"\n",
    "    if not instruction or not output:\n",
    "        return False\n",
    "    # Simple overlap check\n",
    "    instr_set = set(instruction.split())\n",
    "    out_set = set(output.split())\n",
    "    if not out_set:\n",
    "        return False\n",
    "    overlap = len(instr_set & out_set) / len(out_set)\n",
    "    return overlap > 0.80\n",
    "\n",
    "\n",
    "print(f\"Running quality heuristics on {len(candidates):,} candidates...\")\n",
    "heuristic_stats = Counter()\n",
    "clean_candidates = []\n",
    "\n",
    "for s in candidates:\n",
    "    flags = []\n",
    "\n",
    "    # Tamil % threshold\n",
    "    if s[\"tamil_pct\"] < 0.30:\n",
    "        flags.append(\"low_tamil_pct\")\n",
    "\n",
    "    # Repetition score\n",
    "    rep = repetition_score(s[\"output\"])\n",
    "    s[\"repetition\"] = round(rep, 4)\n",
    "    if rep > 0.25 and len(s[\"output\"].split()) > 20:\n",
    "        flags.append(\"high_repetition\")\n",
    "\n",
    "    # Echo detection\n",
    "    if is_echo(s[\"instruction\"], s[\"output\"]):\n",
    "        flags.append(\"echo\")\n",
    "\n",
    "    # Format sanity\n",
    "    if not is_sane(s[\"instruction\"], s[\"output\"]):\n",
    "        flags.append(\"format_insane\")\n",
    "\n",
    "    # Empty/trivial filter\n",
    "    if len(s[\"output\"].strip()) < 10:\n",
    "        flags.append(\"trivial_output\")\n",
    "    if len(s[\"instruction\"].strip()) < 5:\n",
    "        flags.append(\"trivial_instruction\")\n",
    "\n",
    "    s[\"heuristic_flags\"] = flags\n",
    "\n",
    "    for f in flags:\n",
    "        heuristic_stats[f] += 1\n",
    "\n",
    "    if len(flags) == 0:\n",
    "        clean_candidates.append(s)\n",
    "\n",
    "heuristic_dropped = len(candidates) - len(clean_candidates)\n",
    "print(f\"\\n\\u2705 Heuristic filtering complete\")\n",
    "print(f\"   Before: {len(candidates):,}\")\n",
    "print(f\"   Dropped: {heuristic_dropped:,}\")\n",
    "print(f\"   Remaining: {len(clean_candidates):,}\")\n",
    "print(f\"   Flag distribution:\")\n",
    "for flag, count in heuristic_stats.most_common():\n",
    "    print(f\"     {flag}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Pass 1C: Deduplication with Source Priority\n",
    "# Phase 1: exact dedupe on instruction[:200] (fast hash-based)\n",
    "# Phase 2: near-duplicate detection via MinHash (if time permits)\n",
    "\n",
    "print(f\"Deduplicating {len(clean_candidates):,} candidates...\")\n",
    "\n",
    "# Phase 1: Exact dedupe on instruction[:200]\n",
    "seen = {}\n",
    "for s in clean_candidates:\n",
    "    key = s[\"instruction\"][:200].strip().lower()\n",
    "    if key not in seen:\n",
    "        seen[key] = s\n",
    "    else:\n",
    "        # Keep higher-priority source\n",
    "        existing_priority = SOURCE_PRIORITY.get(seen[key][\"source\"], 1)\n",
    "        new_priority = SOURCE_PRIORITY.get(s[\"source\"], 1)\n",
    "        if new_priority > existing_priority:\n",
    "            seen[key] = s\n",
    "        elif new_priority == existing_priority and s[\"tamil_pct\"] > seen[key][\"tamil_pct\"]:\n",
    "            seen[key] = s\n",
    "\n",
    "exact_deduped = list(seen.values())\n",
    "exact_dupes = len(clean_candidates) - len(exact_deduped)\n",
    "print(f\"  Phase 1 (exact): {len(clean_candidates):,} \\u2192 {len(exact_deduped):,} ({exact_dupes:,} duplicates removed)\")\n",
    "\n",
    "# Phase 2: MinHash near-duplicate detection (Jaccard >= 0.8)\n",
    "# Uses MD5 hash of character 3-grams for efficiency\n",
    "def get_char_ngrams(text, n=3):\n",
    "    \"\"\"Get character n-grams for MinHash.\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    return set(text[i:i+n] for i in range(len(text) - n + 1))\n",
    "\n",
    "def minhash_signature(ngrams, num_hashes=128):\n",
    "    \"\"\"Compute MinHash signature.\"\"\"\n",
    "    if not ngrams:\n",
    "        return [float('inf')] * num_hashes\n",
    "    sig = []\n",
    "    for i in range(num_hashes):\n",
    "        min_hash = float('inf')\n",
    "        for ng in ngrams:\n",
    "            h = int(hashlib.md5(f\"{i}_{ng}\".encode()).hexdigest(), 16) % (2**32)\n",
    "            min_hash = min(min_hash, h)\n",
    "        sig.append(min_hash)\n",
    "    return sig\n",
    "\n",
    "def jaccard_from_sigs(sig1, sig2):\n",
    "    \"\"\"Estimate Jaccard similarity from MinHash signatures.\"\"\"\n",
    "    return sum(a == b for a, b in zip(sig1, sig2)) / len(sig1)\n",
    "\n",
    "# Only run MinHash if pool is manageable (< 300K)\n",
    "if len(exact_deduped) <= 300000:\n",
    "    print(f\"  Phase 2 (MinHash): computing signatures for {len(exact_deduped):,} samples...\")\n",
    "    sigs = []\n",
    "    for i, s in enumerate(exact_deduped):\n",
    "        ngrams = get_char_ngrams(s[\"instruction\"])\n",
    "        sigs.append(minhash_signature(ngrams))\n",
    "        if i % 50000 == 0 and i > 0:\n",
    "            print(f\"    ...{i:,} / {len(exact_deduped):,} signatures computed\")\n",
    "\n",
    "    # LSH-style banding for efficient near-duplicate detection\n",
    "    # Band size = 4, so 128/4 = 32 bands — catches Jaccard >= ~0.8\n",
    "    BAND_SIZE = 4\n",
    "    NUM_BANDS = 128 // BAND_SIZE\n",
    "    near_dupes = set()\n",
    "\n",
    "    for band_idx in range(NUM_BANDS):\n",
    "        buckets = {}\n",
    "        start = band_idx * BAND_SIZE\n",
    "        end = start + BAND_SIZE\n",
    "        for i, sig in enumerate(sigs):\n",
    "            band_hash = tuple(sig[start:end])\n",
    "            if band_hash in buckets:\n",
    "                # Found a candidate pair — verify with full Jaccard\n",
    "                j = buckets[band_hash]\n",
    "                if i not in near_dupes and j not in near_dupes:\n",
    "                    jac = jaccard_from_sigs(sigs[i], sigs[j])\n",
    "                    if jac >= 0.8:\n",
    "                        # Keep higher-priority source\n",
    "                        si, sj = exact_deduped[i], exact_deduped[j]\n",
    "                        pi = SOURCE_PRIORITY.get(si[\"source\"], 1)\n",
    "                        pj = SOURCE_PRIORITY.get(sj[\"source\"], 1)\n",
    "                        if pi < pj or (pi == pj and si[\"tamil_pct\"] < sj[\"tamil_pct\"]):\n",
    "                            near_dupes.add(i)\n",
    "                        else:\n",
    "                            near_dupes.add(j)\n",
    "            else:\n",
    "                buckets[band_hash] = i\n",
    "\n",
    "    deduped = [s for i, s in enumerate(exact_deduped) if i not in near_dupes]\n",
    "    print(f\"  Phase 2 (MinHash): {len(exact_deduped):,} \\u2192 {len(deduped):,} ({len(near_dupes):,} near-duplicates removed)\")\n",
    "else:\n",
    "    deduped = exact_deduped\n",
    "    print(f\"  Phase 2 (MinHash): skipped (pool > 300K, exact dedupe sufficient)\")\n",
    "\n",
    "# Mark all with is_duplicate = False (survivors)\n",
    "for s in deduped:\n",
    "    s[\"is_duplicate\"] = False\n",
    "\n",
    "total_dedup_removed = len(clean_candidates) - len(deduped)\n",
    "print(f\"\\n\\u2705 Deduplication complete: {len(clean_candidates):,} \\u2192 {len(deduped):,} ({total_dedup_removed:,} total removed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Pass 1D: Toxicity Screening with Source-Aware Logic\n",
    "# For Toxic_Matrix/HHRLHF_T: toxic prompt + safe output = safety training data\n",
    "# For all others: any toxicity = flag for exclusion\n",
    "\n",
    "# Tamil toxicity wordlist (common slurs and NSFW terms)\n",
    "# Minimal list — expand as needed\n",
    "TOXIC_WORDS_TA = [\n",
    "    \"\\u0ba4\\u0bc7\\u0bb5\\u0b9f\\u0bbf\\u0baf\\u0bbe\",  # derogatory\n",
    "    \"\\u0bb5\\u0bc7\\u0b9a\\u0bbf\",  # slur\n",
    "    \"\\u0baa\\u0bc1\\u0ba3\\u0bcd\\u0b9f\\u0bc8\",  # vulgar\n",
    "    \"\\u0ba4\\u0bb1\\u0bcd\\u0b95\\u0bca\\u0bb2\\u0bc8\",  # murder\n",
    "    \"\\u0b95\\u0bca\\u0bb2\\u0bcd\\u0bb2\",  # kill\n",
    "    \"\\u0bb5\\u0bc6\\u0b9f\\u0bbf\\u0baa\\u0bca\\u0bb0\\u0bc1\\u0bb3\\u0bcd\",  # explosive\n",
    "]\n",
    "\n",
    "TOXIC_WORDS_EN = [\n",
    "    \"suicide method\", \"how to kill\", \"make a bomb\", \"hack into\",\n",
    "    \"child porn\", \"exploit children\",\n",
    "]\n",
    "\n",
    "\n",
    "def check_wordlist(text):\n",
    "    \"\"\"Check text against toxicity wordlists. Returns list of matched terms.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    flags = []\n",
    "    for word in TOXIC_WORDS_TA:\n",
    "        if word in text:\n",
    "            flags.append(f\"ta:{word[:10]}\")\n",
    "    for phrase in TOXIC_WORDS_EN:\n",
    "        if phrase in text_lower:\n",
    "            flags.append(f\"en:{phrase[:15]}\")\n",
    "    return flags\n",
    "\n",
    "\n",
    "def classify_toxicity(instruction, output, source, subset):\n",
    "    \"\"\"Source-aware toxicity classification.\"\"\"\n",
    "    instr_flags = check_wordlist(instruction)\n",
    "    output_flags = check_wordlist(output)\n",
    "\n",
    "    # Toxic_Matrix / HHRLHF_T: toxic prompt + safe response = safety training data\n",
    "    if subset in (\"Toxic_Matrix\", \"HHRLHF_T\") and instr_flags and not output_flags:\n",
    "        return instr_flags, True  # is_safety_sample = True\n",
    "\n",
    "    # Normal sources: any toxicity = flag for exclusion\n",
    "    return instr_flags + output_flags, False\n",
    "\n",
    "\n",
    "print(f\"Running toxicity screening on {len(deduped):,} candidates...\")\n",
    "safety_count = 0\n",
    "toxic_dropped = 0\n",
    "\n",
    "for s in deduped:\n",
    "    tox_flags, is_safety = classify_toxicity(\n",
    "        s[\"instruction\"], s[\"output\"], s[\"source\"], s[\"subset\"]\n",
    "    )\n",
    "    s[\"toxicity_flags\"] = tox_flags\n",
    "    s[\"is_safety_sample\"] = is_safety\n",
    "    if is_safety:\n",
    "        safety_count += 1\n",
    "    elif tox_flags:\n",
    "        toxic_dropped += 1\n",
    "\n",
    "print(f\"\\u2705 Toxicity screening complete\")\n",
    "print(f\"   Safety samples (toxic prompt + safe refusal): {safety_count:,}\")\n",
    "print(f\"   Toxic (will be excluded in Stage 3): {toxic_dropped:,}\")\n",
    "print(f\"   Clean: {len(deduped) - safety_count - toxic_dropped:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Pass 1 Summary Stats\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PASS 1 SUMMARY (CPU Filters)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Raw input:          {len(raw_samples):,}\")\n",
    "print(f\"  After lang-id:      {len(candidates):,} (-{lang_dropped:,})\")\n",
    "print(f\"  After heuristics:   {len(clean_candidates):,} (-{heuristic_dropped:,})\")\n",
    "print(f\"  After dedup:        {len(deduped):,} (-{total_dedup_removed:,})\")\n",
    "print(f\"  Safety routed:      {safety_count:,}\")\n",
    "print(f\"  Toxic flagged:      {toxic_dropped:,}\")\n",
    "print(f\"  \\u2192 Pass 1 output:   {len(deduped):,} candidates\")\n",
    "\n",
    "# Per-source breakdown of survivors\n",
    "survivor_sources = Counter(s[\"source\"] for s in deduped)\n",
    "print(f\"\\nSurvivors by source:\")\n",
    "for src, count in survivor_sources.most_common():\n",
    "    print(f\"  {src}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Pass 2E: Perplexity Scoring (GPU)\n",
    "# Uses DAPT v1.1 model to score output quality.\n",
    "# PPL is a fluency metric, NOT a quality metric — use as weak signal for garbage detection.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"Loading DAPT v1.1 model for perplexity scoring: {DAPT_MODEL}\")\n",
    "ppl_tokenizer = AutoTokenizer.from_pretrained(DAPT_MODEL)\n",
    "ppl_model = AutoModelForCausalLM.from_pretrained(\n",
    "    DAPT_MODEL, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "ppl_model.eval()\n",
    "print(f\"\\u2705 PPL model loaded on {ppl_model.device}\")\n",
    "\n",
    "# Stratified sample if pool > 200K (proportional per source)\n",
    "MAX_PPL_SAMPLES = 200000\n",
    "if len(deduped) > MAX_PPL_SAMPLES:\n",
    "    print(f\"Pool ({len(deduped):,}) > {MAX_PPL_SAMPLES:,}, stratified sampling...\")\n",
    "    by_source = {}\n",
    "    for s in deduped:\n",
    "        by_source.setdefault(s[\"source\"], []).append(s)\n",
    "    ppl_candidates = []\n",
    "    for src, samples in by_source.items():\n",
    "        proportion = len(samples) / len(deduped)\n",
    "        n = max(100, int(MAX_PPL_SAMPLES * proportion))\n",
    "        ppl_candidates.extend(random.sample(samples, min(n, len(samples))))\n",
    "    ppl_candidate_set = set(id(s) for s in ppl_candidates)\n",
    "    print(f\"  Selected {len(ppl_candidates):,} for PPL scoring\")\n",
    "else:\n",
    "    ppl_candidates = deduped\n",
    "    ppl_candidate_set = set(id(s) for s in ppl_candidates)\n",
    "    print(f\"  Scoring all {len(ppl_candidates):,} candidates\")\n",
    "\n",
    "\n",
    "def compute_perplexity(text, max_length=512):\n",
    "    \"\"\"Compute perplexity for a single text.\"\"\"\n",
    "    inputs = ppl_tokenizer(\n",
    "        text, return_tensors=\"pt\", truncation=True, max_length=max_length\n",
    "    ).to(ppl_model.device)\n",
    "    with torch.no_grad():\n",
    "        out = ppl_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    return torch.exp(out.loss).item()\n",
    "\n",
    "\n",
    "print(f\"\\nComputing perplexity for {len(ppl_candidates):,} samples...\")\n",
    "for i, s in enumerate(ppl_candidates):\n",
    "    try:\n",
    "        ppl = compute_perplexity(s[\"output\"])\n",
    "        s[\"perplexity\"] = round(ppl, 2)\n",
    "    except Exception:\n",
    "        s[\"perplexity\"] = None\n",
    "    if (i + 1) % 10000 == 0:\n",
    "        print(f\"  ...{i+1:,} / {len(ppl_candidates):,}\")\n",
    "\n",
    "# Mark unscored samples\n",
    "for s in deduped:\n",
    "    if \"perplexity\" not in s:\n",
    "        s[\"perplexity\"] = None\n",
    "\n",
    "# Stats\n",
    "scored = [s[\"perplexity\"] for s in deduped if s[\"perplexity\"] is not None]\n",
    "if scored:\n",
    "    print(f\"\\n\\u2705 Perplexity scoring complete\")\n",
    "    print(f\"   Scored: {len(scored):,} / {len(deduped):,}\")\n",
    "    print(f\"   PPL stats: min={min(scored):.1f}, median={sorted(scored)[len(scored)//2]:.1f}, max={max(scored):.1f}, mean={sum(scored)/len(scored):.1f}\")\n",
    "    print(f\"   PPL < 50: {sum(1 for p in scored if p < 50):,}\")\n",
    "    print(f\"   PPL 50-200: {sum(1 for p in scored if 50 <= p < 200):,}\")\n",
    "    print(f\"   PPL >= 200 (likely garbage): {sum(1 for p in scored if p >= 200):,}\")\n",
    "\n",
    "# Free GPU memory\n",
    "del ppl_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\u2705 PPL model unloaded, GPU memory freed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Pass 2F: Semantic Categorization (IndicSBERT + HDBSCAN)\n",
    "# Compute sentence embeddings, cluster with HDBSCAN, map to VAZHI domains.\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hdbscan\n",
    "\n",
    "print(\"Loading IndicSBERT for semantic categorization...\")\n",
    "sbert_model = SentenceTransformer(\"l3cube-pune/tamil-sentence-similarity-sbert\")\n",
    "print(f\"\\u2705 IndicSBERT loaded\")\n",
    "\n",
    "# Compute embeddings for instructions (batch processing)\n",
    "instructions = [s[\"instruction\"][:512] for s in deduped]  # Truncate for efficiency\n",
    "print(f\"Computing embeddings for {len(instructions):,} instructions...\")\n",
    "embeddings = sbert_model.encode(\n",
    "    instructions, batch_size=256, show_progress_bar=True, normalize_embeddings=True\n",
    ")\n",
    "print(f\"\\u2705 Embeddings computed: shape {embeddings.shape}\")\n",
    "\n",
    "# Cluster with HDBSCAN\n",
    "print(\"Clustering with HDBSCAN...\")\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=50,\n",
    "    min_samples=10,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    ")\n",
    "cluster_labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "noise_count = sum(1 for l in cluster_labels if l == -1)\n",
    "print(f\"\\u2705 Clustering complete: {n_clusters} clusters, {noise_count:,} noise points\")\n",
    "\n",
    "# Assign cluster labels to samples\n",
    "for i, s in enumerate(deduped):\n",
    "    s[\"embedding_cluster\"] = int(cluster_labels[i])\n",
    "    s[\"auto_category\"] = f\"cluster_{cluster_labels[i]}\" if cluster_labels[i] >= 0 else \"unclustered\"\n",
    "\n",
    "# Show top 10 clusters with sample instructions\n",
    "cluster_counts = Counter(cluster_labels)\n",
    "print(f\"\\nTop 10 clusters:\")\n",
    "for cluster_id, count in cluster_counts.most_common(11):\n",
    "    if cluster_id == -1:\n",
    "        print(f\"  noise: {count:,} samples\")\n",
    "        continue\n",
    "    cluster_samples = [s for s in deduped if s[\"embedding_cluster\"] == cluster_id]\n",
    "    example = cluster_samples[0][\"instruction\"][:80]\n",
    "    print(f\"  cluster_{cluster_id}: {count:,} samples — e.g. \\\"{example}...\\\"\")\n",
    "\n",
    "# Free GPU memory\n",
    "del sbert_model, embeddings\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n\\u2705 SBERT model unloaded, GPU memory freed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Pass 2G: Composite Quality Score + Tokenized Length\n",
    "\n",
    "from transformers import AutoTokenizer as AT\n",
    "\n",
    "print(f\"Loading tokenizer for length validation: {TOKENIZER_MODEL}\")\n",
    "sft_tokenizer = AT.from_pretrained(TOKENIZER_MODEL, trust_remote_code=True)\n",
    "print(f\"\\u2705 Tokenizer loaded (vocab size: {sft_tokenizer.vocab_size:,})\")\n",
    "\n",
    "\n",
    "def compute_quality_score(s):\n",
    "    \"\"\"Composite quality score (0-1). Higher = better.\"\"\"\n",
    "    lang_conf = s.get(\"lang_confidence\", 0.0)\n",
    "    rep = s.get(\"repetition\", 0.0)\n",
    "    tamil = s.get(\"tamil_pct\", 0.0)\n",
    "    tok_len = s.get(\"tokenized_length\", SFT_MAX_SEQ_LENGTH + 1)\n",
    "    tox = s.get(\"toxicity_flags\", [])\n",
    "\n",
    "    score = (\n",
    "        lang_conf * 0.35 +\n",
    "        (1 / (1 + rep * 10)) * 0.25 +\n",
    "        tamil * 0.20 +\n",
    "        (1.0 if tok_len <= SFT_MAX_SEQ_LENGTH else 0.0) * 0.10 +\n",
    "        (0.0 if tox else 1.0) * 0.10\n",
    "    )\n",
    "    return round(score, 4)\n",
    "\n",
    "\n",
    "print(f\"Computing tokenized lengths and quality scores for {len(deduped):,} samples...\")\n",
    "for i, s in enumerate(deduped):\n",
    "    # Tokenized length (ChatML-wrapped)\n",
    "    chatml_text = to_chatml(s[\"instruction\"], s[\"output\"])\n",
    "    tok_len = len(sft_tokenizer.encode(chatml_text, add_special_tokens=False))\n",
    "    s[\"tokenized_length\"] = tok_len\n",
    "\n",
    "    # Quality score\n",
    "    s[\"quality_score\"] = compute_quality_score(s)\n",
    "\n",
    "    if (i + 1) % 50000 == 0:\n",
    "        print(f\"  ...{i+1:,} / {len(deduped):,}\")\n",
    "\n",
    "# Stats\n",
    "tok_lengths = [s[\"tokenized_length\"] for s in deduped]\n",
    "quality_scores = [s[\"quality_score\"] for s in deduped]\n",
    "\n",
    "print(f\"\\n\\u2705 Scoring complete\")\n",
    "print(f\"   Token length: min={min(tok_lengths)}, median={sorted(tok_lengths)[len(tok_lengths)//2]}, max={max(tok_lengths)}\")\n",
    "print(f\"   Within 2048 tokens: {sum(1 for t in tok_lengths if t <= SFT_MAX_SEQ_LENGTH):,} / {len(tok_lengths):,}\")\n",
    "print(f\"   Quality score: min={min(quality_scores):.3f}, median={sorted(quality_scores)[len(quality_scores)//2]:.3f}, mean={sum(quality_scores)/len(quality_scores):.3f}\")\n",
    "print(f\"   Score >= 0.45: {sum(1 for q in quality_scores if q >= 0.45):,}\")\n",
    "print(f\"   Score < 0.45: {sum(1 for q in quality_scores if q < 0.45):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Upload curated dataset to HF\n",
    "\n",
    "# Ensure all fields are serializable\n",
    "curated_records = []\n",
    "for s in deduped:\n",
    "    curated_records.append({\n",
    "        \"instruction\": s[\"instruction\"],\n",
    "        \"output\": s[\"output\"],\n",
    "        \"source\": s[\"source\"],\n",
    "        \"subset\": s[\"subset\"],\n",
    "        \"char_length\": s[\"char_length\"],\n",
    "        \"tamil_pct\": s[\"tamil_pct\"],\n",
    "        \"lang_id\": s[\"lang_id\"],\n",
    "        \"lang_confidence\": s[\"lang_confidence\"],\n",
    "        \"heuristic_flags\": s.get(\"heuristic_flags\", []),\n",
    "        \"repetition\": s.get(\"repetition\", 0.0),\n",
    "        \"toxicity_flags\": s.get(\"toxicity_flags\", []),\n",
    "        \"is_safety_sample\": s.get(\"is_safety_sample\", False),\n",
    "        \"is_duplicate\": s.get(\"is_duplicate\", False),\n",
    "        \"perplexity\": s.get(\"perplexity\"),\n",
    "        \"embedding_cluster\": s.get(\"embedding_cluster\"),\n",
    "        \"auto_category\": s.get(\"auto_category\"),\n",
    "        \"quality_score\": s.get(\"quality_score\", 0.0),\n",
    "        \"tokenized_length\": s.get(\"tokenized_length\", 0),\n",
    "    })\n",
    "\n",
    "print(f\"\\nUploading {len(curated_records):,} curated samples to {CURATED_DATASET}...\")\n",
    "curated_ds = Dataset.from_list(curated_records)\n",
    "api.create_repo(CURATED_DATASET, repo_type=\"dataset\", exist_ok=True)\n",
    "curated_ds.push_to_hub(CURATED_DATASET)\n",
    "\n",
    "print(f\"\\n\\u2705 Curated dataset uploaded: https://huggingface.co/datasets/{CURATED_DATASET}\")\n",
    "print(f\"   Total samples: {len(curated_records):,}\")\n",
    "print(f\"   Schema: {list(curated_records[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 3: COMPOSE\n",
    "\n",
    "Select from curated pools with absolute count targets, build final SFT dataset.\n",
    "\n",
    "**Input:** In-memory curated dataset (also backed up on HF from Stage 2)  \n",
    "**Output:** `CryptoYogi/vazhi-tamil-sft-v4_1` on HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3A: Filtering\n",
    "\n",
    "print(\"Applying Stage 3 filters...\")\n",
    "df = curated_records.copy()  # Work on list of dicts\n",
    "\n",
    "before = len(df)\n",
    "\n",
    "# Hard filters\n",
    "df = [s for s in df if not s[\"is_duplicate\"]]\n",
    "print(f\"  After dedup filter: {len(df):,} (-{before - len(df):,})\")\n",
    "before2 = len(df)\n",
    "\n",
    "df = [s for s in df if s[\"tokenized_length\"] <= SFT_MAX_SEQ_LENGTH]\n",
    "print(f\"  After token length \\u2264 {SFT_MAX_SEQ_LENGTH}: {len(df):,} (-{before2 - len(df):,})\")\n",
    "before3 = len(df)\n",
    "\n",
    "df = [s for s in df if s[\"lang_id\"] == \"ta\"]\n",
    "print(f\"  After lang_id == ta: {len(df):,} (-{before3 - len(df):,})\")\n",
    "before4 = len(df)\n",
    "\n",
    "df = [s for s in df if len(s[\"heuristic_flags\"]) == 0]\n",
    "print(f\"  After clean heuristics: {len(df):,} (-{before4 - len(df):,})\")\n",
    "before5 = len(df)\n",
    "\n",
    "# Toxicity: exclude flagged UNLESS safety sample\n",
    "df = [s for s in df if len(s[\"toxicity_flags\"]) == 0 or s[\"is_safety_sample\"]]\n",
    "print(f\"  After toxicity filter: {len(df):,} (-{before5 - len(df):,})\")\n",
    "before6 = len(df)\n",
    "\n",
    "# Soft quality filter\n",
    "df = [s for s in df if s[\"quality_score\"] >= 0.45]\n",
    "print(f\"  After quality \\u2265 0.45: {len(df):,} (-{before6 - len(df):,})\")\n",
    "before7 = len(df)\n",
    "\n",
    "# PPL garbage filter (only for scored samples)\n",
    "df = [s for s in df if s[\"perplexity\"] is None or s[\"perplexity\"] < 200]\n",
    "print(f\"  After PPL < 200: {len(df):,} (-{before7 - len(df):,})\")\n",
    "\n",
    "print(f\"\\n\\u2705 Filtering complete: {before:,} \\u2192 {len(df):,} ({before - len(df):,} removed)\")\n",
    "\n",
    "# Per-source counts after filtering\n",
    "filtered_sources = Counter(s[\"source\"] for s in df)\n",
    "print(f\"\\nFiltered pool by source:\")\n",
    "for src, count in filtered_sources.most_common():\n",
    "    print(f\"  {src}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3B: Composition with Absolute Count Targets\n",
    "# Each source has independent min/max — no anchoring, no cascading.\n",
    "\n",
    "# Separate safety samples from source pools\n",
    "safety_pool = [s for s in df if s[\"is_safety_sample\"]]\n",
    "non_safety = [s for s in df if not s[\"is_safety_sample\"]]\n",
    "\n",
    "# Build source pools\n",
    "source_pools = {}\n",
    "for s in non_safety:\n",
    "    source_pools.setdefault(s[\"source\"], []).append(s)\n",
    "source_pools[\"safety\"] = safety_pool\n",
    "\n",
    "print(\"Composing final dataset with absolute count targets...\")\n",
    "composed = {}\n",
    "total_composed = 0\n",
    "\n",
    "for bucket_name, targets in BUCKET_TARGETS.items():\n",
    "    pool = source_pools.get(bucket_name, [])\n",
    "    target = targets[\"target\"]\n",
    "    min_count = targets[\"min\"]\n",
    "    max_count = targets[\"max\"]\n",
    "\n",
    "    if len(pool) < min_count:\n",
    "        print(f\"  \\u26a0\\ufe0f {bucket_name}: only {len(pool):,} available, min is {min_count}\")\n",
    "        # Use all available — warn but don't fail\n",
    "        selected = pool\n",
    "    elif len(pool) <= target:\n",
    "        # Pool smaller than target: use all\n",
    "        selected = pool\n",
    "    else:\n",
    "        # Pool larger than target: sample down (capped at max)\n",
    "        use_count = min(target, max_count)\n",
    "        # Sort by quality_score descending, take top N\n",
    "        pool_sorted = sorted(pool, key=lambda x: x[\"quality_score\"], reverse=True)\n",
    "        selected = pool_sorted[:use_count]\n",
    "\n",
    "    composed[bucket_name] = selected\n",
    "    total_composed += len(selected)\n",
    "    print(f\"  {bucket_name}: {len(selected):,} / {len(pool):,} available (target: {target}, range: {min_count}-{max_count})\")\n",
    "\n",
    "print(f\"\\n\\u2705 Composition complete: {total_composed:,} total samples\")\n",
    "\n",
    "# Verify minimums\n",
    "all_met = True\n",
    "for bucket_name, targets in BUCKET_TARGETS.items():\n",
    "    actual = len(composed.get(bucket_name, []))\n",
    "    if actual < targets[\"min\"]:\n",
    "        print(f\"  \\u274c {bucket_name}: {actual} < min {targets['min']}\")\n",
    "        all_met = False\n",
    "if all_met:\n",
    "    print(\"\\u2705 All bucket minimums met\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3C: ChatML Conversion + Validation\n",
    "\n",
    "all_samples = []\n",
    "chatml_failures = 0\n",
    "\n",
    "for bucket_name, samples in composed.items():\n",
    "    for s in samples:\n",
    "        # Apply anti-memorization filter for Thirukkural (vazhi_packs only)\n",
    "        if s[\"source\"] == \"vazhi_packs\" and is_verbatim_kural_qa(s[\"instruction\"], s[\"output\"]):\n",
    "            continue\n",
    "\n",
    "        text = to_chatml(s[\"instruction\"], s[\"output\"])\n",
    "\n",
    "        # Strict ChatML validation\n",
    "        valid, reason = validate_chatml_strict(text)\n",
    "        if not valid:\n",
    "            chatml_failures += 1\n",
    "            continue\n",
    "\n",
    "        all_samples.append({\n",
    "            \"text\": text,\n",
    "            \"bucket\": bucket_name,\n",
    "            \"source\": s[\"source\"],\n",
    "            \"subset\": s[\"subset\"],\n",
    "            \"quality_score\": s[\"quality_score\"],\n",
    "            \"tokenized_length\": s[\"tokenized_length\"],\n",
    "        })\n",
    "\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "print(f\"\\u2705 ChatML conversion complete\")\n",
    "print(f\"   Valid samples: {len(all_samples):,}\")\n",
    "print(f\"   ChatML failures: {chatml_failures}\")\n",
    "\n",
    "# Bucket distribution\n",
    "bucket_counts = Counter(s[\"bucket\"] for s in all_samples)\n",
    "print(f\"\\n\\U0001f4ca Final bucket distribution:\")\n",
    "for bucket, count in sorted(bucket_counts.items()):\n",
    "    pct = 100 * count / len(all_samples)\n",
    "    print(f\"  {bucket}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3D: Stratified Train/Eval Split (90/10 by bucket)\n",
    "\n",
    "EVAL_RATIO = 0.10\n",
    "train_samples = []\n",
    "eval_samples = []\n",
    "\n",
    "by_bucket = {}\n",
    "for s in all_samples:\n",
    "    by_bucket.setdefault(s[\"bucket\"], []).append(s)\n",
    "\n",
    "for bucket, samples in by_bucket.items():\n",
    "    random.shuffle(samples)\n",
    "    n_eval = max(1, int(len(samples) * EVAL_RATIO))\n",
    "    eval_samples.extend(samples[:n_eval])\n",
    "    train_samples.extend(samples[n_eval:])\n",
    "\n",
    "random.shuffle(train_samples)\n",
    "random.shuffle(eval_samples)\n",
    "\n",
    "print(f\"\\U0001f4ca Stratified split:\")\n",
    "print(f\"  Train: {len(train_samples):,}\")\n",
    "print(f\"  Eval:  {len(eval_samples):,}\")\n",
    "print(f\"  Eval ratio: {len(eval_samples) / (len(train_samples) + len(eval_samples)):.1%}\")\n",
    "\n",
    "# Verify eval has all buckets\n",
    "eval_buckets = Counter(s[\"bucket\"] for s in eval_samples)\n",
    "print(f\"\\n  Eval bucket distribution:\")\n",
    "for bucket, count in sorted(eval_buckets.items()):\n",
    "    print(f\"    {bucket}: {count}\")\n",
    "\n",
    "# Verify all samples within token limit\n",
    "max_tok = max(s[\"tokenized_length\"] for s in all_samples)\n",
    "print(f\"\\n  Max tokenized length: {max_tok} (limit: {SFT_MAX_SEQ_LENGTH})\")\n",
    "assert max_tok <= SFT_MAX_SEQ_LENGTH, f\"Token length violation: {max_tok} > {SFT_MAX_SEQ_LENGTH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3E: Upload to HuggingFace + Summary\n",
    "\n",
    "train_ds = Dataset.from_list(train_samples)\n",
    "eval_ds = Dataset.from_list(eval_samples)\n",
    "dataset_dict = DatasetDict({\"train\": train_ds, \"validation\": eval_ds})\n",
    "\n",
    "api.create_repo(OUTPUT_DATASET, repo_type=\"dataset\", exist_ok=True)\n",
    "dataset_dict.push_to_hub(OUTPUT_DATASET)\n",
    "\n",
    "print(f\"\\n\\u2705 Dataset uploaded: https://huggingface.co/datasets/{OUTPUT_DATASET}\")\n",
    "print(f\"   Train: {len(train_ds):,} samples\")\n",
    "print(f\"   Validation: {len(eval_ds):,} samples\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"VAZHI Dataset Factory v{VERSION} \\u2014 COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Stage 1 (Retrieve): {len(raw_samples):,} raw samples \\u2192 {RAW_DATASET}\")\n",
    "print(f\"  Stage 2 (Curate):   {len(curated_records):,} curated samples \\u2192 {CURATED_DATASET}\")\n",
    "print(f\"  Stage 3 (Compose):  {len(all_samples):,} final SFT samples \\u2192 {OUTPUT_DATASET}\")\n",
    "print(f\"\\n  Train: {len(train_samples):,} | Eval: {len(eval_samples):,}\")\n",
    "print(f\"  max_seq_length: {SFT_MAX_SEQ_LENGTH}\")\n",
    "\n",
    "print(f\"\\n  Bucket composition:\")\n",
    "for bucket, count in sorted(bucket_counts.items()):\n",
    "    target = BUCKET_TARGETS[bucket]\n",
    "    status = \"\\u2705\" if count >= target[\"min\"] else \"\\u26a0\\ufe0f\"\n",
    "    print(f\"    {status} {bucket}: {count:,} (target: {target['target']}, range: {target['min']}-{target['max']})\")\n",
    "\n",
    "# Sample outputs\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"Sample outputs (2 per bucket):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "shown = Counter()\n",
    "for s in all_samples:\n",
    "    if shown[s['bucket']] < 2:\n",
    "        shown[s['bucket']] += 1\n",
    "        print(f\"\\n[{s['bucket'].upper()}] source={s['source']} quality={s['quality_score']:.3f}\")\n",
    "        match = CHATML_PATTERN.search(s[\"text\"])\n",
    "        if match:\n",
    "            print(f\"  Q: {match.group(1)[:100]}\")\n",
    "            print(f\"  A: {match.group(2)[:150]}\")\n",
    "    if all(shown[b] >= 2 for b in BUCKET_TARGETS):\n",
    "        break\n",
    "\n",
    "print(f\"\\n\\u2705 Dataset Factory v{VERSION} complete!\")\n",
    "print(f\"   Next step: SFT training with conservative LoRA (r=8, q_proj+v_proj, 2 epochs)\")\n",
    "print(f\"   Base model: {DAPT_MODEL}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
