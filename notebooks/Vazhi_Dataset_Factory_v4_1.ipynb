{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# VAZHI Dataset Factory v4.1 — 3-Stage Data Pipeline\n\n**Pipeline:** Retrieve → Curate → Compose\n\n```\n┌─ Stage 1: RETRIEVE (CPU, ~15 min) ──────────────────────────────┐\n│ IndicAlign (5 verified subsets, capped) + local sources          │\n│ → Upload raw to HF: CryptoYogi/vazhi-raw-tamil-qa-v1            │\n├─ Stage 2: CURATE (CPU then GPU, ~2-3 hours) ───────────────────┤\n│ Pass 1 (CPU): lang-id → tamil% → empties → MinHash dedup        │\n│ Pass 2 (GPU): perplexity + IndicSBERT on candidate subset       │\n│ → Upload curated to HF: CryptoYogi/vazhi-curated-tamil-qa-v1    │\n├─ Stage 3: COMPOSE (CPU, ~5 min) ────────────────────────────────┤\n│ Filter (quality + dedup + toxicity + token-length ≤ 2048)        │\n│ → ChatML conversion → absolute count targets → stratified split  │\n│ → Upload final SFT to HF: CryptoYogi/vazhi-tamil-sft-v4_1      │\n└──────────────────────────────────────────────────────────────────┘\n```\n\n**Sources (8 total):**\n- **IndicAlign** (5 subsets): Dolly_T, WikiHow, Indic_ShareLlama, HHRLHF_T, Toxic_Matrix\n- **Local**: vazhi-packs (6 domain packs), handcrafted (guardrails), general (LEGACY dialects)\n- **Dropped**: tamil-orca (misaligned Q&A), GSM8K_TAMIL (irrelevant math), OpenAssistant_T (world knowledge), Anudesh (unverified), Wiki_Chat (unverified), Wiki_Conv (OOM on Kaggle)\n\n**Key design decisions:**\n- **Focused retrieval** — only verified, VAZHI-relevant sources (~29K retrieval, 2x oversampling)\n- **ML-based curation** — fasttext lang-id, MinHash dedup, perplexity scoring, semantic clustering\n- **Absolute count targets** — no percentage-based anchoring that caused cascading downsampling\n- **max_seq_length=2048** — stops the 74% domain pack rejection caused by 1024 window\n- **Safety routing** — HHRLHF_T/Toxic_Matrix refusal pairs routed to safety bucket, not filtered\n- **HF checkpointing** — each stage uploads to HF before continuing\n\n**Run on:** Kaggle P100 (GPU needed for perplexity + embeddings in Stage 2)\n\n**Target:** ~17.6K SFT samples (range 14.4K-20.4K) in `CryptoYogi/vazhi-tamil-sft-v4_1`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Config & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stage 1 needs only: datasets, huggingface_hub (pre-installed on Kaggle)\n# Heavy packages (fasttext, sentence-transformers, hdbscan) deferred to Stage 2 cell\n!pip install -q datasets huggingface_hub\n\nimport json\nimport os\nimport re\nimport gc\nimport random\nimport hashlib\nimport subprocess\nfrom collections import Counter\nfrom pathlib import Path\n\nimport numpy as np\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom huggingface_hub import login, HfApi\n\n# === CONFIG ===\nVERSION = \"4.1\"\nRAW_DATASET = \"CryptoYogi/vazhi-raw-tamil-qa-v1\"\nCURATED_DATASET = \"CryptoYogi/vazhi-curated-tamil-qa-v1\"\nOUTPUT_DATASET = \"CryptoYogi/vazhi-tamil-sft-v4_1\"\nDAPT_MODEL = \"CryptoYogi/qwen3-0.6b-tamil-v1_1\"\nTOKENIZER_MODEL = \"Qwen/Qwen3-0.6B\"\nREPO_URL = \"https://github.com/CryptoYogiLLC/vazhi.git\"\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\nSFT_MAX_SEQ_LENGTH = 2048  # Training window — stops domain pack rejection\n\n# Disk-backed storage to avoid OOM on large datasets\nRAW_JSONL = Path(\"/kaggle/working/raw_samples.jsonl\")\n\nSYSTEM_PROMPT = (\n    \"\\u0ba8\\u0bc0\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd VAZHI (\\u0bb5\\u0bb4\\u0bbf), \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd \\u0bae\\u0b95\\u0bcd\\u0b95\\u0bb3\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bbe\\u0ba9 AI \\u0b89\\u0ba4\\u0bb5\\u0bbf\\u0baf\\u0bbe\\u0bb3\\u0bb0\\u0bcd. \"\n    \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bbf\\u0bb2\\u0bcd \\u0ba4\\u0bc6\\u0bb3\\u0bbf\\u0bb5\\u0bbe\\u0b95\\u0bb5\\u0bc1\\u0bae\\u0bcd \\u0b89\\u0ba4\\u0bb5\\u0bbf\\u0baf\\u0bbe\\u0b95\\u0bb5\\u0bc1\\u0bae\\u0bcd \\u0baa\\u0ba4\\u0bbf\\u0bb2\\u0bb3\\u0bbf\\u0baf\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd. \"\n    '\\u0ba4\\u0bc6\\u0bb0\\u0bbf\\u0baf\\u0bbe\\u0bb5\\u0bbf\\u0b9f\\u0bcd\\u0b9f\\u0bbe\\u0bb2\\u0bcd \"\\u0ba4\\u0bc6\\u0bb0\\u0bbf\\u0baf\\u0bb5\\u0bbf\\u0bb2\\u0bcd\\u0bb2\\u0bc8\" \\u0b8e\\u0ba9\\u0bcd\\u0bb1\\u0bc1 \\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd.'\n)\n\n# Absolute count targets per source bucket\n# Sources: 5 IndicAlign subsets + 3 local. ~17.6K target, ~20K max.\n# Dropped: tamil-orca (misaligned Q&A), GSM8K_TAMIL (irrelevant math),\n#   OpenAssistant_T (world knowledge), Anudesh (unverified), Wiki_Chat (unverified),\n#   Wiki_Conv (OOM on Kaggle — massive conversation rows crash streaming)\nBUCKET_TARGETS = {\n    \"vazhi_packs\":    {\"min\": 2500, \"target\": 3000, \"max\": 3000},\n    \"handcrafted\":    {\"min\": 100,  \"target\": 147,  \"max\": 200},\n    \"general\":        {\"min\": 300,  \"target\": 500,  \"max\": 700},\n    \"indicalign\":     {\"min\": 10000, \"target\": 12000, \"max\": 14000},\n    \"safety\":         {\"min\": 1500, \"target\": 2000,  \"max\": 2500},\n}\n\n# Per-subset retrieval caps (2-3x of bucket targets)\n# Total retrieval ~29K (5 IndicAlign subsets + ~6K local)\nRETRIEVAL_CAPS = {\n    \"WikiHow\": 8000,\n    \"Dolly_T\": 6000,\n    \"Indic_ShareLlama\": 5000,\n    \"HHRLHF_T\": 4000,       # routes to safety bucket\n    \"Toxic_Matrix\": 3000,   # routes to safety bucket\n}\n\n# Source priority for dedup (higher = keep)\nSOURCE_PRIORITY = {\n    \"vazhi_packs\": 10, \"handcrafted\": 10,\n    \"general\": 5,\n    \"indicalign\": 2,\n}\n\n\ndef flush_jsonl(filepath, batch, mode=\"a\"):\n    \"\"\"Write a batch of dicts to JSONL on disk.\"\"\"\n    with open(filepath, mode, encoding=\"utf-8\") as f:\n        for sample in batch:\n            f.write(json.dumps(sample, ensure_ascii=False) + \"\\n\")\n\n\ndef print_memory():\n    \"\"\"Print memory usage (Linux/Kaggle).\"\"\"\n    try:\n        with open(\"/proc/meminfo\") as f:\n            lines = f.readlines()\n        total = int(lines[0].split()[1]) / 1e6  # GB\n        avail = int(lines[2].split()[1]) / 1e6  # GB\n        used = total - avail\n        print(f\"  RAM: {used:.1f}GB / {total:.1f}GB ({100*used/total:.0f}%)\")\n    except Exception:\n        pass\n\n\nprint(f\"\\u2705 Config loaded: Dataset Factory v{VERSION}\")\nprint(f\"   Raw output: {RAW_DATASET}\")\nprint(f\"   Curated output: {CURATED_DATASET}\")\nprint(f\"   Final SFT output: {OUTPUT_DATASET}\")\nprint(f\"   max_seq_length: {SFT_MAX_SEQ_LENGTH}\")\nprint(f\"   Disk buffer: {RAW_JSONL}\")\nprint(f\"\\n   Bucket targets (absolute counts):\")\ntarget_total = sum(v[\"target\"] for v in BUCKET_TARGETS.values())\nfor name, cfg in BUCKET_TARGETS.items():\n    print(f\"     {name}: {cfg['target']} (range {cfg['min']}-{cfg['max']})\")\nprint(f\"   Expected total: ~{target_total}\")\nprint(f\"\\n   Retrieval caps per IndicAlign subset:\")\ncap_total = sum(RETRIEVAL_CAPS.values())\nfor name, cap in RETRIEVAL_CAPS.items():\n    print(f\"     {name}: {cap:,}\")\nprint(f\"   IndicAlign retrieval total: ~{cap_total:,}\")\nprint_memory()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace login\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "    login(token=hf_token)\n",
    "    print(\"\\u2705 Logged in via Kaggle secrets\")\n",
    "except Exception:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        hf_token = userdata.get('HF_TOKEN')\n",
    "        login(token=hf_token)\n",
    "        print(\"\\u2705 Logged in via Colab secrets\")\n",
    "    except Exception:\n",
    "        login()\n",
    "        print(\"\\u2705 Logged in interactively\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Stage 1: RETRIEVE\n\nPull Tamil Q&A from 5 verified IndicAlign subsets (capped at 2-3x) + 3 local sources. Minimal filtering (only reject empty/null). Store with source metadata for downstream curation.\n\n**Sources:** Dolly_T, WikiHow, Indic_ShareLlama, HHRLHF_T, Toxic_Matrix + vazhi-packs, handcrafted, general  \n**Output:** `CryptoYogi/vazhi-raw-tamil-qa-v1` on HuggingFace"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone only data/ and vazhi-packs/ from the public repo (sparse checkout)\n",
    "REPO_DIR = Path(\"/kaggle/working/vazhi\")\n",
    "if not REPO_DIR.exists():\n",
    "    print(f\"Cloning {REPO_URL} (sparse: data/ + vazhi-packs/)...\")\n",
    "    subprocess.run(\n",
    "        [\"git\", \"clone\", \"--depth\", \"1\", \"--filter=blob:none\", \"--sparse\", REPO_URL, str(REPO_DIR)],\n",
    "        check=True,\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\"git\", \"sparse-checkout\", \"set\", \"data/\", \"vazhi-packs/\"],\n",
    "        cwd=str(REPO_DIR),\n",
    "        check=True,\n",
    "    )\n",
    "    print(f\"\\u2705 Cloned to {REPO_DIR}\")\n",
    "else:\n",
    "    subprocess.run([\"git\", \"pull\"], cwd=str(REPO_DIR), check=True)\n",
    "    print(f\"\\u2705 Repo already at {REPO_DIR}, pulled latest\")\n",
    "\n",
    "SOURCES_DIR = REPO_DIR / \"data\" / \"sources\"\n",
    "LEGACY_DIR = REPO_DIR / \"data\" / \"LEGACY\"\n",
    "PACKS_DIR = SOURCES_DIR / \"sft\" / \"vazhi-packs\"\n",
    "HANDCRAFTED_DIR = SOURCES_DIR / \"sft\" / \"handcrafted\"\n",
    "\n",
    "for label, d in [(\"sources/sft/vazhi-packs\", PACKS_DIR), (\"sources/sft/handcrafted\", HANDCRAFTED_DIR), (\"LEGACY\", LEGACY_DIR)]:\n",
    "    assert d.exists(), f\"Missing: {d}\"\n",
    "    print(f\"  \\u2705 {label}: {d}\")\n",
    "print(f\"\\n\\u2705 All source data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions (reused from v4.0 + new additions)\n",
    "\n",
    "def to_chatml(instruction, output, system_prompt=None):\n",
    "    \"\"\"Convert instruction/output pair to strict ChatML format.\"\"\"\n",
    "    sp = system_prompt or SYSTEM_PROMPT\n",
    "    return (\n",
    "        f\"<|im_start|>system\\n{sp}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{instruction}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n{output}<|im_end|>\"\n",
    "    )\n",
    "\n",
    "\n",
    "CHATML_PATTERN = re.compile(\n",
    "    r'<\\|im_start\\|>system\\n.+?<\\|im_end\\|>\\n'\n",
    "    r'<\\|im_start\\|>user\\n(.+?)<\\|im_end\\|>\\n'\n",
    "    r'<\\|im_start\\|>assistant\\n(.+?)<\\|im_end\\|>',\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "\n",
    "def validate_chatml_strict(text):\n",
    "    \"\"\"Validate a sample has proper ChatML with non-empty user AND assistant.\"\"\"\n",
    "    match = CHATML_PATTERN.search(text)\n",
    "    if not match:\n",
    "        return False, \"no ChatML structure found\"\n",
    "    user_content = match.group(1).strip()\n",
    "    assistant_content = match.group(2).strip()\n",
    "    if len(user_content) < 2:\n",
    "        return False, \"empty user content\"\n",
    "    if len(assistant_content) < 2:\n",
    "        return False, \"empty assistant content\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "\n",
    "def count_tamil_chars(text):\n",
    "    \"\"\"Count Tamil Unicode characters (U+0B80 to U+0BFF).\"\"\"\n",
    "    return sum(1 for c in text if '\\u0b80' <= c <= '\\u0bff')\n",
    "\n",
    "\n",
    "def tamil_char_pct(text):\n",
    "    \"\"\"Get Tamil character percentage (0.0-1.0).\"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    return count_tamil_chars(text) / len(text)\n",
    "\n",
    "\n",
    "def is_verbatim_kural_qa(question, answer):\n",
    "    \"\"\"Reject Q&As that ask for exact verse text.\"\"\"\n",
    "    verbatim_patterns = [\n",
    "        r'\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*\\d+\\s*(\\u0b8e\\u0ba9\\u0bcd\\u0ba9|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd|\\u0b8e\\u0bb4\\u0bc1\\u0ba4\\u0bbf\\s*\\u0b95\\u0bbe\\u0b9f\\u0bcd\\u0b9f\\u0bc1|\\u0b95\\u0bc2\\u0bb1\\u0bc1\\u0b95)',\n",
    "        r'(first|\\u0bae\\u0bc1\\u0ba4\\u0bb2\\u0bcd)\\s*\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*(\\u0b8e\\u0ba9\\u0bcd\\u0ba9|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd)',\n",
    "        r'\\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bbf\\u0ba9\\u0bcd\\s+\\u0bae\\u0bc1\\u0ba4\\u0bb2\\u0bcd\\s+\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd',\n",
    "        r'\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*[\\d]+(?:\\s*\\u0b90)?\\s*\\u0b8e\\u0bb4\\u0bc1\\u0ba4\\u0bbf',\n",
    "    ]\n",
    "    for pat in verbatim_patterns:\n",
    "        if re.search(pat, question, re.IGNORECASE):\n",
    "            return True\n",
    "    if len(answer) < 200 and \"\\n\" in answer and not any(\n",
    "        w in answer for w in [\"\\u0bb5\\u0bbf\\u0bb3\\u0b95\\u0bcd\\u0b95\\u0bae\\u0bcd\", \"\\u0baa\\u0bca\\u0bb0\\u0bc1\\u0bb3\\u0bcd\", \"\\u0b85\\u0bb0\\u0bcd\\u0ba4\\u0bcd\\u0ba4\\u0bae\\u0bcd\"]\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Self-test\n",
    "good = to_chatml(\"test question\", \"test answer\")\n",
    "valid, reason = validate_chatml_strict(good)\n",
    "assert valid, f\"Self-test failed: {reason}\"\n",
    "assert 0.0 <= tamil_char_pct(\"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd test\") <= 1.0\n",
    "print(\"\\u2705 Helper functions defined and self-tested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stage 1A: Load IndicAlign — 5 verified Tamil subsets (capped)\n# Dropped: Anudesh (unverified), Wiki_Chat (unverified), OpenAssistant_T (irrelevant),\n#   Wiki_Conv (OOM — massive conversation rows crash Kaggle kernel during streaming)\n# Each subset capped at 2-3x of bucket target to avoid over-retrieval.\n\nimport shutil\n\nINDICALIGN_SUBSETS = [\n    \"Dolly_T\", \"WikiHow\",\n    \"Indic_ShareLlama\", \"HHRLHF_T\", \"Toxic_Matrix\",\n]\n\n# Clear JSONL file for fresh run\nRAW_JSONL.unlink(missing_ok=True)\ntotal_raw = 0  # Global counter across all Stage 1 sources\n\nprint(\"Loading IndicAlign subsets (streaming, capped)...\")\nprint_memory()\nindicalign_total = 0\n\nfor subset in INDICALIGN_SUBSETS:\n    cap = RETRIEVAL_CAPS.get(subset, float(\"inf\"))\n    print(f\"\\n  Starting {subset} (cap={cap:,})...\")\n    print_memory()\n\n    try:\n        ds = load_dataset(\n            \"ai4bharat/indic-align\",\n            subset,\n            split=\"train\",\n            streaming=True,\n        )\n\n        subset_count = 0\n        batch = []\n\n        for item in ds:\n            if subset_count >= cap:\n                break\n\n            pairs = item.get(\"tam_Taml\", [])\n            if not pairs or not isinstance(pairs, (list, tuple)):\n                continue\n\n            for pair in pairs:\n                if subset_count >= cap:\n                    break\n                if not isinstance(pair, (list, tuple)) or len(pair) < 2:\n                    continue\n\n                instruction = str(pair[0]).strip() if pair[0] else \"\"\n                output = str(pair[1]).strip() if pair[1] else \"\"\n\n                # Minimal filter: reject empty/null only\n                if not instruction or not output:\n                    continue\n\n                batch.append({\n                    \"instruction\": instruction,\n                    \"output\": output,\n                    \"source\": \"indicalign\",\n                    \"subset\": subset,\n                })\n                subset_count += 1\n\n            # Flush batch every 5K to disk (smaller batches = less RAM)\n            if len(batch) >= 5000:\n                flush_jsonl(RAW_JSONL, batch)\n                batch = []\n\n        # Flush remaining\n        if batch:\n            flush_jsonl(RAW_JSONL, batch)\n            batch = []\n\n        indicalign_total += subset_count\n        total_raw += subset_count\n        status = f\"(capped at {cap:,})\" if subset_count >= cap else \"\"\n        print(f\"  {subset}: {subset_count:,} pairs {status}\")\n\n    except Exception as e:\n        print(f\"  {subset}: FAILED - {e}\")\n\n    # Aggressive cleanup between subsets\n    try:\n        del ds\n    except NameError:\n        pass\n    gc.collect()\n\n    # Clear HF datasets cache to free disk/memory\n    hf_cache = os.path.expanduser(\"~/.cache/huggingface/datasets/ai4bharat___indic-align\")\n    if os.path.exists(hf_cache):\n        shutil.rmtree(hf_cache, ignore_errors=True)\n\n    print_memory()\n\nprint(f\"\\n\\u2705 IndicAlign total: {indicalign_total:,} pairs\")\nprint(f\"   Running total on disk: {total_raw:,}\")\nprint_memory()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stage 1B: Load local sources (vazhi-packs, handcrafted, general)\n# These are JSON files from the sparse-checked-out repo (cell-5).\n\nprint(\"Loading local sources from cloned repo...\")\nprint_memory()\nlocal_total = 0\n\ndef load_json_pairs(directory, source_name):\n    \"\"\"Load instruction/output pairs from all JSON files in a directory.\"\"\"\n    count = 0\n    batch = []\n    for json_file in sorted(directory.glob(\"*.json\")):\n        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        if not isinstance(data, list):\n            print(f\"    Skipping {json_file.name} (not a list)\")\n            continue\n        for item in data:\n            instr = str(item.get(\"instruction\", \"\")).strip()\n            out = str(item.get(\"output\", \"\")).strip()\n            if not instr or not out:\n                continue\n            batch.append({\n                \"instruction\": instr,\n                \"output\": out,\n                \"source\": source_name,\n                \"subset\": json_file.stem,\n            })\n            count += 1\n    if batch:\n        flush_jsonl(RAW_JSONL, batch)\n    return count\n\n# vazhi-packs (6 domain packs: culture, education, govt, healthcare, legal, security)\npack_count = load_json_pairs(PACKS_DIR, \"vazhi_packs\")\ntotal_raw += pack_count\nlocal_total += pack_count\nprint(f\"  vazhi-packs: {pack_count:,} pairs\")\n\n# handcrafted (guardrails, refusal, brevity, greeting)\nhc_count = load_json_pairs(HANDCRAFTED_DIR, \"handcrafted\")\ntotal_raw += hc_count\nlocal_total += hc_count\nprint(f\"  handcrafted: {hc_count:,} pairs\")\n\n# general (LEGACY dialects, thirukkural, etc.)\ngen_count = load_json_pairs(LEGACY_DIR, \"general\")\ntotal_raw += gen_count\nlocal_total += gen_count\nprint(f\"  general: {gen_count:,} pairs\")\n\nprint(f\"\\n\\u2705 Local sources total: {local_total:,} pairs\")\nprint(f\"   Running total on disk: {total_raw:,}\")\nprint_memory()\ngc.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stage 1E: Load JSONL as Arrow-backed HF Dataset + compute tamil_pct and char_length\n# Arrow-backed datasets stay on disk, avoiding OOM on 520K+ samples.\n\nprint(f\"Loading {total_raw:,} samples from JSONL into Arrow-backed Dataset...\")\nraw_ds = load_dataset(\"json\", data_files=str(RAW_JSONL), split=\"train\")\nprint(f\"  Loaded: {len(raw_ds):,} samples (Arrow-backed, low RAM)\")\n\n\ndef add_computed_fields(example):\n    \"\"\"Add char_length and tamil_pct to each sample.\"\"\"\n    combined = example[\"instruction\"] + example[\"output\"]\n    example[\"char_length\"] = len(combined)\n    example[\"tamil_pct\"] = round(tamil_char_pct(combined), 4)\n    return example\n\n\nraw_ds = raw_ds.map(add_computed_fields, desc=\"Computing tamil_pct & char_length\")\n\nprint(f\"\\u2705 Schema normalization complete\")\nprint(f\"   Columns: {raw_ds.column_names}\")\nprint(f\"   Samples: {len(raw_ds):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stage 1F: Stats, spot-check, and upload raw dataset to HF\n\nprint(\"=\" * 60)\nprint(f\"STAGE 1 COMPLETE: {len(raw_ds):,} raw samples\")\nprint(\"=\" * 60)\n\n# Per-source counts\nsource_counts = Counter(raw_ds[\"source\"])\nprint(\"\\nPer-source counts:\")\nfor src, count in source_counts.most_common():\n    print(f\"  {src}: {count:,}\")\n\n# Per-subset counts for IndicAlign\nindicalign_subset = raw_ds.filter(lambda x: x[\"source\"] == \"indicalign\")\nsubset_counts = Counter(indicalign_subset[\"subset\"])\nprint(\"\\nIndicAlign subset breakdown:\")\nfor sub, count in subset_counts.most_common():\n    print(f\"  {sub}: {count:,}\")\ndel indicalign_subset\n\n# Length distribution\nlengths = raw_ds[\"char_length\"]\nlengths_sorted = sorted(lengths)\nprint(f\"\\nLength stats: min={min(lengths)}, median={lengths_sorted[len(lengths)//2]}, max={max(lengths)}, mean={sum(lengths)/len(lengths):.0f}\")\n\n# Tamil % distribution\ntamil_pcts = raw_ds[\"tamil_pct\"]\ntamil_sorted = sorted(tamil_pcts)\nprint(f\"Tamil% stats: min={min(tamil_pcts):.2f}, median={tamil_sorted[len(tamil_pcts)//2]:.2f}, mean={sum(tamil_pcts)/len(tamil_pcts):.2f}\")\n\n# Spot-check: 3 random samples from each source\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Spot-check (3 per source):\")\nprint(\"=\" * 60)\nfor src in sorted(source_counts.keys()):\n    src_ds = raw_ds.filter(lambda x, s=src: x[\"source\"] == s)\n    indices = random.sample(range(len(src_ds)), min(3, len(src_ds)))\n    print(f\"\\n[{src.upper()}]\")\n    for idx in indices:\n        p = src_ds[idx]\n        print(f\"  Q: {p['instruction'][:80]}...\")\n        print(f\"  A: {p['output'][:80]}...\")\n        print(f\"  tamil%={p['tamil_pct']:.2f} len={p['char_length']}\")\n    del src_ds\n\n# Upload to HuggingFace\nprint(f\"\\nUploading {len(raw_ds):,} raw samples to {RAW_DATASET}...\")\napi = HfApi()\napi.create_repo(RAW_DATASET, repo_type=\"dataset\", exist_ok=True)\nraw_ds.push_to_hub(RAW_DATASET)\nprint(f\"\\u2705 Raw dataset uploaded: https://huggingface.co/datasets/{RAW_DATASET}\")\n\n# Free temp variables, keep raw_ds for Stage 2\ndel lengths, lengths_sorted, tamil_pcts, tamil_sorted\ngc.collect()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 2: CURATE\n",
    "\n",
    "Two-pass curation: cheap CPU filters first to reduce the pool, then GPU-based scoring on the surviving candidates.\n",
    "\n",
    "**Input:** In-memory raw dataset (also backed up on HF from Stage 1)  \n",
    "**Output:** `CryptoYogi/vazhi-curated-tamil-qa-v1` on HuggingFace\n",
    "\n",
    "### Pass 1 (CPU): Language detection → heuristics → dedup → toxicity  \n",
    "### Pass 2 (GPU): Perplexity scoring → semantic categorization → composite quality score"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Install Stage 2 dependencies (deferred from cell-2 to reduce Stage 1 RAM usage)\n# These packages are large (~1GB+) and only needed for curation, not retrieval.\n!pip install -q fasttext-wheel sentence-transformers hdbscan\n\nprint(\"\\u2705 Stage 2 dependencies installed\")\nprint_memory()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stage 2 — Pass 1A: Language Detection with fasttext lid.176.bin\n# Note: fasttext has a NumPy 2.x incompatibility (np.array copy=False).\n# We monkey-patch fasttext.FastText.predict to use np.asarray instead.\n\nimport fasttext\nimport fasttext.FastText\nimport urllib.request\n\n# --- Monkey-patch for NumPy 2.x compatibility ---\n_original_predict = fasttext.FastText.FastText.predict\n\ndef _patched_predict(self, text, k=1, threshold=0.0, on_unicode_error=\"strict\"):\n    \"\"\"Patched predict that works with NumPy 2.x.\"\"\"\n    import numpy as _np\n    # Call the C++ binding directly\n    if isinstance(text, str):\n        text = [text]\n        was_single = True\n    else:\n        was_single = False\n\n    all_labels = []\n    all_probs = []\n    for t in text:\n        try:\n            labels, probs = self.f.predict(t, k, threshold, on_unicode_error)\n        except Exception:\n            labels, probs = ((), [])\n        all_labels.append(labels)\n        all_probs.append(_np.asarray(probs))\n\n    if was_single:\n        return all_labels[0], all_probs[0]\n    return all_labels, all_probs\n\nfasttext.FastText.FastText.predict = _patched_predict\n# --- End monkey-patch ---\n\nLID_MODEL_PATH = \"/kaggle/working/lid.176.bin\"\nif not os.path.exists(LID_MODEL_PATH):\n    print(\"Downloading fasttext language ID model (126MB)...\")\n    urllib.request.urlretrieve(\n        \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\",\n        LID_MODEL_PATH,\n    )\nlid_model = fasttext.load_model(LID_MODEL_PATH)\nprint(\"\\u2705 fasttext lid.176.bin loaded (with NumPy 2.x patch)\")\n\n\ndef detect_lang(text):\n    \"\"\"Detect language using fasttext. Returns (lang_code, confidence).\"\"\"\n    clean = text.replace(\"\\n\", \" \").strip()\n    if not clean:\n        return \"unknown\", 0.0\n    labels, scores = lid_model.predict(clean, k=1)\n    if not labels:\n        return \"unknown\", 0.0\n    return labels[0].replace(\"__label__\", \"\"), float(scores[0])\n\n\ndef add_lang_fields(example):\n    \"\"\"Add lang_id and lang_confidence via fasttext.\"\"\"\n    combined = example[\"instruction\"] + \" \" + example[\"output\"]\n    lang_id, lang_conf = detect_lang(combined)\n    example[\"lang_id\"] = lang_id\n    example[\"lang_confidence\"] = round(lang_conf, 4)\n    return example\n\n\n# Run language detection on Arrow-backed dataset (low RAM)\nprint(f\"Running language detection on {len(raw_ds):,} samples...\")\nraw_ds = raw_ds.map(add_lang_fields, desc=\"Language detection\")\n\n# Stats before filtering\nlang_counts = Counter(raw_ds[\"lang_id\"])\nprint(f\"\\nLanguage distribution (top 10): {lang_counts.most_common(10)}\")\n\n# Filter to Tamil with confidence >= 0.6\nbefore = len(raw_ds)\ntamil_ds = raw_ds.filter(lambda x: x[\"lang_id\"] == \"ta\" and x[\"lang_confidence\"] >= 0.6)\nlang_dropped = before - len(tamil_ds)\n\nprint(f\"\\n\\u2705 Language detection complete\")\nprint(f\"   Before: {before:,}\")\nprint(f\"   Dropped (non-Tamil or low confidence): {lang_dropped:,}\")\nprint(f\"   Remaining: {len(tamil_ds):,}\")\n\n# Convert filtered (smaller) dataset to Python list for Stage 2 mutations\nprint(f\"\\nConverting {len(tamil_ds):,} Tamil samples to Python list for Stage 2...\")\ncandidates = tamil_ds.to_list()\nprint(f\"\\u2705 {len(candidates):,} candidates in memory for curation\")\n\n# Free Arrow datasets — we have the Python list now\ndel raw_ds, tamil_ds\ngc.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Pass 1B: Quality Heuristics + Format Sanity\n",
    "\n",
    "def repetition_score(text):\n",
    "    \"\"\"Most-common-token ratio. High = repetitive/garbage.\"\"\"\n",
    "    toks = text.split()\n",
    "    if len(toks) < 5:\n",
    "        return 0.0\n",
    "    most = Counter(toks).most_common(1)[0][1] / len(toks)\n",
    "    return most\n",
    "\n",
    "\n",
    "def is_sane(instruction, output):\n",
    "    \"\"\"Format sanity — catches template corruption.\"\"\"\n",
    "    if \"<think>\" in output:\n",
    "        return False\n",
    "    if output.strip() == instruction.strip():\n",
    "        return False\n",
    "    if \"data:image\" in output:\n",
    "        return False\n",
    "    if \"<|im_start|>system\" in output:\n",
    "        return False\n",
    "    if \"systemsystem\" in output.lower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def is_echo(instruction, output):\n",
    "    \"\"\"Check if answer is >80% copy of instruction.\"\"\"\n",
    "    if not instruction or not output:\n",
    "        return False\n",
    "    # Simple overlap check\n",
    "    instr_set = set(instruction.split())\n",
    "    out_set = set(output.split())\n",
    "    if not out_set:\n",
    "        return False\n",
    "    overlap = len(instr_set & out_set) / len(out_set)\n",
    "    return overlap > 0.80\n",
    "\n",
    "\n",
    "print(f\"Running quality heuristics on {len(candidates):,} candidates...\")\n",
    "heuristic_stats = Counter()\n",
    "clean_candidates = []\n",
    "\n",
    "for s in candidates:\n",
    "    flags = []\n",
    "\n",
    "    # Tamil % threshold\n",
    "    if s[\"tamil_pct\"] < 0.30:\n",
    "        flags.append(\"low_tamil_pct\")\n",
    "\n",
    "    # Repetition score\n",
    "    rep = repetition_score(s[\"output\"])\n",
    "    s[\"repetition\"] = round(rep, 4)\n",
    "    if rep > 0.25 and len(s[\"output\"].split()) > 20:\n",
    "        flags.append(\"high_repetition\")\n",
    "\n",
    "    # Echo detection\n",
    "    if is_echo(s[\"instruction\"], s[\"output\"]):\n",
    "        flags.append(\"echo\")\n",
    "\n",
    "    # Format sanity\n",
    "    if not is_sane(s[\"instruction\"], s[\"output\"]):\n",
    "        flags.append(\"format_insane\")\n",
    "\n",
    "    # Empty/trivial filter\n",
    "    if len(s[\"output\"].strip()) < 10:\n",
    "        flags.append(\"trivial_output\")\n",
    "    if len(s[\"instruction\"].strip()) < 5:\n",
    "        flags.append(\"trivial_instruction\")\n",
    "\n",
    "    s[\"heuristic_flags\"] = flags\n",
    "\n",
    "    for f in flags:\n",
    "        heuristic_stats[f] += 1\n",
    "\n",
    "    if len(flags) == 0:\n",
    "        clean_candidates.append(s)\n",
    "\n",
    "heuristic_dropped = len(candidates) - len(clean_candidates)\n",
    "print(f\"\\n\\u2705 Heuristic filtering complete\")\n",
    "print(f\"   Before: {len(candidates):,}\")\n",
    "print(f\"   Dropped: {heuristic_dropped:,}\")\n",
    "print(f\"   Remaining: {len(clean_candidates):,}\")\n",
    "print(f\"   Flag distribution:\")\n",
    "for flag, count in heuristic_stats.most_common():\n",
    "    print(f\"     {flag}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Pass 1C: Deduplication with Source Priority\n",
    "# Phase 1: exact dedupe on instruction[:200] (fast hash-based)\n",
    "# Phase 2: near-duplicate detection via MinHash (if time permits)\n",
    "\n",
    "print(f\"Deduplicating {len(clean_candidates):,} candidates...\")\n",
    "\n",
    "# Phase 1: Exact dedupe on instruction[:200]\n",
    "seen = {}\n",
    "for s in clean_candidates:\n",
    "    key = s[\"instruction\"][:200].strip().lower()\n",
    "    if key not in seen:\n",
    "        seen[key] = s\n",
    "    else:\n",
    "        # Keep higher-priority source\n",
    "        existing_priority = SOURCE_PRIORITY.get(seen[key][\"source\"], 1)\n",
    "        new_priority = SOURCE_PRIORITY.get(s[\"source\"], 1)\n",
    "        if new_priority > existing_priority:\n",
    "            seen[key] = s\n",
    "        elif new_priority == existing_priority and s[\"tamil_pct\"] > seen[key][\"tamil_pct\"]:\n",
    "            seen[key] = s\n",
    "\n",
    "exact_deduped = list(seen.values())\n",
    "exact_dupes = len(clean_candidates) - len(exact_deduped)\n",
    "print(f\"  Phase 1 (exact): {len(clean_candidates):,} \\u2192 {len(exact_deduped):,} ({exact_dupes:,} duplicates removed)\")\n",
    "\n",
    "# Phase 2: MinHash near-duplicate detection (Jaccard >= 0.8)\n",
    "# Uses MD5 hash of character 3-grams for efficiency\n",
    "def get_char_ngrams(text, n=3):\n",
    "    \"\"\"Get character n-grams for MinHash.\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    return set(text[i:i+n] for i in range(len(text) - n + 1))\n",
    "\n",
    "def minhash_signature(ngrams, num_hashes=128):\n",
    "    \"\"\"Compute MinHash signature.\"\"\"\n",
    "    if not ngrams:\n",
    "        return [float('inf')] * num_hashes\n",
    "    sig = []\n",
    "    for i in range(num_hashes):\n",
    "        min_hash = float('inf')\n",
    "        for ng in ngrams:\n",
    "            h = int(hashlib.md5(f\"{i}_{ng}\".encode()).hexdigest(), 16) % (2**32)\n",
    "            min_hash = min(min_hash, h)\n",
    "        sig.append(min_hash)\n",
    "    return sig\n",
    "\n",
    "def jaccard_from_sigs(sig1, sig2):\n",
    "    \"\"\"Estimate Jaccard similarity from MinHash signatures.\"\"\"\n",
    "    return sum(a == b for a, b in zip(sig1, sig2)) / len(sig1)\n",
    "\n",
    "# Only run MinHash if pool is manageable (< 300K)\n",
    "if len(exact_deduped) <= 300000:\n",
    "    print(f\"  Phase 2 (MinHash): computing signatures for {len(exact_deduped):,} samples...\")\n",
    "    sigs = []\n",
    "    for i, s in enumerate(exact_deduped):\n",
    "        ngrams = get_char_ngrams(s[\"instruction\"])\n",
    "        sigs.append(minhash_signature(ngrams))\n",
    "        if i % 50000 == 0 and i > 0:\n",
    "            print(f\"    ...{i:,} / {len(exact_deduped):,} signatures computed\")\n",
    "\n",
    "    # LSH-style banding for efficient near-duplicate detection\n",
    "    # Band size = 4, so 128/4 = 32 bands — catches Jaccard >= ~0.8\n",
    "    BAND_SIZE = 4\n",
    "    NUM_BANDS = 128 // BAND_SIZE\n",
    "    near_dupes = set()\n",
    "\n",
    "    for band_idx in range(NUM_BANDS):\n",
    "        buckets = {}\n",
    "        start = band_idx * BAND_SIZE\n",
    "        end = start + BAND_SIZE\n",
    "        for i, sig in enumerate(sigs):\n",
    "            band_hash = tuple(sig[start:end])\n",
    "            if band_hash in buckets:\n",
    "                # Found a candidate pair — verify with full Jaccard\n",
    "                j = buckets[band_hash]\n",
    "                if i not in near_dupes and j not in near_dupes:\n",
    "                    jac = jaccard_from_sigs(sigs[i], sigs[j])\n",
    "                    if jac >= 0.8:\n",
    "                        # Keep higher-priority source\n",
    "                        si, sj = exact_deduped[i], exact_deduped[j]\n",
    "                        pi = SOURCE_PRIORITY.get(si[\"source\"], 1)\n",
    "                        pj = SOURCE_PRIORITY.get(sj[\"source\"], 1)\n",
    "                        if pi < pj or (pi == pj and si[\"tamil_pct\"] < sj[\"tamil_pct\"]):\n",
    "                            near_dupes.add(i)\n",
    "                        else:\n",
    "                            near_dupes.add(j)\n",
    "            else:\n",
    "                buckets[band_hash] = i\n",
    "\n",
    "    deduped = [s for i, s in enumerate(exact_deduped) if i not in near_dupes]\n",
    "    print(f\"  Phase 2 (MinHash): {len(exact_deduped):,} \\u2192 {len(deduped):,} ({len(near_dupes):,} near-duplicates removed)\")\n",
    "else:\n",
    "    deduped = exact_deduped\n",
    "    print(f\"  Phase 2 (MinHash): skipped (pool > 300K, exact dedupe sufficient)\")\n",
    "\n",
    "# Mark all with is_duplicate = False (survivors)\n",
    "for s in deduped:\n",
    "    s[\"is_duplicate\"] = False\n",
    "\n",
    "total_dedup_removed = len(clean_candidates) - len(deduped)\n",
    "print(f\"\\n\\u2705 Deduplication complete: {len(clean_candidates):,} \\u2192 {len(deduped):,} ({total_dedup_removed:,} total removed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Pass 1D: Toxicity Screening with Source-Aware Logic\n",
    "# For Toxic_Matrix/HHRLHF_T: toxic prompt + safe output = safety training data\n",
    "# For all others: any toxicity = flag for exclusion\n",
    "\n",
    "# Tamil toxicity wordlist (common slurs and NSFW terms)\n",
    "# Minimal list — expand as needed\n",
    "TOXIC_WORDS_TA = [\n",
    "    \"\\u0ba4\\u0bc7\\u0bb5\\u0b9f\\u0bbf\\u0baf\\u0bbe\",  # derogatory\n",
    "    \"\\u0bb5\\u0bc7\\u0b9a\\u0bbf\",  # slur\n",
    "    \"\\u0baa\\u0bc1\\u0ba3\\u0bcd\\u0b9f\\u0bc8\",  # vulgar\n",
    "    \"\\u0ba4\\u0bb1\\u0bcd\\u0b95\\u0bca\\u0bb2\\u0bc8\",  # murder\n",
    "    \"\\u0b95\\u0bca\\u0bb2\\u0bcd\\u0bb2\",  # kill\n",
    "    \"\\u0bb5\\u0bc6\\u0b9f\\u0bbf\\u0baa\\u0bca\\u0bb0\\u0bc1\\u0bb3\\u0bcd\",  # explosive\n",
    "]\n",
    "\n",
    "TOXIC_WORDS_EN = [\n",
    "    \"suicide method\", \"how to kill\", \"make a bomb\", \"hack into\",\n",
    "    \"child porn\", \"exploit children\",\n",
    "]\n",
    "\n",
    "\n",
    "def check_wordlist(text):\n",
    "    \"\"\"Check text against toxicity wordlists. Returns list of matched terms.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    flags = []\n",
    "    for word in TOXIC_WORDS_TA:\n",
    "        if word in text:\n",
    "            flags.append(f\"ta:{word[:10]}\")\n",
    "    for phrase in TOXIC_WORDS_EN:\n",
    "        if phrase in text_lower:\n",
    "            flags.append(f\"en:{phrase[:15]}\")\n",
    "    return flags\n",
    "\n",
    "\n",
    "def classify_toxicity(instruction, output, source, subset):\n",
    "    \"\"\"Source-aware toxicity classification.\"\"\"\n",
    "    instr_flags = check_wordlist(instruction)\n",
    "    output_flags = check_wordlist(output)\n",
    "\n",
    "    # Toxic_Matrix / HHRLHF_T: toxic prompt + safe response = safety training data\n",
    "    if subset in (\"Toxic_Matrix\", \"HHRLHF_T\") and instr_flags and not output_flags:\n",
    "        return instr_flags, True  # is_safety_sample = True\n",
    "\n",
    "    # Normal sources: any toxicity = flag for exclusion\n",
    "    return instr_flags + output_flags, False\n",
    "\n",
    "\n",
    "print(f\"Running toxicity screening on {len(deduped):,} candidates...\")\n",
    "safety_count = 0\n",
    "toxic_dropped = 0\n",
    "\n",
    "for s in deduped:\n",
    "    tox_flags, is_safety = classify_toxicity(\n",
    "        s[\"instruction\"], s[\"output\"], s[\"source\"], s[\"subset\"]\n",
    "    )\n",
    "    s[\"toxicity_flags\"] = tox_flags\n",
    "    s[\"is_safety_sample\"] = is_safety\n",
    "    if is_safety:\n",
    "        safety_count += 1\n",
    "    elif tox_flags:\n",
    "        toxic_dropped += 1\n",
    "\n",
    "print(f\"\\u2705 Toxicity screening complete\")\n",
    "print(f\"   Safety samples (toxic prompt + safe refusal): {safety_count:,}\")\n",
    "print(f\"   Toxic (will be excluded in Stage 3): {toxic_dropped:,}\")\n",
    "print(f\"   Clean: {len(deduped) - safety_count - toxic_dropped:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stage 2 — Pass 1 Summary Stats\n\nprint(\"=\" * 60)\nprint(\"PASS 1 SUMMARY (CPU Filters)\")\nprint(\"=\" * 60)\nprint(f\"  Raw input:          {total_raw:,}\")\nprint(f\"  After lang-id:      {len(candidates):,} (-{lang_dropped:,})\")\nprint(f\"  After heuristics:   {len(clean_candidates):,} (-{heuristic_dropped:,})\")\nprint(f\"  After dedup:        {len(deduped):,} (-{total_dedup_removed:,})\")\nprint(f\"  Safety routed:      {safety_count:,}\")\nprint(f\"  Toxic flagged:      {toxic_dropped:,}\")\nprint(f\"  \\u2192 Pass 1 output:   {len(deduped):,} candidates\")\n\n# Per-source breakdown of survivors\nsurvivor_sources = Counter(s[\"source\"] for s in deduped)\nprint(f\"\\nSurvivors by source:\")\nfor src, count in survivor_sources.most_common():\n    print(f\"  {src}: {count:,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Pass 2E: Perplexity Scoring (GPU)\n",
    "# Uses DAPT v1.1 model to score output quality.\n",
    "# PPL is a fluency metric, NOT a quality metric — use as weak signal for garbage detection.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"Loading DAPT v1.1 model for perplexity scoring: {DAPT_MODEL}\")\n",
    "ppl_tokenizer = AutoTokenizer.from_pretrained(DAPT_MODEL)\n",
    "ppl_model = AutoModelForCausalLM.from_pretrained(\n",
    "    DAPT_MODEL, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "ppl_model.eval()\n",
    "print(f\"\\u2705 PPL model loaded on {ppl_model.device}\")\n",
    "\n",
    "# Stratified sample if pool > 200K (proportional per source)\n",
    "MAX_PPL_SAMPLES = 200000\n",
    "if len(deduped) > MAX_PPL_SAMPLES:\n",
    "    print(f\"Pool ({len(deduped):,}) > {MAX_PPL_SAMPLES:,}, stratified sampling...\")\n",
    "    by_source = {}\n",
    "    for s in deduped:\n",
    "        by_source.setdefault(s[\"source\"], []).append(s)\n",
    "    ppl_candidates = []\n",
    "    for src, samples in by_source.items():\n",
    "        proportion = len(samples) / len(deduped)\n",
    "        n = max(100, int(MAX_PPL_SAMPLES * proportion))\n",
    "        ppl_candidates.extend(random.sample(samples, min(n, len(samples))))\n",
    "    ppl_candidate_set = set(id(s) for s in ppl_candidates)\n",
    "    print(f\"  Selected {len(ppl_candidates):,} for PPL scoring\")\n",
    "else:\n",
    "    ppl_candidates = deduped\n",
    "    ppl_candidate_set = set(id(s) for s in ppl_candidates)\n",
    "    print(f\"  Scoring all {len(ppl_candidates):,} candidates\")\n",
    "\n",
    "\n",
    "def compute_perplexity(text, max_length=512):\n",
    "    \"\"\"Compute perplexity for a single text.\"\"\"\n",
    "    inputs = ppl_tokenizer(\n",
    "        text, return_tensors=\"pt\", truncation=True, max_length=max_length\n",
    "    ).to(ppl_model.device)\n",
    "    with torch.no_grad():\n",
    "        out = ppl_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    return torch.exp(out.loss).item()\n",
    "\n",
    "\n",
    "print(f\"\\nComputing perplexity for {len(ppl_candidates):,} samples...\")\n",
    "for i, s in enumerate(ppl_candidates):\n",
    "    try:\n",
    "        ppl = compute_perplexity(s[\"output\"])\n",
    "        s[\"perplexity\"] = round(ppl, 2)\n",
    "    except Exception:\n",
    "        s[\"perplexity\"] = None\n",
    "    if (i + 1) % 10000 == 0:\n",
    "        print(f\"  ...{i+1:,} / {len(ppl_candidates):,}\")\n",
    "\n",
    "# Mark unscored samples\n",
    "for s in deduped:\n",
    "    if \"perplexity\" not in s:\n",
    "        s[\"perplexity\"] = None\n",
    "\n",
    "# Stats\n",
    "scored = [s[\"perplexity\"] for s in deduped if s[\"perplexity\"] is not None]\n",
    "if scored:\n",
    "    print(f\"\\n\\u2705 Perplexity scoring complete\")\n",
    "    print(f\"   Scored: {len(scored):,} / {len(deduped):,}\")\n",
    "    print(f\"   PPL stats: min={min(scored):.1f}, median={sorted(scored)[len(scored)//2]:.1f}, max={max(scored):.1f}, mean={sum(scored)/len(scored):.1f}\")\n",
    "    print(f\"   PPL < 50: {sum(1 for p in scored if p < 50):,}\")\n",
    "    print(f\"   PPL 50-200: {sum(1 for p in scored if 50 <= p < 200):,}\")\n",
    "    print(f\"   PPL >= 200 (likely garbage): {sum(1 for p in scored if p >= 200):,}\")\n",
    "\n",
    "# Free GPU memory\n",
    "del ppl_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\u2705 PPL model unloaded, GPU memory freed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Pass 2F: Semantic Categorization (IndicSBERT + HDBSCAN)\n",
    "# Compute sentence embeddings, cluster with HDBSCAN, map to VAZHI domains.\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hdbscan\n",
    "\n",
    "print(\"Loading IndicSBERT for semantic categorization...\")\n",
    "sbert_model = SentenceTransformer(\"l3cube-pune/tamil-sentence-similarity-sbert\")\n",
    "print(f\"\\u2705 IndicSBERT loaded\")\n",
    "\n",
    "# Compute embeddings for instructions (batch processing)\n",
    "instructions = [s[\"instruction\"][:512] for s in deduped]  # Truncate for efficiency\n",
    "print(f\"Computing embeddings for {len(instructions):,} instructions...\")\n",
    "embeddings = sbert_model.encode(\n",
    "    instructions, batch_size=256, show_progress_bar=True, normalize_embeddings=True\n",
    ")\n",
    "print(f\"\\u2705 Embeddings computed: shape {embeddings.shape}\")\n",
    "\n",
    "# Cluster with HDBSCAN\n",
    "print(\"Clustering with HDBSCAN...\")\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=50,\n",
    "    min_samples=10,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    ")\n",
    "cluster_labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "noise_count = sum(1 for l in cluster_labels if l == -1)\n",
    "print(f\"\\u2705 Clustering complete: {n_clusters} clusters, {noise_count:,} noise points\")\n",
    "\n",
    "# Assign cluster labels to samples\n",
    "for i, s in enumerate(deduped):\n",
    "    s[\"embedding_cluster\"] = int(cluster_labels[i])\n",
    "    s[\"auto_category\"] = f\"cluster_{cluster_labels[i]}\" if cluster_labels[i] >= 0 else \"unclustered\"\n",
    "\n",
    "# Show top 10 clusters with sample instructions\n",
    "cluster_counts = Counter(cluster_labels)\n",
    "print(f\"\\nTop 10 clusters:\")\n",
    "for cluster_id, count in cluster_counts.most_common(11):\n",
    "    if cluster_id == -1:\n",
    "        print(f\"  noise: {count:,} samples\")\n",
    "        continue\n",
    "    cluster_samples = [s for s in deduped if s[\"embedding_cluster\"] == cluster_id]\n",
    "    example = cluster_samples[0][\"instruction\"][:80]\n",
    "    print(f\"  cluster_{cluster_id}: {count:,} samples — e.g. \\\"{example}...\\\"\")\n",
    "\n",
    "# Free GPU memory\n",
    "del sbert_model, embeddings\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n\\u2705 SBERT model unloaded, GPU memory freed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Pass 2G: Composite Quality Score + Tokenized Length\n",
    "\n",
    "from transformers import AutoTokenizer as AT\n",
    "\n",
    "print(f\"Loading tokenizer for length validation: {TOKENIZER_MODEL}\")\n",
    "sft_tokenizer = AT.from_pretrained(TOKENIZER_MODEL, trust_remote_code=True)\n",
    "print(f\"\\u2705 Tokenizer loaded (vocab size: {sft_tokenizer.vocab_size:,})\")\n",
    "\n",
    "\n",
    "def compute_quality_score(s):\n",
    "    \"\"\"Composite quality score (0-1). Higher = better.\"\"\"\n",
    "    lang_conf = s.get(\"lang_confidence\", 0.0)\n",
    "    rep = s.get(\"repetition\", 0.0)\n",
    "    tamil = s.get(\"tamil_pct\", 0.0)\n",
    "    tok_len = s.get(\"tokenized_length\", SFT_MAX_SEQ_LENGTH + 1)\n",
    "    tox = s.get(\"toxicity_flags\", [])\n",
    "\n",
    "    score = (\n",
    "        lang_conf * 0.35 +\n",
    "        (1 / (1 + rep * 10)) * 0.25 +\n",
    "        tamil * 0.20 +\n",
    "        (1.0 if tok_len <= SFT_MAX_SEQ_LENGTH else 0.0) * 0.10 +\n",
    "        (0.0 if tox else 1.0) * 0.10\n",
    "    )\n",
    "    return round(score, 4)\n",
    "\n",
    "\n",
    "print(f\"Computing tokenized lengths and quality scores for {len(deduped):,} samples...\")\n",
    "for i, s in enumerate(deduped):\n",
    "    # Tokenized length (ChatML-wrapped)\n",
    "    chatml_text = to_chatml(s[\"instruction\"], s[\"output\"])\n",
    "    tok_len = len(sft_tokenizer.encode(chatml_text, add_special_tokens=False))\n",
    "    s[\"tokenized_length\"] = tok_len\n",
    "\n",
    "    # Quality score\n",
    "    s[\"quality_score\"] = compute_quality_score(s)\n",
    "\n",
    "    if (i + 1) % 50000 == 0:\n",
    "        print(f\"  ...{i+1:,} / {len(deduped):,}\")\n",
    "\n",
    "# Stats\n",
    "tok_lengths = [s[\"tokenized_length\"] for s in deduped]\n",
    "quality_scores = [s[\"quality_score\"] for s in deduped]\n",
    "\n",
    "print(f\"\\n\\u2705 Scoring complete\")\n",
    "print(f\"   Token length: min={min(tok_lengths)}, median={sorted(tok_lengths)[len(tok_lengths)//2]}, max={max(tok_lengths)}\")\n",
    "print(f\"   Within 2048 tokens: {sum(1 for t in tok_lengths if t <= SFT_MAX_SEQ_LENGTH):,} / {len(tok_lengths):,}\")\n",
    "print(f\"   Quality score: min={min(quality_scores):.3f}, median={sorted(quality_scores)[len(quality_scores)//2]:.3f}, mean={sum(quality_scores)/len(quality_scores):.3f}\")\n",
    "print(f\"   Score >= 0.45: {sum(1 for q in quality_scores if q >= 0.45):,}\")\n",
    "print(f\"   Score < 0.45: {sum(1 for q in quality_scores if q < 0.45):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 — Upload curated dataset to HF\n",
    "\n",
    "# Ensure all fields are serializable\n",
    "curated_records = []\n",
    "for s in deduped:\n",
    "    curated_records.append({\n",
    "        \"instruction\": s[\"instruction\"],\n",
    "        \"output\": s[\"output\"],\n",
    "        \"source\": s[\"source\"],\n",
    "        \"subset\": s[\"subset\"],\n",
    "        \"char_length\": s[\"char_length\"],\n",
    "        \"tamil_pct\": s[\"tamil_pct\"],\n",
    "        \"lang_id\": s[\"lang_id\"],\n",
    "        \"lang_confidence\": s[\"lang_confidence\"],\n",
    "        \"heuristic_flags\": s.get(\"heuristic_flags\", []),\n",
    "        \"repetition\": s.get(\"repetition\", 0.0),\n",
    "        \"toxicity_flags\": s.get(\"toxicity_flags\", []),\n",
    "        \"is_safety_sample\": s.get(\"is_safety_sample\", False),\n",
    "        \"is_duplicate\": s.get(\"is_duplicate\", False),\n",
    "        \"perplexity\": s.get(\"perplexity\"),\n",
    "        \"embedding_cluster\": s.get(\"embedding_cluster\"),\n",
    "        \"auto_category\": s.get(\"auto_category\"),\n",
    "        \"quality_score\": s.get(\"quality_score\", 0.0),\n",
    "        \"tokenized_length\": s.get(\"tokenized_length\", 0),\n",
    "    })\n",
    "\n",
    "print(f\"\\nUploading {len(curated_records):,} curated samples to {CURATED_DATASET}...\")\n",
    "curated_ds = Dataset.from_list(curated_records)\n",
    "api.create_repo(CURATED_DATASET, repo_type=\"dataset\", exist_ok=True)\n",
    "curated_ds.push_to_hub(CURATED_DATASET)\n",
    "\n",
    "print(f\"\\n\\u2705 Curated dataset uploaded: https://huggingface.co/datasets/{CURATED_DATASET}\")\n",
    "print(f\"   Total samples: {len(curated_records):,}\")\n",
    "print(f\"   Schema: {list(curated_records[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 3: COMPOSE\n",
    "\n",
    "Select from curated pools with absolute count targets, build final SFT dataset.\n",
    "\n",
    "**Input:** In-memory curated dataset (also backed up on HF from Stage 2)  \n",
    "**Output:** `CryptoYogi/vazhi-tamil-sft-v4_1` on HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3A: Filtering\n",
    "\n",
    "print(\"Applying Stage 3 filters...\")\n",
    "df = curated_records.copy()  # Work on list of dicts\n",
    "\n",
    "before = len(df)\n",
    "\n",
    "# Hard filters\n",
    "df = [s for s in df if not s[\"is_duplicate\"]]\n",
    "print(f\"  After dedup filter: {len(df):,} (-{before - len(df):,})\")\n",
    "before2 = len(df)\n",
    "\n",
    "df = [s for s in df if s[\"tokenized_length\"] <= SFT_MAX_SEQ_LENGTH]\n",
    "print(f\"  After token length \\u2264 {SFT_MAX_SEQ_LENGTH}: {len(df):,} (-{before2 - len(df):,})\")\n",
    "before3 = len(df)\n",
    "\n",
    "df = [s for s in df if s[\"lang_id\"] == \"ta\"]\n",
    "print(f\"  After lang_id == ta: {len(df):,} (-{before3 - len(df):,})\")\n",
    "before4 = len(df)\n",
    "\n",
    "df = [s for s in df if len(s[\"heuristic_flags\"]) == 0]\n",
    "print(f\"  After clean heuristics: {len(df):,} (-{before4 - len(df):,})\")\n",
    "before5 = len(df)\n",
    "\n",
    "# Toxicity: exclude flagged UNLESS safety sample\n",
    "df = [s for s in df if len(s[\"toxicity_flags\"]) == 0 or s[\"is_safety_sample\"]]\n",
    "print(f\"  After toxicity filter: {len(df):,} (-{before5 - len(df):,})\")\n",
    "before6 = len(df)\n",
    "\n",
    "# Soft quality filter\n",
    "df = [s for s in df if s[\"quality_score\"] >= 0.45]\n",
    "print(f\"  After quality \\u2265 0.45: {len(df):,} (-{before6 - len(df):,})\")\n",
    "before7 = len(df)\n",
    "\n",
    "# PPL garbage filter (only for scored samples)\n",
    "df = [s for s in df if s[\"perplexity\"] is None or s[\"perplexity\"] < 200]\n",
    "print(f\"  After PPL < 200: {len(df):,} (-{before7 - len(df):,})\")\n",
    "\n",
    "print(f\"\\n\\u2705 Filtering complete: {before:,} \\u2192 {len(df):,} ({before - len(df):,} removed)\")\n",
    "\n",
    "# Per-source counts after filtering\n",
    "filtered_sources = Counter(s[\"source\"] for s in df)\n",
    "print(f\"\\nFiltered pool by source:\")\n",
    "for src, count in filtered_sources.most_common():\n",
    "    print(f\"  {src}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3B: Composition with Absolute Count Targets\n",
    "# Each source has independent min/max — no anchoring, no cascading.\n",
    "\n",
    "# Separate safety samples from source pools\n",
    "safety_pool = [s for s in df if s[\"is_safety_sample\"]]\n",
    "non_safety = [s for s in df if not s[\"is_safety_sample\"]]\n",
    "\n",
    "# Build source pools\n",
    "source_pools = {}\n",
    "for s in non_safety:\n",
    "    source_pools.setdefault(s[\"source\"], []).append(s)\n",
    "source_pools[\"safety\"] = safety_pool\n",
    "\n",
    "print(\"Composing final dataset with absolute count targets...\")\n",
    "composed = {}\n",
    "total_composed = 0\n",
    "\n",
    "for bucket_name, targets in BUCKET_TARGETS.items():\n",
    "    pool = source_pools.get(bucket_name, [])\n",
    "    target = targets[\"target\"]\n",
    "    min_count = targets[\"min\"]\n",
    "    max_count = targets[\"max\"]\n",
    "\n",
    "    if len(pool) < min_count:\n",
    "        print(f\"  \\u26a0\\ufe0f {bucket_name}: only {len(pool):,} available, min is {min_count}\")\n",
    "        # Use all available — warn but don't fail\n",
    "        selected = pool\n",
    "    elif len(pool) <= target:\n",
    "        # Pool smaller than target: use all\n",
    "        selected = pool\n",
    "    else:\n",
    "        # Pool larger than target: sample down (capped at max)\n",
    "        use_count = min(target, max_count)\n",
    "        # Sort by quality_score descending, take top N\n",
    "        pool_sorted = sorted(pool, key=lambda x: x[\"quality_score\"], reverse=True)\n",
    "        selected = pool_sorted[:use_count]\n",
    "\n",
    "    composed[bucket_name] = selected\n",
    "    total_composed += len(selected)\n",
    "    print(f\"  {bucket_name}: {len(selected):,} / {len(pool):,} available (target: {target}, range: {min_count}-{max_count})\")\n",
    "\n",
    "print(f\"\\n\\u2705 Composition complete: {total_composed:,} total samples\")\n",
    "\n",
    "# Verify minimums\n",
    "all_met = True\n",
    "for bucket_name, targets in BUCKET_TARGETS.items():\n",
    "    actual = len(composed.get(bucket_name, []))\n",
    "    if actual < targets[\"min\"]:\n",
    "        print(f\"  \\u274c {bucket_name}: {actual} < min {targets['min']}\")\n",
    "        all_met = False\n",
    "if all_met:\n",
    "    print(\"\\u2705 All bucket minimums met\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3C: ChatML Conversion + Validation\n",
    "\n",
    "all_samples = []\n",
    "chatml_failures = 0\n",
    "\n",
    "for bucket_name, samples in composed.items():\n",
    "    for s in samples:\n",
    "        # Apply anti-memorization filter for Thirukkural (vazhi_packs only)\n",
    "        if s[\"source\"] == \"vazhi_packs\" and is_verbatim_kural_qa(s[\"instruction\"], s[\"output\"]):\n",
    "            continue\n",
    "\n",
    "        text = to_chatml(s[\"instruction\"], s[\"output\"])\n",
    "\n",
    "        # Strict ChatML validation\n",
    "        valid, reason = validate_chatml_strict(text)\n",
    "        if not valid:\n",
    "            chatml_failures += 1\n",
    "            continue\n",
    "\n",
    "        all_samples.append({\n",
    "            \"text\": text,\n",
    "            \"bucket\": bucket_name,\n",
    "            \"source\": s[\"source\"],\n",
    "            \"subset\": s[\"subset\"],\n",
    "            \"quality_score\": s[\"quality_score\"],\n",
    "            \"tokenized_length\": s[\"tokenized_length\"],\n",
    "        })\n",
    "\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "print(f\"\\u2705 ChatML conversion complete\")\n",
    "print(f\"   Valid samples: {len(all_samples):,}\")\n",
    "print(f\"   ChatML failures: {chatml_failures}\")\n",
    "\n",
    "# Bucket distribution\n",
    "bucket_counts = Counter(s[\"bucket\"] for s in all_samples)\n",
    "print(f\"\\n\\U0001f4ca Final bucket distribution:\")\n",
    "for bucket, count in sorted(bucket_counts.items()):\n",
    "    pct = 100 * count / len(all_samples)\n",
    "    print(f\"  {bucket}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3D: Stratified Train/Eval Split (90/10 by bucket)\n",
    "\n",
    "EVAL_RATIO = 0.10\n",
    "train_samples = []\n",
    "eval_samples = []\n",
    "\n",
    "by_bucket = {}\n",
    "for s in all_samples:\n",
    "    by_bucket.setdefault(s[\"bucket\"], []).append(s)\n",
    "\n",
    "for bucket, samples in by_bucket.items():\n",
    "    random.shuffle(samples)\n",
    "    n_eval = max(1, int(len(samples) * EVAL_RATIO))\n",
    "    eval_samples.extend(samples[:n_eval])\n",
    "    train_samples.extend(samples[n_eval:])\n",
    "\n",
    "random.shuffle(train_samples)\n",
    "random.shuffle(eval_samples)\n",
    "\n",
    "print(f\"\\U0001f4ca Stratified split:\")\n",
    "print(f\"  Train: {len(train_samples):,}\")\n",
    "print(f\"  Eval:  {len(eval_samples):,}\")\n",
    "print(f\"  Eval ratio: {len(eval_samples) / (len(train_samples) + len(eval_samples)):.1%}\")\n",
    "\n",
    "# Verify eval has all buckets\n",
    "eval_buckets = Counter(s[\"bucket\"] for s in eval_samples)\n",
    "print(f\"\\n  Eval bucket distribution:\")\n",
    "for bucket, count in sorted(eval_buckets.items()):\n",
    "    print(f\"    {bucket}: {count}\")\n",
    "\n",
    "# Verify all samples within token limit\n",
    "max_tok = max(s[\"tokenized_length\"] for s in all_samples)\n",
    "print(f\"\\n  Max tokenized length: {max_tok} (limit: {SFT_MAX_SEQ_LENGTH})\")\n",
    "assert max_tok <= SFT_MAX_SEQ_LENGTH, f\"Token length violation: {max_tok} > {SFT_MAX_SEQ_LENGTH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stage 3E: Upload to HuggingFace + Summary\n\ntrain_ds = Dataset.from_list(train_samples)\neval_ds = Dataset.from_list(eval_samples)\ndataset_dict = DatasetDict({\"train\": train_ds, \"validation\": eval_ds})\n\napi.create_repo(OUTPUT_DATASET, repo_type=\"dataset\", exist_ok=True)\ndataset_dict.push_to_hub(OUTPUT_DATASET)\n\nprint(f\"\\n\\u2705 Dataset uploaded: https://huggingface.co/datasets/{OUTPUT_DATASET}\")\nprint(f\"   Train: {len(train_ds):,} samples\")\nprint(f\"   Validation: {len(eval_ds):,} samples\")\n\n# Final summary\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"VAZHI Dataset Factory v{VERSION} \\u2014 COMPLETE\")\nprint(\"=\" * 60)\nprint(f\"\\n  Stage 1 (Retrieve): {total_raw:,} raw samples \\u2192 {RAW_DATASET}\")\nprint(f\"  Stage 2 (Curate):   {len(curated_records):,} curated samples \\u2192 {CURATED_DATASET}\")\nprint(f\"  Stage 3 (Compose):  {len(all_samples):,} final SFT samples \\u2192 {OUTPUT_DATASET}\")\nprint(f\"\\n  Train: {len(train_samples):,} | Eval: {len(eval_samples):,}\")\nprint(f\"  max_seq_length: {SFT_MAX_SEQ_LENGTH}\")\n\nprint(f\"\\n  Bucket composition:\")\nfor bucket, count in sorted(bucket_counts.items()):\n    target = BUCKET_TARGETS[bucket]\n    status = \"\\u2705\" if count >= target[\"min\"] else \"\\u26a0\\ufe0f\"\n    print(f\"    {status} {bucket}: {count:,} (target: {target['target']}, range: {target['min']}-{target['max']})\")\n\n# Sample outputs\nprint(f\"\\n{'=' * 60}\")\nprint(\"Sample outputs (2 per bucket):\")\nprint(\"=\" * 60)\n\nshown = Counter()\nfor s in all_samples:\n    if shown[s['bucket']] < 2:\n        shown[s['bucket']] += 1\n        print(f\"\\n[{s['bucket'].upper()}] source={s['source']} quality={s['quality_score']:.3f}\")\n        match = CHATML_PATTERN.search(s[\"text\"])\n        if match:\n            print(f\"  Q: {match.group(1)[:100]}\")\n            print(f\"  A: {match.group(2)[:150]}\")\n    if all(shown[b] >= 2 for b in BUCKET_TARGETS):\n        break\n\nprint(f\"\\n\\u2705 Dataset Factory v{VERSION} complete!\")\nprint(f\"   Next step: SFT training with conservative LoRA (r=8, q_proj+v_proj, 2 epochs)\")\nprint(f\"   Base model: {DAPT_MODEL}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
