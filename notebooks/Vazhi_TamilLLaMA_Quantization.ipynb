{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI - Tamil-LLaMA 7B Extreme Quantization Testing\n",
    "\n",
    "**Goal**: Test if Tamil-LLaMA 7B can be quantized to ~1-1.5GB while preserving Tamil quality\n",
    "\n",
    "**Why Tamil-LLaMA?**\n",
    "- Only model that produces correct Tamil responses out of the box\n",
    "- Q4_K_M is 3.9GB (too large), but extreme quants might work\n",
    "\n",
    "**Quantization Levels to Test**:\n",
    "| Type | Expected Size | Risk |\n",
    "|------|---------------|------|\n",
    "| Q4_K_M | 3.9GB | Known good |\n",
    "| Q3_K_M | ~3.0GB | Low |\n",
    "| Q2_K | ~2.5GB | Medium |\n",
    "| IQ3_XXS | ~2.0GB | Medium |\n",
    "| IQ2_XXS | ~1.5GB | High |\n",
    "| IQ2_XS | ~1.3GB | Higher |\n",
    "| IQ1_M | ~1.1GB | Very High |\n",
    "| IQ1_S | ~1.0GB | Experimental |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "# Clone llama.cpp for quantization\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "!cd llama.cpp && pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build llama.cpp tools\n",
    "!cd llama.cpp && mkdir -p build && cd build && cmake .. && make -j4 llama-quantize llama-cli\n",
    "print(\"Build complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from huggingface_hub import login, hf_hub_download, snapshot_download\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    login(token=secrets.get_secret(\"HF_TOKEN\"))\n",
    "    print(\"Logged in via Kaggle\")\n",
    "except:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        login(token=userdata.get('HF_TOKEN'))\n",
    "        print(\"Logged in via Colab\")\n",
    "    except:\n",
    "        login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Tamil-LLaMA 7B\n",
    "\n",
    "Model: `abhinand/tamil-llama-7b-instruct-v0.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GGUF already exists on HuggingFace\n",
    "from huggingface_hub import HfApi, list_models\n",
    "\n",
    "api = HfApi()\n",
    "print(\"Searching for Tamil-LLaMA GGUF models...\")\n",
    "\n",
    "# Search for existing GGUFs\n",
    "models = list(api.list_models(search=\"tamil-llama gguf\", limit=10))\n",
    "for m in models:\n",
    "    print(f\"  - {m.id}\")\n",
    "\n",
    "# Also check the original repo for GGUF files\n",
    "print(\"\\nChecking original repo for GGUF files...\")\n",
    "try:\n",
    "    files = api.list_repo_files(\"abhinand/tamil-llama-7b-instruct-v0.2\")\n",
    "    gguf_files = [f for f in files if f.endswith('.gguf')]\n",
    "    if gguf_files:\n",
    "        print(f\"Found GGUF files: {gguf_files}\")\n",
    "    else:\n",
    "        print(\"No GGUF files in original repo - need to convert\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Download existing GGUF if available\n",
    "# Uncomment if GGUF exists on HuggingFace\n",
    "\n",
    "# GGUF_REPO = \"TheBloke/tamil-llama-7b-instruct-v0.2-GGUF\"  # Example\n",
    "# hf_hub_download(GGUF_REPO, \"tamil-llama-7b-instruct-v0.2.Q4_K_M.gguf\", local_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Download HF model and convert to GGUF\n",
    "# This is needed if no GGUF exists\n",
    "\n",
    "MODEL_ID = \"abhinand/tamil-llama-7b-instruct-v0.2\"\n",
    "LOCAL_DIR = \"./tamil-llama-7b\"\n",
    "\n",
    "print(f\"Downloading {MODEL_ID}...\")\n",
    "print(\"This will take a while (~14GB)...\")\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=MODEL_ID,\n",
    "    local_dir=LOCAL_DIR,\n",
    "    ignore_patterns=[\"*.md\", \"*.txt\"],  # Skip docs\n",
    ")\n",
    "\n",
    "print(f\"Downloaded to {LOCAL_DIR}\")\n",
    "!ls -lh {LOCAL_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to GGUF F16 (base for all quantizations)\n",
    "print(\"Converting to GGUF F16...\")\n",
    "!python llama.cpp/convert_hf_to_gguf.py {LOCAL_DIR} --outfile tamil-llama-f16.gguf --outtype f16\n",
    "\n",
    "print(\"\\nF16 GGUF created:\")\n",
    "!ls -lh tamil-llama-f16.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Questions\n",
    "\n",
    "Standard Tamil questions to evaluate quality at each quantization level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_QUESTIONS = [\n",
    "    \"வணக்கம், நீங்கள் யார்?\",\n",
    "    \"திருக்குறளின் முதல் குறள் என்ன?\",\n",
    "    \"தமிழ்நாட்டின் தலைநகரம் எது?\",\n",
    "    \"OTP யாரிடமும் சொல்லலாமா?\",\n",
    "]\n",
    "\n",
    "EXPECTED_ANSWERS = {\n",
    "    \"திருக்குறளின் முதல் குறள் என்ன?\": \"அகர முதல\",\n",
    "    \"தமிழ்நாட்டின் தலைநகரம் எது?\": \"சென்னை\",\n",
    "    \"OTP யாரிடமும் சொல்லலாமா?\": \"இல்லை\",  # Should warn against sharing\n",
    "}\n",
    "\n",
    "def format_prompt(question):\n",
    "    \"\"\"Format prompt for Tamil-LLaMA instruction format\"\"\"\n",
    "    # Tamil-LLaMA uses Alpaca-style format\n",
    "    return f\"\"\"### Instruction:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def test_gguf(model_path, question, max_tokens=150):\n",
    "    \"\"\"Test a GGUF model with a Tamil question\"\"\"\n",
    "    prompt = format_prompt(question)\n",
    "    \n",
    "    cmd = [\n",
    "        \"./llama.cpp/build/bin/llama-cli\",\n",
    "        \"-m\", model_path,\n",
    "        \"-p\", prompt,\n",
    "        \"-n\", str(max_tokens),\n",
    "        \"--temp\", \"0.7\",\n",
    "        \"-ngl\", \"0\",  # CPU only for compatibility\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)\n",
    "        output = result.stdout\n",
    "        \n",
    "        # Extract response after prompt\n",
    "        if \"### Response:\" in output:\n",
    "            response = output.split(\"### Response:\")[-1].strip()\n",
    "            # Clean up\n",
    "            response = response.split(\"###\")[0].strip()  # Stop at next section\n",
    "            return response[:500]  # Limit length\n",
    "        return output[-500:]\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"[TIMEOUT]\"\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR: {e}]\"\n",
    "\n",
    "def evaluate_quality(response, question):\n",
    "    \"\"\"Simple quality check\"\"\"\n",
    "    # Check if response contains Tamil\n",
    "    tamil_chars = sum(1 for c in response if 0x0B80 <= ord(c) <= 0x0BFF)\n",
    "    has_tamil = tamil_chars > 10\n",
    "    \n",
    "    # Check for expected answer if available\n",
    "    expected = EXPECTED_ANSWERS.get(question, \"\")\n",
    "    has_expected = expected.lower() in response.lower() if expected else None\n",
    "    \n",
    "    # Check for gibberish patterns\n",
    "    is_gibberish = (\n",
    "        \"system\" in response.lower() * 3 or\n",
    "        len(set(response)) < 10 or  # Too few unique chars\n",
    "        response.count(response[:10]) > 3  # Repetition\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"has_tamil\": has_tamil,\n",
    "        \"has_expected\": has_expected,\n",
    "        \"is_gibberish\": is_gibberish,\n",
    "        \"tamil_chars\": tamil_chars,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantize to Multiple Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantization levels to test\n",
    "QUANT_LEVELS = [\n",
    "    (\"Q4_K_M\", \"q4_k_m\"),      # ~3.9GB - baseline\n",
    "    (\"Q3_K_M\", \"q3_k_m\"),      # ~3.0GB\n",
    "    (\"Q2_K\", \"q2_k\"),          # ~2.5GB\n",
    "    (\"IQ3_XXS\", \"iq3_xxs\"),    # ~2.0GB\n",
    "    (\"IQ2_XXS\", \"iq2_xxs\"),    # ~1.5GB - TARGET\n",
    "    (\"IQ2_XS\", \"iq2_xs\"),      # ~1.3GB\n",
    "    (\"IQ1_M\", \"iq1_m\"),        # ~1.1GB\n",
    "    (\"IQ1_S\", \"iq1_s\"),        # ~1.0GB - Most aggressive\n",
    "]\n",
    "\n",
    "print(\"Quantization levels to test:\")\n",
    "for name, code in QUANT_LEVELS:\n",
    "    print(f\"  - {name} ({code})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize to each level\n",
    "import os\n",
    "\n",
    "BASE_GGUF = \"tamil-llama-f16.gguf\"\n",
    "results = {}\n",
    "\n",
    "for name, code in QUANT_LEVELS:\n",
    "    output_file = f\"tamil-llama-{code}.gguf\"\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"\\n{name}: Already exists, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Quantizing to {name} ({code})...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    !./llama.cpp/build/bin/llama-quantize {BASE_GGUF} {output_file} {code}\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        size_bytes = os.path.getsize(output_file)\n",
    "        size_gb = size_bytes / (1024**3)\n",
    "        print(f\"\\nCreated: {output_file} ({size_gb:.2f} GB)\")\n",
    "        results[name] = {\"file\": output_file, \"size_gb\": size_gb}\n",
    "    else:\n",
    "        print(f\"\\nFailed to create {output_file}\")\n",
    "        results[name] = {\"file\": None, \"error\": \"Quantization failed\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all created files\n",
    "print(\"\\nAll GGUF files:\")\n",
    "!ls -lh tamil-llama-*.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Each Quantization Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each quantized model\n",
    "import time\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for name, code in QUANT_LEVELS:\n",
    "    model_file = f\"tamil-llama-{code}.gguf\"\n",
    "    \n",
    "    if not os.path.exists(model_file):\n",
    "        print(f\"\\nSkipping {name} - file not found\")\n",
    "        continue\n",
    "    \n",
    "    size_gb = os.path.getsize(model_file) / (1024**3)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing {name} ({size_gb:.2f} GB)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model_results = {\n",
    "        \"quant\": name,\n",
    "        \"size_gb\": size_gb,\n",
    "        \"questions\": []\n",
    "    }\n",
    "    \n",
    "    for q in TEST_QUESTIONS:\n",
    "        print(f\"\\nQ: {q}\")\n",
    "        \n",
    "        start = time.time()\n",
    "        response = test_gguf(model_file, q)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        quality = evaluate_quality(response, q)\n",
    "        \n",
    "        print(f\"A: {response[:300]}...\")\n",
    "        print(f\"   [Time: {elapsed:.1f}s, Tamil chars: {quality['tamil_chars']}, \"\n",
    "              f\"Gibberish: {quality['is_gibberish']}]\")\n",
    "        \n",
    "        model_results[\"questions\"].append({\n",
    "            \"question\": q,\n",
    "            \"response\": response,\n",
    "            \"time_s\": elapsed,\n",
    "            \"quality\": quality\n",
    "        })\n",
    "    \n",
    "    all_results.append(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TAMIL-LLAMA 7B QUANTIZATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Quant':<12} {'Size':<10} {'Tamil':<8} {'Correct':<10} {'Gibberish':<10} {'Verdict'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for result in all_results:\n",
    "    quant = result[\"quant\"]\n",
    "    size = f\"{result['size_gb']:.2f}GB\"\n",
    "    \n",
    "    # Aggregate quality across questions\n",
    "    tamil_count = sum(1 for q in result[\"questions\"] if q[\"quality\"][\"has_tamil\"])\n",
    "    correct_count = sum(1 for q in result[\"questions\"] \n",
    "                       if q[\"quality\"][\"has_expected\"] is True)\n",
    "    gibberish_count = sum(1 for q in result[\"questions\"] \n",
    "                         if q[\"quality\"][\"is_gibberish\"])\n",
    "    \n",
    "    total = len(result[\"questions\"])\n",
    "    \n",
    "    tamil_pct = f\"{tamil_count}/{total}\"\n",
    "    correct_pct = f\"{correct_count}/{total}\"\n",
    "    gibberish_pct = f\"{gibberish_count}/{total}\"\n",
    "    \n",
    "    # Verdict\n",
    "    if gibberish_count > 0:\n",
    "        verdict = \"FAIL\"\n",
    "    elif tamil_count == total and correct_count >= total - 1:\n",
    "        verdict = \"GOOD\"\n",
    "    elif tamil_count >= total - 1:\n",
    "        verdict = \"OK\"\n",
    "    else:\n",
    "        verdict = \"POOR\"\n",
    "    \n",
    "    print(f\"{quant:<12} {size:<10} {tamil_pct:<8} {correct_pct:<10} {gibberish_pct:<10} {verdict}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best quantization under 1.5GB\n",
    "print(\"\\nBest quantization under 1.5GB:\")\n",
    "\n",
    "viable = [r for r in all_results if r[\"size_gb\"] <= 1.5]\n",
    "if viable:\n",
    "    # Sort by quality (fewest gibberish, most Tamil)\n",
    "    for r in sorted(viable, key=lambda x: x[\"size_gb\"], reverse=True):\n",
    "        gibberish = sum(1 for q in r[\"questions\"] if q[\"quality\"][\"is_gibberish\"])\n",
    "        tamil = sum(1 for q in r[\"questions\"] if q[\"quality\"][\"has_tamil\"])\n",
    "        \n",
    "        status = \"USABLE\" if gibberish == 0 and tamil >= len(r[\"questions\"]) - 1 else \"DEGRADED\"\n",
    "        print(f\"  {r['quant']}: {r['size_gb']:.2f}GB - {status}\")\n",
    "else:\n",
    "    print(\"  No viable quantization under 1.5GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Output Comparison\n",
    "\n",
    "Compare actual responses across quantization levels for key questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Thirukkural responses across all quants\n",
    "thirukkural_q = \"திருக்குறளின் முதல் குறள் என்ன?\"\n",
    "\n",
    "print(\"\\nThirukkural Response Comparison:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for result in all_results:\n",
    "    for q in result[\"questions\"]:\n",
    "        if q[\"question\"] == thirukkural_q:\n",
    "            print(f\"\\n[{result['quant']} - {result['size_gb']:.2f}GB]\")\n",
    "            print(q[\"response\"][:400])\n",
    "            print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Capital city responses\n",
    "capital_q = \"தமிழ்நாட்டின் தலைநகரம் எது?\"\n",
    "\n",
    "print(\"\\nCapital City Response Comparison:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for result in all_results:\n",
    "    for q in result[\"questions\"]:\n",
    "        if q[\"question\"] == capital_q:\n",
    "            has_chennai = \"சென்னை\" in q[\"response\"]\n",
    "            status = \"CORRECT\" if has_chennai else \"WRONG\"\n",
    "            print(f\"\\n[{result['quant']} - {result['size_gb']:.2f}GB] {status}\")\n",
    "            print(q[\"response\"][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best viable model\n",
    "# Manual selection based on test results above\n",
    "\n",
    "BEST_QUANT = \"iq2_xxs\"  # Update based on test results\n",
    "BEST_FILE = f\"tamil-llama-{BEST_QUANT}.gguf\"\n",
    "\n",
    "if os.path.exists(BEST_FILE):\n",
    "    size_gb = os.path.getsize(BEST_FILE) / (1024**3)\n",
    "    print(f\"Best model: {BEST_FILE} ({size_gb:.2f}GB)\")\n",
    "    \n",
    "    # Rename for clarity\n",
    "    final_name = f\"vazhi-tamil-llama-{BEST_QUANT}.gguf\"\n",
    "    !cp {BEST_FILE} {final_name}\n",
    "    print(f\"Copied to: {final_name}\")\n",
    "else:\n",
    "    print(f\"File not found: {BEST_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Upload to HuggingFace (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload viable quantizations to HuggingFace\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "UPLOAD = False  # Set to True to upload\n",
    "\n",
    "if UPLOAD:\n",
    "    api = HfApi()\n",
    "    REPO_ID = \"CryptoYogi/vazhi-tamil-llama-gguf\"\n",
    "    \n",
    "    # Create repo\n",
    "    create_repo(REPO_ID, repo_type=\"model\", exist_ok=True)\n",
    "    \n",
    "    # Upload viable models (under 1.5GB with good quality)\n",
    "    files_to_upload = [\n",
    "        f\"tamil-llama-{code}.gguf\" \n",
    "        for name, code in QUANT_LEVELS \n",
    "        if os.path.exists(f\"tamil-llama-{code}.gguf\")\n",
    "    ]\n",
    "    \n",
    "    for f in files_to_upload:\n",
    "        print(f\"Uploading {f}...\")\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=f,\n",
    "            path_in_repo=f,\n",
    "            repo_id=REPO_ID,\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nUploaded to: https://huggingface.co/{REPO_ID}\")\n",
    "else:\n",
    "    print(\"Upload disabled. Set UPLOAD=True to upload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "================================================================\n",
    "TAMIL-LLAMA 7B QUANTIZATION STUDY - CONCLUSIONS\n",
    "================================================================\n",
    "\n",
    "Fill in after running tests:\n",
    "\n",
    "| Quant     | Size    | Tamil Quality | Verdict     |\n",
    "|-----------|---------|---------------|-------------|\n",
    "| Q4_K_M    | 3.9GB   | [            ]| Baseline    |\n",
    "| Q3_K_M    | ~3.0GB  | [            ]| [          ]|\n",
    "| Q2_K      | ~2.5GB  | [            ]| [          ]|\n",
    "| IQ3_XXS   | ~2.0GB  | [            ]| [          ]|\n",
    "| IQ2_XXS   | ~1.5GB  | [            ]| [          ]|\n",
    "| IQ2_XS    | ~1.3GB  | [            ]| [          ]|\n",
    "| IQ1_M     | ~1.1GB  | [            ]| [          ]|\n",
    "| IQ1_S     | ~1.0GB  | [            ]| [          ]|\n",
    "\n",
    "RECOMMENDATION:\n",
    "[Fill in the best viable quantization for VAZHI mobile app]\n",
    "\n",
    "NEXT STEPS:\n",
    "1. If viable quant found: Integrate with VAZHI app\n",
    "2. If no viable quant: Try alternative approaches\n",
    "   - Distillation from Tamil-LLaMA to smaller model\n",
    "   - Custom Tamil tokenizer + small model\n",
    "   - Hybrid architecture (small LLM + lookup tables)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
