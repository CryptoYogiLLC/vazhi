{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# VAZHI v0.4 Training\n\n**Goal**: Train Tamil LLM with complete 6-pack training data\n\n## All Packs Included:\n- ğŸª· **à®•à®²à®¾à®šà¯à®šà®¾à®°à®®à¯** (Culture) - Thirukkural, Siddhars, temples, festivals\n- ğŸ“š **à®•à®²à¯à®µà®¿** (Education) - Scholarships, exams, admissions\n- ğŸ›¡ï¸ **à®ªà®¾à®¤à¯à®•à®¾à®ªà¯à®ªà¯** (Security) - Cyber safety, scam prevention\n- âš–ï¸ **à®šà®Ÿà¯à®Ÿà®®à¯** (Legal) - RTI, consumer rights, legal procedures\n- ğŸ›ï¸ **à®…à®°à®šà¯** (Government) - Government schemes & services\n- ğŸ¥ **à®šà¯à®•à®¾à®¤à®¾à®°à®®à¯** (Healthcare) - Health, Siddha medicine\n\n## Training Data:\n- Training: 2,732 samples\n- Validation: 301 samples\n- Total: 3,033 samples",
   "metadata": {
    "id": "title"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 1: Install Dependencies"
   ],
   "metadata": {
    "id": "cell1_header"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install Unsloth and dependencies\n",
    "!pip install unsloth\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 2: Upload Training Data\n\nUpload these files from `data/v04/training/`:\n- `vazhi_v04_train.json` (2,732 samples)\n- `vazhi_v04_val.json` (301 samples)",
   "metadata": {
    "id": "cell2_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()  # Upload vazhi_train_v02.json and vazhi_val_v02.json"
   ],
   "metadata": {
    "id": "upload_data"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Verify uploaded files\n",
    "import os\n",
    "print(\"Uploaded files:\")\n",
    "for f in os.listdir('.'):\n",
    "    if f.endswith('.json'):\n",
    "        print(f\"  - {f} ({os.path.getsize(f) / 1024:.1f} KB)\")"
   ],
   "metadata": {
    "id": "verify_upload"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 3: Load Base Model with Unsloth"
   ],
   "metadata": {
    "id": "cell3_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "# Load model in 4-bit for memory efficiency\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ],
   "metadata": {
    "id": "load_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 4: Add LoRA Adapters"
   ],
   "metadata": {
    "id": "cell4_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Configure LoRA for Tamil fine-tuning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"LoRA adapters added!\")\n",
    "print(f\"Trainable parameters: {model.num_parameters(only_trainable=True):,}\")"
   ],
   "metadata": {
    "id": "add_lora"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 5: Define Tamil Chat Template"
   ],
   "metadata": {
    "id": "cell5_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Skip this cell - formatting is done in Cell 6\npass",
   "metadata": {
    "id": "chat_template"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 6: Load Training Data"
   ],
   "metadata": {
    "id": "cell6_header"
   }
  },
  {
   "cell_type": "code",
   "source": "from datasets import Dataset\nimport json\n\n# Tamil system prompt for VAZHI\nSYSTEM_PROMPT = \"\"\"à®¨à¯€à®™à¯à®•à®³à¯ VAZHI (à®µà®´à®¿), à®¤à®®à®¿à®´à¯ à®®à®•à¯à®•à®³à¯à®•à¯à®•à®¾à®© AI à®‰à®¤à®µà®¿à®¯à®¾à®³à®°à¯. à®¤à®®à®¿à®´à®¿à®²à¯ à®¤à¯†à®³à®¿à®µà®¾à®•à®µà¯à®®à¯ à®‰à®¤à®µà®¿à®¯à®¾à®•à®µà¯à®®à¯ à®ªà®¤à®¿à®²à®³à®¿à®¯à¯à®™à¯à®•à®³à¯. You can respond in Tamil, Tanglish, or English based on how the user asks.\"\"\"\n\ndef format_example(example):\n    \"\"\"Format a training example into Qwen chat template.\"\"\"\n    return f\"\"\"<|im_start|>system\n{SYSTEM_PROMPT}<|im_end|>\n<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>\"\"\"\n\n# Load v0.4 training data\nwith open(\"vazhi_v04_train.json\", \"r\") as f:\n    train_data = json.load(f)\n\nwith open(\"vazhi_v04_val.json\", \"r\") as f:\n    val_data = json.load(f)\n\n# Pre-format all examples (avoid dataset.map stalling)\nprint(\"Formatting training data...\")\ntrain_formatted = [{\"text\": format_example(ex)} for ex in train_data]\nprint(\"Formatting validation data...\")\nval_formatted = [{\"text\": format_example(ex)} for ex in val_data]\n\nprint(f\"\\nTraining samples: {len(train_formatted)}\")\nprint(f\"Validation samples: {len(val_formatted)}\")\nprint(f\"Total: {len(train_formatted) + len(val_formatted)}\")\n\n# Convert to HuggingFace Dataset\ntrain_dataset = Dataset.from_list(train_formatted)\nval_dataset = Dataset.from_list(val_formatted)\n\nprint(\"\\nSample formatted text:\")\nprint(train_dataset[0][\"text\"][:500])",
   "metadata": {
    "id": "load_data"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Check pack distribution (from original data before formatting)\nfrom collections import Counter\n\npack_counts = Counter(item.get('pack', 'unknown') for item in train_data)\nprint(\"Pack distribution:\")\nfor pack, count in sorted(pack_counts.items()):\n    print(f\"  {pack}: {count}\")",
   "metadata": {
    "id": "verify_culture"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 7: Configure Training Arguments"
   ],
   "metadata": {
    "id": "cell7_header"
   }
  },
  {
   "cell_type": "code",
   "source": "from transformers import TrainingArguments\nfrom trl import SFTTrainer\n\n# Training configuration - optimized for T4 GPU\ntraining_args = TrainingArguments(\n    output_dir=\"./vazhi-v04-lora\",\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,  # Effective batch size = 8\n    learning_rate=2e-4,\n    warmup_steps=50,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    logging_steps=10,\n    save_steps=200,\n    save_total_limit=3,\n    eval_strategy=\"steps\",\n    eval_steps=200,\n    seed=42,\n    report_to=\"none\",\n)\n\nprint(\"Training arguments configured!\")\nprint(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")",
   "metadata": {
    "id": "training_args"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 8: Initialize Trainer"
   ],
   "metadata": {
    "id": "cell8_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Create the trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=MAX_SEQ_LENGTH,\n    packing=False,  # Disable packing to avoid issues\n    args=training_args,\n)\n\nprint(\"Trainer initialized!\")\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Steps per epoch: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")",
   "metadata": {
    "id": "init_trainer"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 9: Train the Model\n",
    "\n",
    "**Estimated time**: ~45-60 minutes on T4 GPU"
   ],
   "metadata": {
    "id": "cell9_header"
   }
  },
  {
   "cell_type": "code",
   "source": "import time\n\nprint(\"=\"*50)\nprint(\"Starting VAZHI v0.4 Training\")\nprint(\"=\"*50)\n\nstart_time = time.time()\n\n# Train!\ntrainer.train()\n\nend_time = time.time()\ntraining_time = end_time - start_time\n\nprint(\"=\"*50)\nprint(\"Training Complete!\")\nprint(\"=\"*50)\nprint(f\"Total training time: {training_time:.2f} seconds ({training_time/60:.1f} minutes)\")",
   "metadata": {
    "id": "train"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 10: Save the Model"
   ],
   "metadata": {
    "id": "cell10_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Save the LoRA adapter\nmodel.save_pretrained(\"vazhi-v04-lora-final\")\ntokenizer.save_pretrained(\"vazhi-v04-lora-final\")\n\nprint(\"Model saved to vazhi-v04-lora-final/\")\n\n# Download the model files\nfrom google.colab import files\nimport shutil\n\nshutil.make_archive(\"vazhi-v04-lora-final\", 'zip', \"vazhi-v04-lora-final\")\nfiles.download(\"vazhi-v04-lora-final.zip\")\nprint(\"Model downloaded!\")",
   "metadata": {
    "id": "save_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 11: Define Inference Function (Improved)"
   ],
   "metadata": {
    "id": "cell11_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Set model to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def vazhi_chat(user_message: str, max_tokens: int = 512) -> str:\n",
    "    \"\"\"\n",
    "    Chat with VAZHI v0.2 model.\n",
    "\n",
    "    Args:\n",
    "        user_message: User's question in Tamil/Tanglish/English\n",
    "        max_tokens: Maximum response length (default 512, up from 256 in v0.1)\n",
    "\n",
    "    Returns:\n",
    "        Model's response\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_message}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "    # Extract assistant response (improved parsing)\n",
    "    if \"<|im_start|>assistant\" in response:\n",
    "        response = response.split(\"<|im_start|>assistant\")[-1]\n",
    "\n",
    "    # Clean up markers\n",
    "    response = response.replace(\"<|im_end|>\", \"\").strip()\n",
    "    response = response.replace(\"<|im_start|>\", \"\").strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "print(\"Inference function ready!\")"
   ],
   "metadata": {
    "id": "inference_func"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 12: Comprehensive VAZHI v0.4 Test",
   "metadata": {
    "id": "cell12_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Test questions across all 6 packs\ntest_questions = [\n    # Culture - à®•à®²à®¾à®šà¯à®šà®¾à®°à®®à¯\n    (\"Culture\", \"à®¤à®¿à®°à¯à®•à¯à®•à¯à®±à®³à®¿à®©à¯ à®®à¯à®¤à®²à¯ à®•à¯à®±à®³à¯ à®à®©à¯à®©?\"),\n    (\"Culture\", \"à®…à®•à®¤à¯à®¤à®¿à®¯à®°à¯ à®ªà®±à¯à®±à®¿ à®šà¯Šà®²à¯à®²à¯à®™à¯à®•à®³à¯\"),\n    \n    # Education - à®•à®²à¯à®µà®¿\n    (\"Education\", \"12th à®ªà®Ÿà®¿à®šà¯à®šà®¾à®šà¯à®šà¯, engineering-à®•à¯à®•à¯ à®à®©à¯à®© options?\"),\n    (\"Education\", \"NEET exam-à®•à¯à®•à¯ à®à®ªà¯à®ªà®Ÿà®¿ prepare à®ªà®£à¯à®±à®¤à¯?\"),\n\n    # Security - à®ªà®¾à®¤à¯à®•à®¾à®ªà¯à®ªà¯\n    (\"Security\", \"Scam message-à® à®à®ªà¯à®ªà®Ÿà®¿ identify à®ªà®£à¯à®±à®¤à¯?\"),\n    (\"Security\", \"OTP à®¯à®¾à®°à¯à®•à¯à®•à¯à®®à¯ share à®ªà®£à¯à®£à®•à¯à®•à¯‚à®Ÿà®¾à®¤à¯ à®à®©à¯?\"),\n\n    # Legal - à®šà®Ÿà¯à®Ÿà®®à¯\n    (\"Legal\", \"RTI à®à®©à¯à®±à®¾à®²à¯ à®à®©à¯à®©?\"),\n    (\"Legal\", \"Consumer complaint à®à®ªà¯à®ªà®Ÿà®¿ à®ªà¯‹à®Ÿà¯à®µà®¤à¯?\"),\n\n    # Government - à®…à®°à®šà¯\n    (\"Government\", \"CMCHIS card à®à®ªà¯à®ªà®Ÿà®¿ apply à®ªà®£à¯à®±à®¤à¯?\"),\n    (\"Government\", \"PM Kisan scheme à®à®©à¯à®±à®¾à®²à¯ à®à®©à¯à®©?\"),\n\n    # Healthcare - à®šà¯à®•à®¾à®¤à®¾à®°à®®à¯\n    (\"Healthcare\", \"à®šà®¿à®¤à¯à®¤ à®®à®°à¯à®¤à¯à®¤à¯à®µà®®à¯ à®à®©à¯à®±à®¾à®²à¯ à®à®©à¯à®©?\"),\n    (\"Healthcare\", \"Government hospital-à®²à¯ free treatment à®•à®¿à®Ÿà¯ˆà®•à¯à®•à¯à®®à®¾?\"),\n]\n\nprint(\"=\"*60)\nprint(\"VAZHI v0.4 - Comprehensive Test (All 6 Packs)\")\nprint(\"=\"*60)\n\nfor category, q in test_questions:\n    print(f\"\\n[{category}] Q: {q}\")\n    print(f\"A: {vazhi_chat(q)}\")\n    print(\"-\"*50)",
   "metadata": {
    "id": "comprehensive_test"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 13: Specific Thirukkural Validation\n",
    "\n",
    "This is the critical test - v0.1 failed on Thirukkural accuracy."
   ],
   "metadata": {
    "id": "cell13_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Thirukkural accuracy test - these were WRONG in v0.1\n",
    "thirukkural_tests = [\n",
    "    {\n",
    "        \"question\": \"à®¤à®¿à®°à¯à®•à¯à®•à¯à®±à®³à®¿à®©à¯ à®®à¯à®¤à®²à¯ à®•à¯à®±à®³à¯ à®à®©à¯à®©?\",\n",
    "        \"expected_contains\": [\"à®…à®•à®° à®®à¯à®¤à®²\", \"à®à®´à¯à®¤à¯à®¤à¯†à®²à¯à®²à®¾à®®à¯\", \"à®†à®¤à®¿\", \"à®ªà®•à®µà®©à¯\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"à®•à®±à¯à®±à®¤à®©à®¾à®²à¯ à®†à®¯ à®ªà®¯à®©à¯†à®©à¯à®•à¯Šà®²à¯ à®à®©à¯à®± à®•à¯à®±à®³à¯ à®®à¯à®´à¯à®®à¯ˆà®¯à®¾à®• à®šà¯Šà®²à¯à®²à¯à®™à¯à®•à®³à¯\",\n",
    "        \"expected_contains\": [\"à®µà®¾à®²à®±à®¿à®µà®©à¯\", \"à®¨à®±à¯à®±à®¾à®³à¯\", \"à®¤à¯Šà®´à®¾à®…à®°à¯\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"à®ªà®¿à®±à®µà®¿à®ªà¯ à®ªà¯†à®°à¯à®™à¯à®•à®Ÿà®²à¯ à®à®©à¯à®± à®•à¯à®±à®³à¯ à®à®©à¯à®©?\",\n",
    "        \"expected_contains\": [\"à®ªà®¿à®±à®µà®¿\", \"à®•à®Ÿà®²à¯\", \"à®¨à¯€à®¨à¯à®¤à¯\", \"à®‡à®±à¯ˆà®µà®©à¯\"]\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Thirukkural Accuracy Validation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "passed = 0\n",
    "for test in thirukkural_tests:\n",
    "    response = vazhi_chat(test[\"question\"])\n",
    "    matches = sum(1 for keyword in test[\"expected_contains\"] if keyword in response)\n",
    "    total = len(test[\"expected_contains\"])\n",
    "    status = \"âœ… PASS\" if matches >= total * 0.75 else \"âŒ FAIL\"\n",
    "\n",
    "    if matches >= total * 0.75:\n",
    "        passed += 1\n",
    "\n",
    "    print(f\"\\nQ: {test['question']}\")\n",
    "    print(f\"A: {response[:300]}...\" if len(response) > 300 else f\"A: {response}\")\n",
    "    print(f\"Keywords found: {matches}/{total} {status}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Thirukkural Tests: {passed}/{len(thirukkural_tests)} passed\")\n",
    "print(\"=\"*60)"
   ],
   "metadata": {
    "id": "thirukkural_test"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 14: v0.1 vs v0.2 Comparison Summary"
   ],
   "metadata": {
    "id": "cell14_header"
   }
  },
  {
   "cell_type": "code",
   "source": "print(\"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘              VAZHI v0.4 Training Complete!                   â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘  Training Data:                                              â•‘\nâ•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â•‘\nâ•‘  â”‚ Pack                â”‚ Samples   â”‚                         â•‘\nâ•‘  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                         â•‘\nâ•‘  â”‚ Culture (à®•à®²à®¾à®šà¯à®šà®¾à®°à®®à¯)  â”‚ ~500      â”‚                         â•‘\nâ•‘  â”‚ Education (à®•à®²à¯à®µà®¿)    â”‚ ~460      â”‚                         â•‘\nâ•‘  â”‚ Security (à®ªà®¾à®¤à¯à®•à®¾à®ªà¯à®ªà¯) â”‚ ~450      â”‚                         â•‘\nâ•‘  â”‚ Legal (à®šà®Ÿà¯à®Ÿà®®à¯)       â”‚ ~440      â”‚                         â•‘\nâ•‘  â”‚ Government (à®…à®°à®šà¯)   â”‚ ~430      â”‚                         â•‘\nâ•‘  â”‚ Healthcare (à®šà¯à®•à®¾à®¤à®¾à®°à®®à¯)â”‚ ~460      â”‚                         â•‘\nâ•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â•‘\nâ•‘  Total: 3,033 samples (2,732 train + 301 val)               â•‘\nâ•‘                                                              â•‘\nâ•‘  Files saved:                                                â•‘\nâ•‘  - vazhi-v04-lora-final.zip (~110MB)                        â•‘\nâ•‘                                                              â•‘\nâ•‘  Next steps:                                                 â•‘\nâ•‘  1. Upload to HuggingFace: CryptoYogi/vazhi-lora            â•‘\nâ•‘  2. Space will auto-load the adapter                        â•‘\nâ•‘  3. Test Flutter app with new model                         â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\"\"\")",
   "metadata": {
    "id": "summary"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 15: (Optional) Upload to HuggingFace Hub\n",
    "\n",
    "Run this cell to push your model directly to HuggingFace Hub."
   ],
   "metadata": {
    "id": "cell15_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Upload to HuggingFace Hub\nfrom huggingface_hub import login, HfApi\n\n# Login to HuggingFace (will prompt for token)\nlogin()\n\n# Push model to Hub\nmodel.push_to_hub(\"CryptoYogi/vazhi-lora\")\ntokenizer.push_to_hub(\"CryptoYogi/vazhi-lora\")\n\nprint(\"âœ… Model uploaded to https://huggingface.co/CryptoYogi/vazhi-lora\")",
   "metadata": {
    "id": "upload_hf"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}