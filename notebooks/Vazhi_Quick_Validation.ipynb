{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI Quick Validation Test\n",
    "\n",
    "**Purpose**: Validate training setup in 5-10 minutes before committing to hours of training.\n",
    "\n",
    "**Key Fixes from Failed Run:**\n",
    "1. Match prompt format to base model (Gemma uses Alpaca-style)\n",
    "2. Disable packing (causes cross-contamination without flash attention)\n",
    "3. Lower learning rate (prevent catastrophic forgetting)\n",
    "4. Train on small subset first (50 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CRITICAL: Force single GPU to avoid DataParallel issues\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n# Install dependencies\n!pip install -q bitsandbytes peft trl accelerate datasets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check Base Model's Expected Format\n",
    "\n",
    "First, let's see what format the pre-trained model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"abhinand/gemma-2b-it-tamil-v0.1-alpha\"\n",
    "\n",
    "# Load tokenizer first to check format\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Check special tokens\n",
    "print(\"=\" * 50)\n",
    "print(\"TOKENIZER INFO:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})\")\n",
    "print(f\"BOS token: {tokenizer.bos_token} (id: {tokenizer.bos_token_id})\")\n",
    "print(f\"PAD token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})\")\n",
    "\n",
    "# Check if it has chat template\n",
    "if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:\n",
    "    print(f\"\\nChat template exists: YES\")\n",
    "    print(f\"Template: {tokenizer.chat_template[:200]}...\")\n",
    "else:\n",
    "    print(f\"\\nChat template: NO (use simple format)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model in 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\":0},\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded! Memory: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different prompt formats to see which works best\n",
    "def test_format(prompt_text):\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "question = \"தமிழ்நாட்டின் தலைநகரம் எது?\"\n",
    "\n",
    "# Format 1: Alpaca style\n",
    "print(\"=\" * 50)\n",
    "print(\"FORMAT 1: Alpaca Style\")\n",
    "print(\"=\" * 50)\n",
    "alpaca_prompt = f\"### Instruction:\\n{question}\\n\\n### Response:\\n\"\n",
    "print(f\"Prompt: {alpaca_prompt}\")\n",
    "print(f\"Output: {test_format(alpaca_prompt)}\")\n",
    "\n",
    "# Format 2: Simple\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FORMAT 2: Simple\")\n",
    "print(\"=\" * 50)\n",
    "simple_prompt = f\"Question: {question}\\nAnswer:\"\n",
    "print(f\"Prompt: {simple_prompt}\")\n",
    "print(f\"Output: {test_format(simple_prompt)}\")\n",
    "\n",
    "# Format 3: Direct\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FORMAT 3: Direct\")\n",
    "print(\"=\" * 50)\n",
    "direct_prompt = f\"{question}\"\n",
    "print(f\"Prompt: {direct_prompt}\")\n",
    "print(f\"Output: {test_format(direct_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Training Data with CORRECT Format\n",
    "\n",
    "We need to convert our data to match the base model's expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load full dataset\n",
    "dataset = load_dataset(\"CryptoYogi/vazhi-tamil-v05\")\n",
    "print(f\"Full dataset: {len(dataset['train'])} samples\")\n",
    "\n",
    "# Check first sample's format\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ORIGINAL DATA FORMAT:\")\n",
    "print(\"=\" * 50)\n",
    "sample = dataset['train'][0]\n",
    "print(f\"Keys: {sample.keys()}\")\n",
    "if 'text' in sample:\n",
    "    print(f\"Text format: {sample['text'][:300]}...\")\n",
    "if 'instruction' in sample:\n",
    "    print(f\"Instruction: {sample['instruction'][:100]}...\")\n",
    "if 'output' in sample:\n",
    "    print(f\"Output: {sample['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SMALL SUBSET for quick validation\n",
    "VALIDATION_SIZE = 50  # Just 50 samples for quick test\n",
    "\n",
    "# Format function - convert to Alpaca format (what Gemma expects)\n",
    "def format_to_alpaca(example):\n",
    "    \"\"\"Convert any format to Alpaca format for Gemma\"\"\"\n",
    "    \n",
    "    # If text field exists, try to extract instruction/output\n",
    "    if 'text' in example and example['text']:\n",
    "        text = example['text']\n",
    "        \n",
    "        # Check if it's ChatML format and convert\n",
    "        if '<|im_start|>' in text:\n",
    "            # Extract user message\n",
    "            import re\n",
    "            user_match = re.search(r'<\\|im_start\\|>user\\n(.+?)<\\|im_end\\|>', text, re.DOTALL)\n",
    "            assistant_match = re.search(r'<\\|im_start\\|>assistant\\n(.+?)<\\|im_end\\|>', text, re.DOTALL)\n",
    "            \n",
    "            if user_match and assistant_match:\n",
    "                instruction = user_match.group(1).strip()\n",
    "                output = assistant_match.group(1).strip()\n",
    "                formatted = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
    "                return {\"text\": formatted}\n",
    "        \n",
    "        # If already in correct format or other format, use as-is\n",
    "        if '### Instruction' in text:\n",
    "            return {\"text\": text}\n",
    "    \n",
    "    # If instruction/output fields exist\n",
    "    if 'instruction' in example and 'output' in example:\n",
    "        instruction = example['instruction']\n",
    "        output = example['output']\n",
    "        formatted = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
    "        return {\"text\": formatted}\n",
    "    \n",
    "    return {\"text\": \"\"}\n",
    "\n",
    "# Take small subset\n",
    "small_train = dataset['train'].select(range(VALIDATION_SIZE))\n",
    "\n",
    "# Format to Alpaca\n",
    "formatted_train = small_train.map(format_to_alpaca)\n",
    "formatted_train = formatted_train.filter(lambda x: len(x['text']) > 10)\n",
    "\n",
    "print(f\"Validation subset: {len(formatted_train)} samples\")\n",
    "print(\"\\nFormatted sample:\")\n",
    "print(formatted_train[0]['text'][:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Add LoRA and Quick Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Smaller rank for quick test\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "import time\n",
    "\n",
    "# CONSERVATIVE settings for validation\n",
    "training_config = SFTConfig(\n",
    "    output_dir=\"./vazhi-validation\",\n",
    "    \n",
    "    # Dataset\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=256,  # Shorter for quick test\n",
    "    packing=False,   # DISABLED - was causing issues!\n",
    "    \n",
    "    # Small batch for quick test\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    # LOWER learning rate to prevent catastrophic forgetting\n",
    "    learning_rate=5e-5,  # Was 2e-4, now 5e-5 (4x lower)\n",
    "    num_train_epochs=1,\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Optimizer\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Precision\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    \n",
    "    # Logging - more frequent for validation\n",
    "    logging_steps=5,\n",
    "    \n",
    "    # Speed\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    \n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=formatted_train,\n",
    "    args=training_config,\n",
    ")\n",
    "\n",
    "print(f\"Quick validation: {len(formatted_train)} samples\")\n",
    "print(f\"Learning rate: 5e-5 (conservative)\")\n",
    "print(f\"Packing: DISABLED\")\n",
    "print(f\"Expected time: ~2-5 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test BEFORE training\n",
    "def test_model(prompt):\n",
    "    inputs = tokenizer(\n",
    "        f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"### Response:\" in response:\n",
    "        return response.split(\"### Response:\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BEFORE TRAINING:\")\n",
    "print(\"=\" * 60)\n",
    "before_response = test_model(\"தமிழ்நாட்டின் தலைநகரம் எது?\")\n",
    "print(f\"Q: தமிழ்நாட்டின் தலைநகரம் எது?\")\n",
    "print(f\"A: {before_response[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quick training\n",
    "print(\"Starting quick validation training...\")\n",
    "start = time.time()\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Time: {elapsed / 60:.1f} minutes\")\n",
    "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AFTER training - check if model still works\n",
    "print(\"=\" * 60)\n",
    "print(\"AFTER TRAINING:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "after_response = test_model(\"தமிழ்நாட்டின் தலைநகரம் எது?\")\n",
    "print(f\"Q: தமிழ்நாட்டின் தலைநகரம் எது?\")\n",
    "print(f\"A: {after_response[:200]}\")\n",
    "\n",
    "# Check if response is coherent (not garbage)\n",
    "import re\n",
    "tamil_chars = len(re.findall(r'[\\u0B80-\\u0BFF]', after_response))\n",
    "total_chars = len(after_response)\n",
    "tamil_pct = (tamil_chars / total_chars * 100) if total_chars > 0 else 0\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDATION RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Tamil content: {tamil_pct:.1f}%\")\n",
    "print(f\"Response coherent: {'YES' if tamil_pct > 20 and len(after_response) > 20 else 'NO - TRAINING FAILED'}\")\n",
    "\n",
    "if trainer_stats.training_loss < 1.5 and tamil_pct > 20:\n",
    "    print(\"\\n✅ VALIDATION PASSED - Safe to run full training!\")\n",
    "else:\n",
    "    print(\"\\n❌ VALIDATION FAILED - Do not proceed with full training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Summary\n",
    "\n",
    "**If validation PASSED:**\n",
    "- Update the full training notebook with these fixes:\n",
    "  1. Use Alpaca format (not ChatML)\n",
    "  2. Disable packing\n",
    "  3. Use learning rate 5e-5\n",
    "  4. Run full training\n",
    "\n",
    "**If validation FAILED:**\n",
    "- Check the format conversion\n",
    "- Try even lower learning rate (1e-5)\n",
    "- Check if base model is compatible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
