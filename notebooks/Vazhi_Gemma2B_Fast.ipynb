{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI Gemma-2B Tamil Fine-tuning (Optimized)\n",
    "\n",
    "**No Unsloth required** - uses standard HF with speed optimizations.\n",
    "\n",
    "**Key optimizations:**\n",
    "- 8-bit AdamW optimizer\n",
    "- Flash Attention 2\n",
    "- Gradient checkpointing\n",
    "- Mixed precision (bf16)\n",
    "\n",
    "**Expected time:** ~1-2 hours on Kaggle T4x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies (shows progress)\n!pip install bitsandbytes\n!pip install peft\n!pip install trl\n!pip install accelerate"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Model in 4-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel_name = \"abhinand/gemma-2b-it-tamil-v0.1-alpha\"\n\n# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Load model with 4-bit on single GPU (required for training)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map={\"\": 0},  # Single GPU - required for 4-bit training\n    trust_remote_code=True,\n    attn_implementation=\"eager\",  # Gemma doesn't support flash_attention_2 well\n)\n\n# Enable gradient checkpointing for memory efficiency\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded in 4-bit!\")\nprint(f\"Model memory: {model.get_memory_footprint() / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(prompt, max_tokens=100):\n",
    "    inputs = tokenizer(\n",
    "        f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"### Response:\")[-1].strip()\n",
    "\n",
    "print(\"BEFORE TRAINING:\")\n",
    "print(f\"Q: தமிழ்நாட்டின் தலைநகரம் எது?\")\n",
    "print(f\"A: {test_model('தமிழ்நாட்டின் தலைநகரம் எது?')[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load VAZHI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"CryptoYogi/vazhi-tamil-v05\")\n",
    "print(f\"Train: {len(dataset['train'])} samples\")\n",
    "print(f\"Val: {len(dataset['validation'])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example):\n",
    "    \"\"\"Format for training\"\"\"\n",
    "    if 'text' in example and example['text']:\n",
    "        return {\"text\": example['text']}\n",
    "    elif 'instruction' in example and 'output' in example:\n",
    "        return {\"text\": f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"}\n",
    "    return {\"text\": \"\"}\n",
    "\n",
    "# Format datasets\n",
    "train_data = dataset['train'].map(format_prompt)\n",
    "train_data = train_data.filter(lambda x: len(x['text']) > 10)\n",
    "\n",
    "print(f\"Formatted: {len(train_data)} samples\")\n",
    "print(f\"Sample: {train_data[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTTrainer, SFTConfig\n\n# Detect GPU capability for precision\ngpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"\"\nuse_bf16 = \"A100\" in gpu_name or \"H100\" in gpu_name or \"RTX 30\" in gpu_name or \"RTX 40\" in gpu_name\nuse_fp16 = not use_bf16\nprint(f\"GPU: {gpu_name}\")\nprint(f\"Using: {'bf16' if use_bf16 else 'fp16'}\")\n\n# SFTConfig combines training args\ntraining_config = SFTConfig(\n    output_dir=\"./vazhi-gemma-fast\",\n    \n    # Dataset\n    dataset_text_field=\"text\",\n    max_seq_length=512,\n    packing=False,\n    \n    # Batch settings - smaller batches, more accumulation\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,  # Effective batch = 16\n    \n    # Learning\n    learning_rate=2e-4,\n    num_train_epochs=1,\n    warmup_ratio=0.05,\n    \n    # 8-bit optimizer - KEY for speed!\n    optim=\"paged_adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"cosine\",\n    \n    # Precision - auto-detect based on GPU\n    fp16=use_fp16,\n    bf16=use_bf16,\n    \n    # Logging\n    logging_steps=25,\n    save_steps=200,\n    save_total_limit=2,\n    \n    # Speed optimizations\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    \n    seed=42,\n    report_to=\"none\",\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    processing_class=tokenizer,\n    train_dataset=train_data,\n    args=training_config,\n)\n\nprint(f\"Training config ready!\")\nprint(f\"Effective batch size: {2 * 8} = 16\")\nprint(f\"Steps: ~{len(train_data) // 16}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"Expected time: ~1-2 hours\")\n",
    "start = time.time()\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Total time: {elapsed / 60:.1f} minutes\")\n",
    "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AFTER TRAINING:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_prompts = [\n",
    "    \"தமிழ்நாட்டின் தலைநகரம் எது?\",\n",
    "    \"திருக்குறளின் முதல் குறள் என்ன?\",\n",
    "    \"PM-KISAN திட்டம் என்ன?\",\n",
    "    \"இந்த SMS உண்மையா? 'நீங்கள் lottery வென்றீர்கள், ₹500 அனுப்புங்கள்'\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    print(f\"A: {test_model(prompt)[:300]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter\n",
    "model.save_pretrained(\"./vazhi-lora\")\n",
    "tokenizer.save_pretrained(\"./vazhi-lora\")\n",
    "print(\"LoRA adapter saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA into base model\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Reload in 16-bit for merging\n",
    "merged_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"./vazhi-lora\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# Save merged\n",
    "merged_model.save_pretrained(\"./vazhi-merged\")\n",
    "tokenizer.save_pretrained(\"./vazhi-merged\")\n",
    "print(\"Merged model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Convert to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llama.cpp for conversion\n",
    "!pip install -q llama-cpp-python\n",
    "!git clone --depth 1 https://github.com/ggerganov/llama.cpp /kaggle/working/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to GGUF F16 first\n",
    "!cd /kaggle/working/llama.cpp && pip install -q -r requirements.txt\n",
    "!python /kaggle/working/llama.cpp/convert_hf_to_gguf.py ./vazhi-merged --outfile ./vazhi-f16.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build llama.cpp for quantization\n",
    "!cd /kaggle/working/llama.cpp && make -j quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize to Q4_K_M\n",
    "!/kaggle/working/llama.cpp/llama-quantize ./vazhi-f16.gguf ./vazhi-q4km.gguf Q4_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check output files\n",
    "for f in [\"./vazhi-f16.gguf\", \"./vazhi-q4km.gguf\"]:\n",
    "    if os.path.exists(f):\n",
    "        size = os.path.getsize(f) / 1e9\n",
    "        print(f\"{f}: {size:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"./vazhi-q4km.gguf\",\n",
    "    n_ctx=512,\n",
    "    n_threads=4,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GGUF Q4_K_M TEST:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    response = llm(\n",
    "        f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\",\n",
    "        max_tokens=150,\n",
    "        stop=[\"###\", \"\\n\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    print(f\"A: {response['choices'][0]['text'].strip()[:300]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Download GGUF\n",
    "\n",
    "Download `vazhi-q4km.gguf` (~1.6 GB) for mobile deployment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to output for easy download\n",
    "import shutil\n",
    "shutil.copy(\"./vazhi-q4km.gguf\", \"/kaggle/working/vazhi-q4km.gguf\")\n",
    "print(\"GGUF copied to /kaggle/working/ for download!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Optimizations used:**\n",
    "- 4-bit quantization with double quant\n",
    "- 8-bit paged AdamW optimizer\n",
    "- Gradient checkpointing\n",
    "- BF16 mixed precision\n",
    "\n",
    "**Output:**\n",
    "- `vazhi-q4km.gguf` - Q4_K_M for mobile (~1.6 GB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}