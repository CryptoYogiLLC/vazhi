{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VAZHI v0.4 - High-Tamil LoRA Training\n",
        "\n",
        "**Goal**: Train improved Tamil LLM with 86.7% Tamil content (vs 2.6% in v0.2)\n",
        "\n",
        "## Key Improvements over v0.2:\n",
        "- \u2705 86.7% Tamil content (proper Tamil translations)\n",
        "- \u2705 Tamil-first bilingual format: \"\u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0b9a\u0bca\u0bb2\u0bcd (English term)\"\n",
        "- \u2705 3,033 unique samples (0 duplicates)\n",
        "- \u2705 Fixed batch-compatible formatting function\n",
        "- \u2705 Standardized pack names (vazhi_*)\n",
        "- \u2705 512 max tokens for inference\n",
        "\n",
        "## Training Data:\n",
        "| Split | Samples |\n",
        "|-------|--------:|\n",
        "| Train | 2,732 |\n",
        "| Val | 301 |\n",
        "| **Total** | **3,033** |\n",
        "\n",
        "## Pack Distribution:\n",
        "| Pack | Samples |\n",
        "|------|--------:|\n",
        "| vazhi_panpaadu (Culture) | 625 |\n",
        "| vazhi_sattam (Legal) | 590 |\n",
        "| vazhi_kaval (Security) | 467 |\n",
        "| vazhi_maruthuvam (Healthcare) | 462 |\n",
        "| vazhi_arasu (Government) | 452 |\n",
        "| vazhi_kalvi (Education) | 437 |\n",
        "\n",
        "---\n",
        "**Runtime**: Select GPU (T4 or better) before running\n",
        "\n",
        "**Data Files**: `data/v04/training/vazhi_v04_train.json` and `vazhi_v04_val.json`"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 1: Install Dependencies"
      ],
      "metadata": {
        "id": "install-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Unsloth and dependencies\n",
        "!pip install unsloth\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 2: Upload Training Data\n",
        "\n",
        "Upload these files from `data/v04/training/`:\n",
        "- `vazhi_v04_train.json` (~3.4 MB)\n",
        "- `vazhi_v04_val.json` (~385 KB)"
      ],
      "metadata": {
        "id": "upload-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Upload vazhi_v04_train.json and vazhi_v04_val.json from data/v04/training/\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Verify uploaded files\n",
        "import os\n",
        "print(\"\\nUploaded files:\")\n",
        "for f in os.listdir('.'):\n",
        "    if f.endswith('.json'):\n",
        "        print(f\"  - {f} ({os.path.getsize(f) / 1024:.1f} KB)\")"
      ],
      "metadata": {
        "id": "upload"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 3: Load Base Model with Unsloth"
      ],
      "metadata": {
        "id": "model-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "\n",
        "# Load model in 4-bit for memory efficiency\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto-detect\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")"
      ],
      "metadata": {
        "id": "load-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4: Add LoRA Adapters"
      ],
      "metadata": {
        "id": "lora-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure LoRA for Tamil fine-tuning\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"LoRA adapters added!\")\n",
        "print(f\"Trainable parameters: {model.num_parameters(only_trainable=True):,}\")"
      ],
      "metadata": {
        "id": "lora"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 5: Define Tamil-First Chat Template\n",
        "\n",
        "v0.4 uses Tamil-first format: \"\u0ba4\u0bae\u0bbf\u0bb4\u0bcd\u0b9a\u0bcd\u0b9a\u0bca\u0bb2\u0bcd (English term)\" for technical vocabulary."
      ],
      "metadata": {
        "id": "template-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VAZHI v0.4 Tamil-first system prompt\n",
        "SYSTEM_PROMPT = \"\"\"\u0ba8\u0bc0\u0b99\u0bcd\u0b95\u0bb3\u0bcd \u0bb5\u0bb4\u0bbf (VAZHI), \u0ba4\u0bae\u0bbf\u0bb4\u0bcd\u0ba8\u0bbe\u0b9f\u0bc1 \u0bae\u0b95\u0bcd\u0b95\u0bb3\u0bc1\u0b95\u0bcd\u0b95\u0bbe\u0ba9 AI \u0b89\u0ba4\u0bb5\u0bbf\u0baf\u0bbe\u0bb3\u0bb0\u0bcd. \u0ba4\u0bae\u0bbf\u0bb4\u0bbf\u0bb2\u0bcd \u0ba4\u0bc6\u0bb3\u0bbf\u0bb5\u0bbe\u0b95\u0bb5\u0bc1\u0bae\u0bcd \u0b89\u0ba4\u0bb5\u0bbf\u0baf\u0bbe\u0b95\u0bb5\u0bc1\u0bae\u0bcd \u0baa\u0ba4\u0bbf\u0bb2\u0bb3\u0bbf\u0baf\u0bc1\u0b99\u0bcd\u0b95\u0bb3\u0bcd. \u0ba4\u0bca\u0bb4\u0bbf\u0bb2\u0bcd\u0ba8\u0bc1\u0b9f\u0bcd\u0baa \u0b9a\u0bca\u0bb1\u0bcd\u0b95\u0bb3\u0bc8 \u0ba4\u0bae\u0bbf\u0bb4\u0bbf\u0bb2\u0bcd \u0bae\u0bc1\u0ba4\u0bb2\u0bbf\u0bb2\u0bcd \u0b95\u0bc2\u0bb1\u0bbf, \u0baa\u0bbf\u0bb1\u0b95\u0bc1 \u0b86\u0b99\u0bcd\u0b95\u0bbf\u0bb2\u0ba4\u0bcd\u0ba4\u0bbf\u0bb2\u0bcd \u0b85\u0b9f\u0bc8\u0baa\u0bcd\u0baa\u0bc1\u0b95\u0bcd\u0b95\u0bc1\u0bb1\u0bbf\u0b95\u0bcd\u0b95\u0bc1\u0bb3\u0bcd \u0b95\u0bc1\u0bb1\u0bbf\u0baa\u0bcd\u0baa\u0bbf\u0b9f\u0bc1\u0b99\u0bcd\u0b95\u0bb3\u0bcd.\"\"\"\n",
        "\n",
        "# Fixed formatting function - handles both single and batched examples\n",
        "def format_prompt(examples):\n",
        "    \"\"\"Format training examples into Qwen chat template. Returns list.\"\"\"\n",
        "    texts = []\n",
        "    \n",
        "    # Handle both single example and batch\n",
        "    if isinstance(examples['instruction'], list):\n",
        "        # Batched\n",
        "        for instruction, output in zip(examples['instruction'], examples['output']):\n",
        "            text = f\"\"\"<|im_start|>system\n",
        "{SYSTEM_PROMPT}<|im_end|>\n",
        "<|im_start|>user\n",
        "{instruction}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "{output}<|im_end|>\"\"\"\n",
        "            texts.append(text)\n",
        "    else:\n",
        "        # Single example\n",
        "        text = f\"\"\"<|im_start|>system\n",
        "{SYSTEM_PROMPT}<|im_end|>\n",
        "<|im_start|>user\n",
        "{examples['instruction']}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "{examples['output']}<|im_end|>\"\"\"\n",
        "        texts.append(text)\n",
        "    \n",
        "    return texts\n",
        "\n",
        "# Test the template\n",
        "test_example = {\n",
        "    \"instruction\": \"\u0ba4\u0bbf\u0bb0\u0bc1\u0b95\u0bcd\u0b95\u0bc1\u0bb1\u0bb3\u0bbf\u0ba9\u0bcd \u0bae\u0bc1\u0ba4\u0bb2\u0bcd \u0b95\u0bc1\u0bb1\u0bb3\u0bcd \u0b8e\u0ba9\u0bcd\u0ba9?\",\n",
        "    \"output\": \"\u0bae\u0bc1\u0ba4\u0bb2\u0bcd \u0b95\u0bc1\u0bb1\u0bb3\u0bcd: \u0b85\u0b95\u0bb0 \u0bae\u0bc1\u0ba4\u0bb2 \u0b8e\u0bb4\u0bc1\u0ba4\u0bcd\u0ba4\u0bc6\u0bb2\u0bcd\u0bb2\u0bbe\u0bae\u0bcd \u0b86\u0ba4\u0bbf \u0baa\u0b95\u0bb5\u0ba9\u0bcd \u0bae\u0bc1\u0ba4\u0bb1\u0bcd\u0bb1\u0bc7 \u0b89\u0bb2\u0b95\u0bc1.\"\n",
        "}\n",
        "print(\"Template example:\")\n",
        "print(format_prompt(test_example)[0])"
      ],
      "metadata": {
        "id": "template"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 6: Load and Verify Training Data"
      ],
      "metadata": {
        "id": "data-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Load v0.4 training data\n",
        "with open(\"vazhi_v04_train.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open(\"vazhi_v04_val.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "\n",
        "# Pack distribution\n",
        "from collections import Counter\n",
        "pack_counts = Counter(item.get('pack', 'unknown') for item in train_data + val_data)\n",
        "print(\"\\nPack Distribution:\")\n",
        "for pack, count in sorted(pack_counts.items(), key=lambda x: -x[1]):\n",
        "    print(f\"  {pack}: {count}\")"
      ],
      "metadata": {
        "id": "load-data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify Tamil content in data\n",
        "def tamil_pct(text):\n",
        "    \"\"\"Calculate percentage of Tamil characters.\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    tamil = len(re.findall(r'[\\u0B80-\\u0BFF]', text))\n",
        "    total = len(re.findall(r'[\\u0B80-\\u0BFF\\w]', text))\n",
        "    return (tamil / total * 100) if total > 0 else 0\n",
        "\n",
        "# Check Tamil percentage\n",
        "pcts = [tamil_pct(s['output']) for s in train_data]\n",
        "print(f\"Tamil Content Verification:\")\n",
        "print(f\"  Average: {sum(pcts)/len(pcts):.1f}%\")\n",
        "print(f\"  Min: {min(pcts):.1f}%\")\n",
        "print(f\"  Max: {max(pcts):.1f}%\")\n",
        "print(f\"  Samples < 50% Tamil: {sum(1 for p in pcts if p < 50)}\")\n",
        "\n",
        "# Show sample entry\n",
        "print(\"\\nSample entry:\")\n",
        "print(f\"  Pack: {train_dataset[0]['pack']}\")\n",
        "print(f\"  Instruction: {train_dataset[0]['instruction'][:80]}...\")\n",
        "print(f\"  Output: {train_dataset[0]['output'][:100]}...\")"
      ],
      "metadata": {
        "id": "verify-tamil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 7: Configure Training Arguments"
      ],
      "metadata": {
        "id": "training-args-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Training configuration - optimized for T4 GPU\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./vazhi-v04-lora\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 8\n",
        "    learning_rate=2e-4,\n",
        "    warmup_steps=50,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    save_total_limit=3,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    seed=42,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured!\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")"
      ],
      "metadata": {
        "id": "training-args"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 8: Initialize Trainer"
      ],
      "metadata": {
        "id": "trainer-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    formatting_func=format_prompt,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "steps_per_epoch = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
        "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
        "\n",
        "print(\"Trainer initialized!\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"Total steps: {total_steps}\")"
      ],
      "metadata": {
        "id": "trainer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 9: Train the Model\n",
        "\n",
        "**Estimated time**: ~45-60 minutes on T4 GPU"
      ],
      "metadata": {
        "id": "train-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"VAZHI v0.4 Training - High Tamil Content\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training {len(train_dataset)} samples with 86.7% Tamil content\")\n",
        "print(f\"Expected: ~{steps_per_epoch * 3} steps over 3 epochs\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total training time: {training_time:.2f} seconds ({training_time/60:.1f} minutes)\")"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 10: Save and Download the Model"
      ],
      "metadata": {
        "id": "save-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the LoRA adapter\n",
        "model.save_pretrained(\"vazhi-v04-lora-final\")\n",
        "tokenizer.save_pretrained(\"vazhi-v04-lora-final\")\n",
        "\n",
        "print(\"Model saved to vazhi-v04-lora-final/\")\n",
        "\n",
        "# Download the model files\n",
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"vazhi-v04-lora-final\", 'zip', \"vazhi-v04-lora-final\")\n",
        "files.download(\"vazhi-v04-lora-final.zip\")\n",
        "print(\"Model downloaded!\")"
      ],
      "metadata": {
        "id": "save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 11: Define Inference Function"
      ],
      "metadata": {
        "id": "inference-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model to inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def vazhi_chat(user_message: str, max_tokens: int = 512) -> str:\n",
        "    \"\"\"\n",
        "    Chat with VAZHI v0.4 model.\n",
        "\n",
        "    Args:\n",
        "        user_message: User's question in Tamil/Tanglish/English\n",
        "        max_tokens: Maximum response length (default 512)\n",
        "\n",
        "    Returns:\n",
        "        Model's response\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "{SYSTEM_PROMPT}<|im_end|>\n",
        "<|im_start|>user\n",
        "{user_message}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "    # Extract assistant response (improved parsing)\n",
        "    if \"<|im_start|>assistant\" in response:\n",
        "        response = response.split(\"<|im_start|>assistant\")[-1]\n",
        "\n",
        "    # Clean up markers\n",
        "    response = response.replace(\"<|im_end|>\", \"\").strip()\n",
        "    response = response.replace(\"<|im_start|>\", \"\").strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "print(\"Inference function ready!\")"
      ],
      "metadata": {
        "id": "inference"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 12: Comprehensive Test Across All Packs"
      ],
      "metadata": {
        "id": "test-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test questions across all packs\n",
        "test_questions = [\n",
        "    # Culture - Thirukkural\n",
        "    (\"Culture\", \"\u0ba4\u0bbf\u0bb0\u0bc1\u0b95\u0bcd\u0b95\u0bc1\u0bb1\u0bb3\u0bbf\u0ba9\u0bcd \u0bae\u0bc1\u0ba4\u0bb2\u0bcd \u0b95\u0bc1\u0bb1\u0bb3\u0bcd \u0b8e\u0ba9\u0bcd\u0ba9?\"),\n",
        "    (\"Culture\", \"\u0b85\u0b95\u0bb0 \u0bae\u0bc1\u0ba4\u0bb2 \u0b8e\u0bb4\u0bc1\u0ba4\u0bcd\u0ba4\u0bc6\u0bb2\u0bcd\u0bb2\u0bbe\u0bae\u0bcd \u0b8e\u0ba9\u0bcd\u0bb1 \u0b95\u0bc1\u0bb1\u0bb3\u0bbf\u0ba9\u0bcd \u0baa\u0bca\u0bb0\u0bc1\u0bb3\u0bcd \u0b8e\u0ba9\u0bcd\u0ba9?\"),\n",
        "\n",
        "    # Culture - Siddhars\n",
        "    (\"\u0b9a\u0bbf\u0ba4\u0bcd\u0ba4\u0bb0\u0bcd\u0b95\u0bb3\u0bcd\", \"\u0b9a\u0bbf\u0ba4\u0bcd\u0ba4\u0bb0\u0bcd\u0b95\u0bb3\u0bcd \u0baf\u0bbe\u0bb0\u0bcd? \u0bae\u0bc1\u0b95\u0bcd\u0b95\u0bbf\u0baf \u0b9a\u0bbf\u0ba4\u0bcd\u0ba4\u0bb0\u0bcd\u0b95\u0bb3\u0bbf\u0ba9\u0bcd \u0baa\u0bc6\u0baf\u0bb0\u0bcd\u0b95\u0bb3\u0bcd \u0b9a\u0bca\u0bb2\u0bcd\u0bb2\u0bc1\u0b99\u0bcd\u0b95\u0bb3\u0bcd.\"),\n",
        "    (\"Culture\", \"\u0b85\u0b95\u0ba4\u0bcd\u0ba4\u0bbf\u0baf\u0bb0\u0bcd \u0baa\u0bb1\u0bcd\u0bb1\u0bbf \u0b9a\u0bca\u0bb2\u0bcd\u0bb2\u0bc1\u0b99\u0bcd\u0b95\u0bb3\u0bcd\"),\n",
        "\n",
        "    # Security\n",
        "    (\"Security\", \"\u0bae\u0bcb\u0b9a\u0b9f\u0bbf \u0b9a\u0bc6\u0baf\u0bcd\u0ba4\u0bbf\u0baf\u0bc8 \u0b8e\u0baa\u0bcd\u0baa\u0b9f\u0bbf \u0b95\u0ba3\u0bcd\u0b9f\u0bb1\u0bbf\u0bb5\u0ba4\u0bc1?\"),\n",
        "    (\"Security\", \"\u0b92\u0b9f\u0bcd\u0b9f\u0bc1\u0bae\u0bc1\u0bb1\u0bc8 \u0b95\u0b9f\u0bb5\u0bc1\u0b9a\u0bcd\u0b9a\u0bca\u0bb2\u0bcd\u0bb2\u0bc8 (OTP) \u0baf\u0bbe\u0bb0\u0bc1\u0b95\u0bcd\u0b95\u0bc1\u0bae\u0bcd \u0baa\u0b95\u0bbf\u0bb0\u0bcd\u0ba8\u0bcd\u0ba4\u0bc1\u0b95\u0bcd\u0b95\u0bca\u0bb3\u0bcd\u0bb3\u0b95\u0bcd\u0b95\u0bc2\u0b9f\u0bbe\u0ba4\u0bc1 \u0b8e\u0ba9\u0bcd?\"),\n",
        "\n",
        "    # Government\n",
        "    (\"Government\", \"\u0bae\u0bc1\u0ba4\u0bb2\u0bae\u0bc8\u0b9a\u0bcd\u0b9a\u0bb0\u0bcd \u0b95\u0bbe\u0baa\u0bcd\u0baa\u0bc0\u0b9f\u0bcd\u0b9f\u0bc1 \u0b85\u0b9f\u0bcd\u0b9f\u0bc8 \u0b8e\u0baa\u0bcd\u0baa\u0b9f\u0bbf \u0baa\u0bc6\u0bb1\u0bc1\u0bb5\u0ba4\u0bc1?\"),\n",
        "\n",
        "    # Education\n",
        "    (\"Education\", \"\u0ba8\u0bc0\u0b9f\u0bcd \u0ba4\u0bc7\u0bb0\u0bcd\u0bb5\u0bc1\u0b95\u0bcd\u0b95\u0bc1 \u0b8e\u0baa\u0bcd\u0baa\u0b9f\u0bbf \u0ba4\u0baf\u0bbe\u0bb0\u0bbe\u0bb5\u0ba4\u0bc1?\"),\n",
        "\n",
        "    # Legal\n",
        "    (\"Legal\", \"\u0ba4\u0b95\u0bb5\u0bb2\u0bcd \u0b85\u0bb1\u0bbf\u0baf\u0bc1\u0bae\u0bcd \u0b89\u0bb0\u0bbf\u0bae\u0bc8\u0b9a\u0bcd \u0b9a\u0b9f\u0bcd\u0b9f\u0bae\u0bcd \u0b8e\u0ba9\u0bcd\u0bb1\u0bbe\u0bb2\u0bcd \u0b8e\u0ba9\u0bcd\u0ba9?\"),\n",
        "\n",
        "    # Healthcare\n",
        "    (\"Healthcare\", \"\u0b85\u0bb0\u0b9a\u0bc1 \u0bae\u0bb0\u0bc1\u0ba4\u0bcd\u0ba4\u0bc1\u0bb5\u0bae\u0ba9\u0bc8\u0baf\u0bbf\u0bb2\u0bcd \u0b87\u0bb2\u0bb5\u0b9a \u0b9a\u0bbf\u0b95\u0bbf\u0b9a\u0bcd\u0b9a\u0bc8 \u0b95\u0bbf\u0b9f\u0bc8\u0b95\u0bcd\u0b95\u0bc1\u0bae\u0bbe?\"),\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"VAZHI v0.4 - Comprehensive High-Tamil Test\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for category, q in test_questions:\n",
        "    print(f\"\\n[{category}] Q: {q}\")\n",
        "    response = vazhi_chat(q)\n",
        "    pct = tamil_pct(response)\n",
        "    print(f\"A: {response[:300]}...\" if len(response) > 300 else f\"A: {response}\")\n",
        "    print(f\"Tamil %: {pct:.1f}%\")\n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "id": "test-all"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 13: Thirukkural Validation\n",
        "\n",
        "Critical test - v0.1 failed on Thirukkural accuracy."
      ],
      "metadata": {
        "id": "thirukkural-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Thirukkural accuracy test\n",
        "thirukkural_tests = [\n",
        "    {\n",
        "        \"question\": \"\u0ba4\u0bbf\u0bb0\u0bc1\u0b95\u0bcd\u0b95\u0bc1\u0bb1\u0bb3\u0bbf\u0ba9\u0bcd \u0bae\u0bc1\u0ba4\u0bb2\u0bcd \u0b95\u0bc1\u0bb1\u0bb3\u0bcd \u0b8e\u0ba9\u0bcd\u0ba9?\",\n",
        "        \"expected_contains\": [\"\u0b85\u0b95\u0bb0 \u0bae\u0bc1\u0ba4\u0bb2\", \"\u0b8e\u0bb4\u0bc1\u0ba4\u0bcd\u0ba4\u0bc6\u0bb2\u0bcd\u0bb2\u0bbe\u0bae\u0bcd\", \"\u0b86\u0ba4\u0bbf\", \"\u0baa\u0b95\u0bb5\u0ba9\u0bcd\"]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"\u0b95\u0bb1\u0bcd\u0bb1\u0ba4\u0ba9\u0bbe\u0bb2\u0bcd \u0b86\u0baf \u0baa\u0baf\u0ba9\u0bc6\u0ba9\u0bcd\u0b95\u0bca\u0bb2\u0bcd \u0b8e\u0ba9\u0bcd\u0bb1 \u0b95\u0bc1\u0bb1\u0bb3\u0bcd \u0bae\u0bc1\u0bb4\u0bc1\u0bae\u0bc8\u0baf\u0bbe\u0b95 \u0b9a\u0bca\u0bb2\u0bcd\u0bb2\u0bc1\u0b99\u0bcd\u0b95\u0bb3\u0bcd\",\n",
        "        \"expected_contains\": [\"\u0bb5\u0bbe\u0bb2\u0bb1\u0bbf\u0bb5\u0ba9\u0bcd\", \"\u0ba8\u0bb1\u0bcd\u0bb1\u0bbe\u0bb3\u0bcd\", \"\u0ba4\u0bca\u0bb4\u0bbe\u0b85\u0bb0\u0bcd\"]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"\u0baa\u0bbf\u0bb1\u0bb5\u0bbf\u0baa\u0bcd \u0baa\u0bc6\u0bb0\u0bc1\u0b99\u0bcd\u0b95\u0b9f\u0bb2\u0bcd \u0b8e\u0ba9\u0bcd\u0bb1 \u0b95\u0bc1\u0bb1\u0bb3\u0bcd \u0b8e\u0ba9\u0bcd\u0ba9?\",\n",
        "        \"expected_contains\": [\"\u0baa\u0bbf\u0bb1\u0bb5\u0bbf\", \"\u0b95\u0b9f\u0bb2\u0bcd\", \"\u0ba8\u0bc0\u0ba8\u0bcd\u0ba4\u0bc1\", \"\u0b87\u0bb1\u0bc8\u0bb5\u0ba9\u0bcd\"]\n",
        "    },\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\u0ba4\u0bbf\u0bb0\u0bc1\u0b95\u0bcd\u0b95\u0bc1\u0bb1\u0bb3\u0bcd Accuracy Validation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "passed = 0\n",
        "for test in thirukkural_tests:\n",
        "    response = vazhi_chat(test[\"question\"])\n",
        "    matches = sum(1 for keyword in test[\"expected_contains\"] if keyword in response)\n",
        "    total = len(test[\"expected_contains\"])\n",
        "    status = \"\u2705 PASS\" if matches >= total * 0.75 else \"\u274c FAIL\"\n",
        "\n",
        "    if matches >= total * 0.75:\n",
        "        passed += 1\n",
        "\n",
        "    print(f\"\\nQ: {test['question']}\")\n",
        "    print(f\"A: {response[:300]}...\" if len(response) > 300 else f\"A: {response}\")\n",
        "    print(f\"Keywords found: {matches}/{total} {status}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"\u0ba4\u0bbf\u0bb0\u0bc1\u0b95\u0bcd\u0b95\u0bc1\u0bb1\u0bb3\u0bcd Tests: {passed}/{len(thirukkural_tests)} passed\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "thirukkural-test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 14: v0.2 vs v0.4 Comparison Summary"
      ],
      "metadata": {
        "id": "summary-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\"\"\n",
        "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
        "\u2551              VAZHI v0.4 Training Complete!                   \u2551\n",
        "\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n",
        "\u2551  Key improvements:                                           \u2551\n",
        "\u2551  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2551\n",
        "\u2551  \u2502 Feature             \u2502 v0.2      \u2502 v0.4      \u2502             \u2551\n",
        "\u2551  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524             \u2551\n",
        "\u2551  \u2502 Tamil content       \u2502 ~2.6%     \u2502 86.7%     \u2502             \u2551\n",
        "\u2551  \u2502 Training samples    \u2502 2,860*    \u2502 3,033     \u2502             \u2551\n",
        "\u2551  \u2502 Bilingual format    \u2502 Mixed     \u2502 Tamil 1st \u2502             \u2551\n",
        "\u2551  \u2502 Tanglish            \u2502 Heavy     \u2502 Minimal   \u2502             \u2551\n",
        "\u2551  \u2502 Data format         \u2502 Mixed     \u2502 Standard  \u2502             \u2551\n",
        "\u2551  \u2502 Duplicates          \u2502 320       \u2502 0         \u2502             \u2551\n",
        "\u2551  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2551\n",
        "\u2551  * v0.2 reported 3,180 but had 320 duplicate IDs             \u2551\n",
        "\u2551                                                              \u2551\n",
        "\u2551  Pack Distribution (all standardized as vazhi_*):            \u2551\n",
        "\u2551  - vazhi_panpaadu (Culture): 625 samples                     \u2551\n",
        "\u2551  - vazhi_sattam (Legal): 590 samples                         \u2551\n",
        "\u2551  - vazhi_kaval (Security): 467 samples                       \u2551\n",
        "\u2551  - vazhi_maruthuvam (Healthcare): 462 samples                \u2551\n",
        "\u2551  - vazhi_arasu (Government): 452 samples                     \u2551\n",
        "\u2551  - vazhi_kalvi (Education): 437 samples                      \u2551\n",
        "\u2551                                                              \u2551\n",
        "\u2551  Files saved: vazhi-v04-lora-final.zip                       \u2551\n",
        "\u2551                                                              \u2551\n",
        "\u2551  Next steps:                                                 \u2551\n",
        "\u2551  1. Upload to HuggingFace Hub                                \u2551\n",
        "\u2551  2. Deploy to HuggingFace Spaces                             \u2551\n",
        "\u2551  3. Test Flutter app with new model                          \u2551\n",
        "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 15: (Optional) Upload to HuggingFace Hub\n",
        "\n",
        "Run this cell to push your model directly to HuggingFace Hub."
      ],
      "metadata": {
        "id": "hf-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Push to HuggingFace Hub\n",
        "# Uncomment and run if you want to upload directly\n",
        "\n",
        "# from huggingface_hub import login, HfApi\n",
        "#\n",
        "# # Login to HuggingFace\n",
        "# login()  # This will prompt for your token\n",
        "#\n",
        "# # Push model to Hub\n",
        "# model.push_to_hub(\"cryptoyogillc/vazhi-tamil-v04\")\n",
        "# tokenizer.push_to_hub(\"cryptoyogillc/vazhi-tamil-v04\")\n",
        "#\n",
        "# print(\"Model uploaded to HuggingFace Hub!\")"
      ],
      "metadata": {
        "id": "hf-upload"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
