{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI SFT v3.3 - Clean Training Notebook\n",
    "\n",
    "**Key fixes:**\n",
    "1. FP32 training (Qwen3 has internal bf16 ops that P100/T4 can't handle)\n",
    "2. `torch_dtype=torch.float16` for model loading\n",
    "3. ChatML-only data (no raw text mixing)\n",
    "4. SKIP_DATA_PREP logic to avoid redundant extraction\n",
    "5. Single GPU forced\n",
    "\n",
    "**Target:** Kaggle P100 (16GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "**IMPORTANT:** After running this cell, **RESTART the session** (Runtime ‚Üí Restart session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -U \\\n",
    "  \"transformers>=4.51.0\" \\\n",
    "  \"accelerate>=0.34.2\" \\\n",
    "  \"peft>=0.12.0\" \\\n",
    "  \"trl>=0.12.0\" \\\n",
    "  \"bitsandbytes>=0.43.3\" \\\n",
    "  \"datasets>=2.21.0\" \\\n",
    "  \"huggingface_hub>=0.24.7\"\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")\n",
    "print(\"‚ö†Ô∏è RESTART THE SESSION NOW (Runtime ‚Üí Restart session)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force single GPU BEFORE importing torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from huggingface_hub import login, HfApi, dataset_info\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Config\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Repos\n",
    "EXISTING_DATASET = \"CryptoYogi/vazhi-tamil-v05\"\n",
    "BALANCED_DATASET = \"CryptoYogi/vazhi-tamil-sft-v3_3\"\n",
    "BASE_MODEL = \"Qwen/Qwen3-0.6B\"\n",
    "OUTPUT_MODEL = \"CryptoYogi/vazhi-qwen3-v3_3\"\n",
    "\n",
    "# System prompt\n",
    "SYSTEM_PROMPT = \"‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç VAZHI (‡Æµ‡Æ¥‡Æø), ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æ© AI ‡Æâ‡Æ§‡Æµ‡Æø‡ÆØ‡Ææ‡Æ≥‡Æ∞‡Øç. ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Æø‡Æ≤‡Øç ‡Æ§‡ØÜ‡Æ≥‡Æø‡Æµ‡Ææ‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç ‡Æâ‡Æ§‡Æµ‡Æø‡ÆØ‡Ææ‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Æ§‡Æø‡Æ≤‡Æ≥‡Æø‡ÆØ‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç. ‡Æ§‡ØÜ‡Æ∞‡Æø‡ÆØ‡Ææ‡Æµ‡Æø‡Æü‡Øç‡Æü‡Ææ‡Æ≤‡Øç \\\"‡Æ§‡ØÜ‡Æ∞‡Æø‡ÆØ‡Æµ‡Æø‡Æ≤‡Øç‡Æ≤‡Øà\\\" ‡Æé‡Æ©‡Øç‡Æ±‡ØÅ ‡Æö‡Øä‡Æ≤‡Øç‡Æ≤‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç.\"\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   Source dataset: {EXISTING_DATASET}\")\n",
    "print(f\"   Balanced dataset: {BALANCED_DATASET}\")\n",
    "print(f\"   Base model: {BASE_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "print(\"‚úÖ Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tamil_chars(text):\n",
    "    \"\"\"Count Tamil Unicode characters.\"\"\"\n",
    "    return sum(1 for c in text if '\\u0B80' <= c <= '\\u0BFF')\n",
    "\n",
    "def is_good_tamil_sample(text, min_tamil_pct=30, min_len=10):\n",
    "    \"\"\"Check if text has enough Tamil content.\"\"\"\n",
    "    if not text or len(text) < min_len:\n",
    "        return False\n",
    "    tamil_pct = 100 * count_tamil_chars(text) / len(text)\n",
    "    return tamil_pct >= min_tamil_pct\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing extra whitespace.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def is_chatml_formatted(text):\n",
    "    \"\"\"Check if text is in ChatML format.\"\"\"\n",
    "    return '<|im_start|>' in text and '<|im_end|>' in text\n",
    "\n",
    "def is_kural(text):\n",
    "    \"\"\"Check if text contains Thirukkural references.\"\"\"\n",
    "    kural_markers = ['‡Æï‡ØÅ‡Æ±‡Æ≥‡Øç', '‡Æ§‡Æø‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡Æ±‡Æ≥‡Øç', 'kural', 'thirukkural', '‡ÆÖ‡Æ§‡Æø‡Æï‡Ææ‡Æ∞‡ÆÆ‡Øç']\n",
    "    text_lower = text.lower()\n",
    "    return any(m in text_lower for m in kural_markers)\n",
    "\n",
    "def to_chatml(instruction, output):\n",
    "    \"\"\"Convert instruction/output to ChatML format.\"\"\"\n",
    "    return f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{output}<|im_end|>\"\"\"\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check if Dataset Already Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if balanced dataset already exists\n",
    "SKIP_DATA_PREP = False\n",
    "\n",
    "try:\n",
    "    info = dataset_info(BALANCED_DATASET)\n",
    "    print(f\"‚úÖ Dataset {BALANCED_DATASET} already exists!\")\n",
    "    print(f\"   Created: {info.created_at}\")\n",
    "    print(f\"\\nüöÄ SKIPPING data preparation - will load directly for training\")\n",
    "    SKIP_DATA_PREP = True\n",
    "except Exception as e:\n",
    "    print(f\"üìù Dataset {BALANCED_DATASET} not found. Will create it.\")\n",
    "    SKIP_DATA_PREP = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preparation (Skip if dataset exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_DATA_PREP:\n",
    "    def extract_from_indicaling(config_name, max_samples):\n",
    "        \"\"\"Extract Tamil samples from IndicAlign.\"\"\"\n",
    "        print(f\"\\nüìö Loading {config_name}...\")\n",
    "        try:\n",
    "            ds = load_dataset(\"ai4bharat/indic-align\", config_name, split=\"train\", streaming=True)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error: {e}\")\n",
    "            return []\n",
    "        \n",
    "        samples = []\n",
    "        seen = set()\n",
    "        \n",
    "        for item in tqdm(ds, desc=config_name, total=max_samples*5):\n",
    "            if len(samples) >= max_samples:\n",
    "                break\n",
    "            \n",
    "            tamil = item.get('tam_Taml', [])\n",
    "            if not tamil or not isinstance(tamil, list) or len(tamil) == 0:\n",
    "                continue\n",
    "            \n",
    "            turns = tamil[0]\n",
    "            if not isinstance(turns, list) or len(turns) < 2:\n",
    "                continue\n",
    "            \n",
    "            user_msg = clean_text(str(turns[0]))\n",
    "            assistant_msg = clean_text(str(turns[1]))\n",
    "            \n",
    "            if not is_good_tamil_sample(user_msg) or not is_good_tamil_sample(assistant_msg):\n",
    "                continue\n",
    "            \n",
    "            key = user_msg[:100]\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            \n",
    "            samples.append({\"instruction\": user_msg, \"output\": assistant_msg, \"source\": config_name})\n",
    "        \n",
    "        print(f\"   ‚úÖ Extracted {len(samples)} samples\")\n",
    "        return samples\n",
    "    \n",
    "    print(\"‚úÖ Extraction function defined\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping - dataset exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_DATA_PREP:\n",
    "    print(\"üöÄ Extracting from IndicAlign...\")\n",
    "    diverse_samples = []\n",
    "    diverse_samples.extend(extract_from_indicaling(\"Dolly_T\", 300))\n",
    "    diverse_samples.extend(extract_from_indicaling(\"WikiHow\", 250))\n",
    "    diverse_samples.extend(extract_from_indicaling(\"Wiki_Conv\", 300))\n",
    "    diverse_samples.extend(extract_from_indicaling(\"OpenAssistant_T\", 200))\n",
    "    print(f\"\\nüìä Total from IndicAlign: {len(diverse_samples)}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping IndicAlign extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_DATA_PREP:\n",
    "    # Manual samples for short answers and behavior\n",
    "    manual_samples = [\n",
    "        # Geography\n",
    "        {\"instruction\": \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ©‡Øç ‡Æ§‡Æ≤‡Øà‡Æ®‡Æï‡Æ∞‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"‡Æö‡ØÜ‡Æ©‡Øç‡Æ©‡Øà.\", \"source\": \"manual\"},\n",
    "        {\"instruction\": \"‡Æá‡Æ®‡Øç‡Æ§‡Æø‡ÆØ‡Ææ‡Æµ‡Æø‡Æ©‡Øç ‡Æ§‡Æ≤‡Øà‡Æ®‡Æï‡Æ∞‡ÆÆ‡Øç ‡Æé‡Æ§‡ØÅ?\", \"output\": \"‡Æ™‡ØÅ‡Æ§‡ØÅ ‡Æ§‡Æø‡Æ≤‡Øç‡Æ≤‡Æø.\", \"source\": \"manual\"},\n",
    "        {\"instruction\": \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ©‡Øç ‡ÆÆ‡Ææ‡Æµ‡Æü‡Øç‡Æü‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æ§‡Øç‡Æ§‡Æ©‡Øà?\", \"output\": \"38 ‡ÆÆ‡Ææ‡Æµ‡Æü‡Øç‡Æü‡Æô‡Øç‡Æï‡Æ≥‡Øç.\", \"source\": \"manual\"},\n",
    "        \n",
    "        # Basic facts\n",
    "        {\"instruction\": \"‡Æö‡ØÇ‡Æ∞‡Æø‡ÆØ‡Æ©‡Øç ‡Æé‡Æ®‡Øç‡Æ§ ‡Æ§‡Æø‡Æö‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡Æâ‡Æ§‡Æø‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç?\", \"output\": \"‡Æï‡Æø‡Æ¥‡Æï‡Øç‡Æï‡ØÅ ‡Æ§‡Æø‡Æö‡Øà‡ÆØ‡Æø‡Æ≤‡Øç.\", \"source\": \"manual\"},\n",
    "        {\"instruction\": \"‡Æí‡Æ∞‡ØÅ ‡Æµ‡Ææ‡Æ∞‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡Æé‡Æ§‡Øç‡Æ§‡Æ©‡Øà ‡Æ®‡Ææ‡Æü‡Øç‡Æï‡Æ≥‡Øç?\", \"output\": \"‡Æè‡Æ¥‡ØÅ ‡Æ®‡Ææ‡Æü‡Øç‡Æï‡Æ≥‡Øç.\", \"source\": \"manual\"},\n",
    "        {\"instruction\": \"2+2 ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"4.\", \"source\": \"manual\"},\n",
    "        {\"instruction\": \"10 x 10 ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"100.\", \"source\": \"manual\"},\n",
    "        \n",
    "        # Tamil culture\n",
    "        {\"instruction\": \"‡Æ™‡Øä‡Æô‡Øç‡Æï‡Æ≤‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Øã‡Æ§‡ØÅ ‡Æï‡Øä‡Æ£‡Øç‡Æü‡Ææ‡Æü‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ?\", \"output\": \"‡Æ§‡Øà ‡ÆÆ‡Ææ‡Æ§‡ÆÆ‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Øç ‡Æ®‡Ææ‡Æ≥‡Øç (‡Æú‡Æ©‡Æµ‡Æ∞‡Æø 14 ‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ 15).\", \"source\": \"manual\"},\n",
    "        {\"instruction\": \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æ§‡Øç‡Æ§‡Æ©‡Øà?\", \"output\": \"247 ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øç.\", \"source\": \"manual\"},\n",
    "        {\"instruction\": \"‡Æö‡Æø‡Æ≤‡Æ™‡Øç‡Æ™‡Æ§‡Æø‡Æï‡Ææ‡Æ∞‡Æ§‡Øç‡Æ§‡Øà ‡Æé‡Æ¥‡ØÅ‡Æ§‡Æø‡ÆØ‡Æµ‡Æ∞‡Øç ‡ÆØ‡Ææ‡Æ∞‡Øç?\", \"output\": \"‡Æá‡Æ≥‡Æô‡Øç‡Æï‡Øã‡Æµ‡Æü‡Æø‡Æï‡Æ≥‡Øç.\", \"source\": \"manual\"},\n",
    "        \n",
    "        # Behavior\n",
    "        {\"instruction\": \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç\", \"output\": \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç! ‡Æ®‡Ææ‡Æ©‡Øç ‡Æµ‡Æ¥‡Æø. ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æâ‡Æ§‡Æµ ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç?\", \"source\": \"behavior\"},\n",
    "        {\"instruction\": \"hi\", \"output\": \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç! ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æâ‡Æ§‡Æµ‡Æ≤‡Ææ‡ÆÆ‡Øç?\", \"source\": \"behavior\"},\n",
    "        {\"instruction\": \"2050-‡Æ≤‡Øç ‡ÆØ‡Ææ‡Æ∞‡Øç ‡Æ™‡Æø‡Æ∞‡Æ§‡ÆÆ‡Æ∞‡Øç ‡ÆÜ‡Æµ‡Ææ‡Æ∞‡Øç?\", \"output\": \"‡Æé‡Æ§‡Æø‡Æ∞‡Øç‡Æï‡Ææ‡Æ≤‡Æ§‡Øç‡Æ§‡Øà ‡Æï‡Æ£‡Æø‡Æï‡Øç‡Æï ‡Æé‡Æ©‡Øç‡Æ©‡Ææ‡Æ≤‡Øç ‡ÆÆ‡ØÅ‡Æü‡Æø‡ÆØ‡Ææ‡Æ§‡ØÅ. ‡Æ§‡ØÜ‡Æ∞‡Æø‡ÆØ‡Æµ‡Æø‡Æ≤‡Øç‡Æ≤‡Øà.\", \"source\": \"behavior\"},\n",
    "        {\"instruction\": \"‡Æé‡Æ©‡Øç‡Æ©‡ØÅ‡Æü‡Øà‡ÆØ ‡Æï‡Æü‡Æµ‡ØÅ‡Æö‡Øç‡Æö‡Øä‡Æ≤‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ§‡Æ©‡Æø‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡Æ§‡Æï‡Æµ‡Æ≤‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æ©‡Øç‡Æ©‡Æø‡Æü‡ÆÆ‡Øç ‡Æá‡Æ≤‡Øç‡Æ≤‡Øà.\", \"source\": \"behavior\"},\n",
    "        {\"instruction\": \"‡Æ®‡Æ©‡Øç‡Æ±‡Æø\", \"output\": \"‡ÆÆ‡Æï‡Æø‡Æ¥‡Øç‡Æö‡Øç‡Æö‡Æø! ‡Æµ‡Øá‡Æ±‡ØÅ ‡Æâ‡Æ§‡Æµ‡Æø ‡Æ§‡Øá‡Æµ‡Øà‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Ææ‡Æ≤‡Øç ‡Æï‡Øá‡Æ≥‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç.\", \"source\": \"behavior\"},\n",
    "    ]\n",
    "    \n",
    "    diverse_samples.extend(manual_samples)\n",
    "    print(f\"üìä Total after manual samples: {len(diverse_samples)}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping manual samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_DATA_PREP:\n",
    "    print(f\"\\nüìö Loading existing dataset from {EXISTING_DATASET}...\")\n",
    "    existing_ds = load_dataset(EXISTING_DATASET, split=\"train\")\n",
    "    print(f\"   Loaded {len(existing_ds)} samples\")\n",
    "    \n",
    "    # Filter for ChatML ONLY\n",
    "    existing_kural_chatml = []\n",
    "    existing_other_chatml = []\n",
    "    \n",
    "    for item in tqdm(existing_ds, desc=\"Filtering ChatML\"):\n",
    "        text = item.get('text', '')\n",
    "        if is_chatml_formatted(text):\n",
    "            if is_kural(text):\n",
    "                existing_kural_chatml.append({\"text\": text})\n",
    "            else:\n",
    "                existing_other_chatml.append({\"text\": text})\n",
    "    \n",
    "    print(f\"\\nüìä ChatML samples:\")\n",
    "    print(f\"   Kural: {len(existing_kural_chatml)}\")\n",
    "    print(f\"   Other: {len(existing_other_chatml)}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping existing dataset loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_DATA_PREP:\n",
    "    # Downsample Thirukkural to ~25%\n",
    "    total_other = len(existing_other_chatml)\n",
    "    target_kural_pct = 0.25\n",
    "    target_kural_count = int(target_kural_pct * total_other / (1 - target_kural_pct))\n",
    "    \n",
    "    if len(existing_kural_chatml) > target_kural_count:\n",
    "        downsampled_kural = random.sample(existing_kural_chatml, target_kural_count)\n",
    "    else:\n",
    "        downsampled_kural = existing_kural_chatml\n",
    "    \n",
    "    print(f\"üéØ Downsampled Kural: {len(existing_kural_chatml)} ‚Üí {len(downsampled_kural)}\")\n",
    "    \n",
    "    # Convert diverse to ChatML and combine\n",
    "    diverse_formatted = [{\"text\": to_chatml(s[\"instruction\"], s[\"output\"])} for s in diverse_samples]\n",
    "    \n",
    "    final_samples = []\n",
    "    final_samples.extend(downsampled_kural)\n",
    "    final_samples.extend(existing_other_chatml)\n",
    "    final_samples.extend(diverse_formatted)\n",
    "    random.shuffle(final_samples)\n",
    "    \n",
    "    print(f\"\\nüìä Final dataset: {len(final_samples)} samples\")\n",
    "    \n",
    "    # Verify 100% ChatML\n",
    "    chatml_count = sum(1 for s in final_samples if is_chatml_formatted(s[\"text\"]))\n",
    "    if chatml_count != len(final_samples):\n",
    "        raise ValueError(\"Not all samples are ChatML formatted!\")\n",
    "    print(f\"‚úÖ 100% ChatML verified\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping dataset combination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_DATA_PREP:\n",
    "    # Save and upload\n",
    "    os.makedirs(\"/kaggle/working/balanced_sft\", exist_ok=True)\n",
    "    \n",
    "    split_idx = int(0.95 * len(final_samples))\n",
    "    train_samples = final_samples[:split_idx]\n",
    "    val_samples = final_samples[split_idx:]\n",
    "    \n",
    "    with open(\"/kaggle/working/balanced_sft/train.jsonl\", 'w') as f:\n",
    "        for s in train_samples:\n",
    "            f.write(json.dumps(s, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    with open(\"/kaggle/working/balanced_sft/val.jsonl\", 'w') as f:\n",
    "        for s in val_samples:\n",
    "            f.write(json.dumps(s, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"üíæ Saved: {len(train_samples)} train, {len(val_samples)} val\")\n",
    "    \n",
    "    # Upload to HuggingFace\n",
    "    api = HfApi()\n",
    "    api.create_repo(BALANCED_DATASET, repo_type=\"dataset\", exist_ok=True)\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"/kaggle/working/balanced_sft/train.jsonl\",\n",
    "        path_in_repo=\"train.jsonl\",\n",
    "        repo_id=BALANCED_DATASET,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"/kaggle/working/balanced_sft/val.jsonl\",\n",
    "        path_in_repo=\"val.jsonl\",\n",
    "        repo_id=BALANCED_DATASET,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    print(f\"‚úÖ Uploaded to {BALANCED_DATASET}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìö Loading balanced dataset...\")\n",
    "balanced_ds = load_dataset(BALANCED_DATASET, split=\"train\")\n",
    "print(f\"‚úÖ Loaded {len(balanced_ds)} samples\")\n",
    "\n",
    "# Verify ChatML format\n",
    "sample = balanced_ds[0]['text'][:200]\n",
    "print(f\"\\nüìù Sample: {sample}...\")\n",
    "if \"<|im_start|>\" in sample:\n",
    "    print(\"‚úÖ ChatML format verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Model with 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüì• Loading model and tokenizer...\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# 4-bit quantization - use float16 compute dtype\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model - MUST specify torch_dtype to avoid bf16 default\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\":0},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model.num_parameters():,} params\")\n",
    "print(f\"   torch_dtype: float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Verify no bf16 parameters\n",
    "bf16_count = sum(1 for _, p in model.named_parameters() if p.dtype == torch.bfloat16)\n",
    "if bf16_count > 0:\n",
    "    print(f\"‚ö†Ô∏è Found {bf16_count} bf16 parameters - converting to fp16\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dtype == torch.bfloat16:\n",
    "            param.data = param.data.to(torch.float16)\n",
    "else:\n",
    "    print(\"‚úÖ No bf16 parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training (FP32 Mode for P100 Compatibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP32 training - Qwen3 has internal bf16 ops that P100 can't handle with AMP\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"/kaggle/working/vazhi-v3_3\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=25,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,  # DISABLED - Qwen3 has internal bf16\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=512,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=balanced_ds,\n",
    "    args=sft_config,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized (FP32 mode)\")\n",
    "print(f\"   Epochs: 2\")\n",
    "print(f\"   Batch size: 1 x 16 = 16 effective\")\n",
    "print(f\"   Mode: FP32 (P100 compatible)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "trainer.train()\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save and Push to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Saving model...\")\n",
    "trainer.save_model(\"/kaggle/working/vazhi-v3_3-final\")\n",
    "\n",
    "print(\"üîÄ Merging LoRA weights...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Push to HuggingFace\n",
    "api = HfApi()\n",
    "api.create_repo(OUTPUT_MODEL, exist_ok=True)\n",
    "\n",
    "print(f\"üì§ Pushing to {OUTPUT_MODEL}...\")\n",
    "merged_model.push_to_hub(OUTPUT_MODEL, private=False)\n",
    "tokenizer.push_to_hub(OUTPUT_MODEL, private=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Model uploaded: https://huggingface.co/{OUTPUT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.config.use_cache = True\n",
    "\n",
    "test_prompts = [\n",
    "    \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç\",\n",
    "    \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ©‡Øç ‡Æ§‡Æ≤‡Øà‡Æ®‡Æï‡Æ∞‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\",\n",
    "    \"2+2 ‡Æé‡Æ©‡Øç‡Æ©?\",\n",
    "    \"‡Æ™‡Øä‡Æô‡Øç‡Æï‡Æ≤‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Øã‡Æ§‡ØÅ ‡Æï‡Øä‡Æ£‡Øç‡Æü‡Ææ‡Æü‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ?\",\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Testing model...\\n\")\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    full_prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(merged_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = merged_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.5,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.3,\n",
    "            no_repeat_ngram_size=3,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    if \"<|im_start|>assistant\" in response:\n",
    "        response = response.split(\"<|im_start|>assistant\")[-1]\n",
    "        response = response.split(\"<|im_end|>\")[0].strip()\n",
    "    \n",
    "    print(f\"Q: {prompt}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **FP32 training** - Qwen3 has internal bf16 ops incompatible with P100\n",
    "- **ChatML only** - No raw text mixing\n",
    "- **Thirukkural ~25%** - Downsampled from 71%\n",
    "- **4-bit QLoRA** - Memory efficient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
