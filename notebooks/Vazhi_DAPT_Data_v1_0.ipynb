{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI DAPT Data Prep v1.0 — Tamil Corpus for DAPT\n",
    "\n",
    "**Pipeline Step 1 of 3:** Prepare filtered, packed Tamil training data.\n",
    "\n",
    "```\n",
    "Step 1 (THIS NOTEBOOK): Data Prep — CPU only, no GPU needed\n",
    "  → Input:  AI4Bharat Sangraha verified Tamil corpus (streaming)\n",
    "  → Output: CryptoYogi/vazhi-dapt-tamil-v1_0 (packed 1024-token blocks on HF)\n",
    "\n",
    "Step 2: DAPT Training — Kaggle P100 GPU\n",
    "  → Input:  Packed dataset from Step 1\n",
    "  → Output: CryptoYogi/qwen3-0.6b-tamil (reusable Tamil base model)\n",
    "\n",
    "Step 3: SFT — Kaggle P100 GPU\n",
    "  → Input:  DAPT'd model + ChatML instruction pairs\n",
    "  → Output: CryptoYogi/vazhi-qwen3-v3_9\n",
    "```\n",
    "\n",
    "**Why separate data prep?**\n",
    "- No GPU needed — runs on local machine or Kaggle CPU\n",
    "- If DAPT training fails (OOM, disconnect, wrong LR), data is already on HF\n",
    "- Can experiment with different token budgets without re-downloading\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Loads Qwen3-0.6B-Base tokenizer (CPU only)\n",
    "2. Measures actual tokens/doc on 200 Sangraha samples (GPT5.2 #2)\n",
    "3. Streams & filters Sangraha verified Tamil (GPT5.2 #6)\n",
    "4. Packs into 1024-token blocks for causal LM training (GPT5.2 #5)\n",
    "5. Uploads packed dataset to HuggingFace\n",
    "\n",
    "**Runtime:** ~30-60 min on CPU (streaming + tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Only need transformers (tokenizer) + datasets + huggingface_hub\n# No torch/peft/bitsandbytes/trl needed — this is CPU-only\n# Pin transformers<5.0 — transformers 5.x pulls scipy/sklearn deps\n# that conflict with Colab's pre-installed numpy/scipy versions\n!pip install -q -U \\\n  \"transformers>=4.45.0,<5.0.0\" \\\n  \"datasets>=2.21.0\" \\\n  \"huggingface_hub>=0.24.7\"\n\nprint(\"\\u2705 Dependencies installed (CPU-only — no GPU packages needed)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport random\nimport hashlib\nimport numpy as np\nfrom collections import Counter\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom transformers import AutoTokenizer\nfrom huggingface_hub import login, HfApi\n\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\n# === TOKENIZER SOURCE ===\n# Must match the model that will be trained in the DAPT notebook\nBASE_MODEL = \"Qwen/Qwen3-0.6B-Base\"\n\n# === OUTPUT ===\nOUTPUT_DATASET = \"CryptoYogi/vazhi-dapt-tamil-v1_0\"\n\n# === CORPUS SOURCE ===\nSANGRAHA_CONFIG = \"verified\"   # Cleanest Tamil data\nSANGRAHA_SPLIT = \"tam\"         # Tamil split\n\n# === TOKEN BUDGET ===\n# GPT5.2 #3: Control by token count, not epochs\nTARGET_TOKENS = 30_000_000     # 30M tokens — sweet spot for 0.6B model\nMAX_SEQ_LENGTH = 1024          # Pack into 1024-token blocks\n\n# === DATA QUALITY FILTERS (GPT5.2 #6) ===\nMIN_TAMIL_PCT = 50             # Minimum Tamil character percentage (verified corpus min is 51%)\nMIN_DOC_CHARS = 200            # Drop very short docs\nMAX_DOC_CHARS = 8000           # Drop very long docs (boilerplate risk)\nMAX_REPETITION_RATIO = 0.5     # Drop docs with >50% repeated lines\n\n# === EVAL SPLIT ===\nEVAL_PCT = 0.02                # 2% held out for perplexity eval during training\n\nprint(\"\\U0001f4cb DAPT Data Prep v1.0 Config:\")\nprint(f\"   Tokenizer from:  {BASE_MODEL}\")\nprint(f\"   Output dataset:  {OUTPUT_DATASET}\")\nprint(f\"   Token budget:    {TARGET_TOKENS:,}\")\nprint(f\"   Block size:      {MAX_SEQ_LENGTH} tokens\")\nprint(f\"   Source:          Sangraha {SANGRAHA_CONFIG}/{SANGRAHA_SPLIT}\")\nprint(f\"   Filters:         Tamil>={MIN_TAMIL_PCT}%, chars {MIN_DOC_CHARS}-{MAX_DOC_CHARS}, dedup, no repetition\")\nprint(f\"   Eval holdout:    {EVAL_PCT:.0%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "# On Kaggle:\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# secrets = UserSecretsClient()\n",
    "# hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "# On local machine:\n",
    "# hf_token = os.environ.get(\"HF_TOKEN\") or input(\"Enter HF token: \")\n",
    "\n",
    "# Uncomment the appropriate method above, then:\n",
    "# login(token=hf_token)\n",
    "\n",
    "# Or if you've already run `huggingface-cli login`:\n",
    "login()\n",
    "print(\"\\u2705 Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Tokenizer & Measure Token Counts\n",
    "\n",
    "**GPT5.2 #2:** Don't estimate tokens from chars — measure with the actual tokenizer.\n",
    "Tamil tokenization varies widely by content type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\U0001f4e5 Loading tokenizer from {BASE_MODEL} (CPU only)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "print(f\"\\u2705 Tokenizer ready: {len(tokenizer)} tokens\")\n",
    "print(f\"   eos_token: {tokenizer.eos_token!r} (ID {tokenizer.eos_token_id})\")\n",
    "print(f\"   pad_token: {tokenizer.pad_token!r} (ID {tokenizer.pad_token_id})\")\n",
    "\n",
    "# If pad_token is None, set to eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    print(f\"   \\u26a0\\ufe0f  Set pad_token = eos_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HELPER FUNCTIONS ===\n",
    "\n",
    "def count_tamil_chars(text):\n",
    "    \"\"\"Count Tamil Unicode characters (U+0B80 to U+0BFF).\"\"\"\n",
    "    return sum(1 for c in text if '\\u0B80' <= c <= '\\u0BFF')\n",
    "\n",
    "def tamil_char_pct(text):\n",
    "    \"\"\"Tamil character percentage of total text length.\"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    return 100.0 * count_tamil_chars(text) / len(text)\n",
    "\n",
    "def has_excessive_repetition(text, threshold=MAX_REPETITION_RATIO):\n",
    "    \"\"\"Check if a doc has too many repeated lines (boilerplate/headers).\"\"\"\n",
    "    lines = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n",
    "    if len(lines) < 3:\n",
    "        return False\n",
    "    line_counts = Counter(lines)\n",
    "    most_common_count = line_counts.most_common(1)[0][1]\n",
    "    return most_common_count / len(lines) > threshold\n",
    "\n",
    "def text_hash(text):\n",
    "    \"\"\"Fast MD5 hash for dedup.\"\"\"\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "print(\"\\u2705 Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MEASURE ACTUAL TOKENIZATION ON 200 SAMPLE DOCS ===\n",
    "# GPT5.2 #2: \"Sample 200 docs, run tokenizer, decide N docs from actual tokens\"\n",
    "\n",
    "print(f\"\\U0001f50d Sampling 200 docs from Sangraha {SANGRAHA_CONFIG}/{SANGRAHA_SPLIT}...\")\n",
    "ds_stream = load_dataset(\n",
    "    \"ai4bharat/sangraha\", SANGRAHA_CONFIG, split=SANGRAHA_SPLIT, streaming=True\n",
    ")\n",
    "\n",
    "sample_docs = []\n",
    "for item in ds_stream:\n",
    "    sample_docs.append(item[\"text\"])\n",
    "    if len(sample_docs) >= 200:\n",
    "        break\n",
    "\n",
    "# Tokenize each and measure\n",
    "doc_tokens = []\n",
    "doc_tamil_pcts = []\n",
    "doc_lengths = []\n",
    "\n",
    "for doc in sample_docs:\n",
    "    tokens = tokenizer.encode(doc, add_special_tokens=False)\n",
    "    doc_tokens.append(len(tokens))\n",
    "    doc_tamil_pcts.append(tamil_char_pct(doc))\n",
    "    doc_lengths.append(len(doc))\n",
    "\n",
    "avg_tokens = np.mean(doc_tokens)\n",
    "median_tokens = np.median(doc_tokens)\n",
    "avg_tamil = np.mean(doc_tamil_pcts)\n",
    "avg_chars = np.mean(doc_lengths)\n",
    "tokens_per_char = np.mean([t / max(c, 1) for t, c in zip(doc_tokens, doc_lengths)])\n",
    "\n",
    "print(f\"\\n\\U0001f4ca Tokenization Analysis (200 docs):\")\n",
    "print(f\"   Avg tokens/doc:  {avg_tokens:.0f}\")\n",
    "print(f\"   Median tokens:   {median_tokens:.0f}\")\n",
    "print(f\"   Min/Max tokens:  {min(doc_tokens)} / {max(doc_tokens)}\")\n",
    "print(f\"   Avg chars/doc:   {avg_chars:.0f}\")\n",
    "print(f\"   Tokens/char:     {tokens_per_char:.2f}\")\n",
    "print(f\"   Avg Tamil %:     {avg_tamil:.1f}%\")\n",
    "print(f\"   Avg Latin %:     {np.mean([100 * sum(c.isascii() for c in d) / max(len(d), 1) for d in sample_docs]):.1f}%\")\n",
    "\n",
    "# Estimate docs needed\n",
    "docs_needed_estimate = int(TARGET_TOKENS / avg_tokens)\n",
    "print(f\"\\n\\U0001f3af For {TARGET_TOKENS:,} token budget:\")\n",
    "print(f\"   Estimated docs needed (pre-filter): ~{docs_needed_estimate:,}\")\n",
    "print(f\"   (Actual count depends on filtering — next cell)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stream, Filter & Collect Sangraha Tamil Docs\n",
    "\n",
    "**GPT5.2 #6:** Even \"verified\" corpora need filtering:\n",
    "- Tamil character ratio >= 40%\n",
    "- Length bounds: 200-8000 chars\n",
    "- Drop docs with extreme line repetition\n",
    "- Hash-based exact dedup\n",
    "\n",
    "Streams until token budget is met (with 10% buffer for EOS overhead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\U0001f4e5 Streaming Sangraha {SANGRAHA_CONFIG}/{SANGRAHA_SPLIT} with filters...\")\n",
    "print(f\"   Tamil >= {MIN_TAMIL_PCT}% | Chars {MIN_DOC_CHARS}-{MAX_DOC_CHARS} | Dedup | No repetition\")\n",
    "print(f\"   Token budget: {TARGET_TOKENS:,} (+ 10% buffer)\")\n",
    "print()\n",
    "\n",
    "ds_stream = load_dataset(\n",
    "    \"ai4bharat/sangraha\", SANGRAHA_CONFIG, split=SANGRAHA_SPLIT, streaming=True\n",
    ")\n",
    "\n",
    "clean_texts = []\n",
    "seen_hashes = set()\n",
    "total_tokens = 0\n",
    "stats = {\n",
    "    \"total_seen\": 0,\n",
    "    \"dropped_short\": 0,\n",
    "    \"dropped_long\": 0,\n",
    "    \"dropped_tamil\": 0,\n",
    "    \"dropped_repetition\": 0,\n",
    "    \"dropped_dedup\": 0,\n",
    "    \"kept\": 0,\n",
    "}\n",
    "\n",
    "BUFFER_FACTOR = 1.1\n",
    "effective_budget = int(TARGET_TOKENS * BUFFER_FACTOR)\n",
    "\n",
    "for item in ds_stream:\n",
    "    stats[\"total_seen\"] += 1\n",
    "    text = item.get(\"text\", \"\").strip()\n",
    "\n",
    "    # Length filter\n",
    "    if len(text) < MIN_DOC_CHARS:\n",
    "        stats[\"dropped_short\"] += 1\n",
    "        continue\n",
    "    if len(text) > MAX_DOC_CHARS:\n",
    "        stats[\"dropped_long\"] += 1\n",
    "        continue\n",
    "\n",
    "    # Tamil character ratio\n",
    "    t_pct = tamil_char_pct(text)\n",
    "    if t_pct < MIN_TAMIL_PCT:\n",
    "        stats[\"dropped_tamil\"] += 1\n",
    "        continue\n",
    "\n",
    "    # Repetition filter\n",
    "    if has_excessive_repetition(text):\n",
    "        stats[\"dropped_repetition\"] += 1\n",
    "        continue\n",
    "\n",
    "    # Exact dedup\n",
    "    h = text_hash(text)\n",
    "    if h in seen_hashes:\n",
    "        stats[\"dropped_dedup\"] += 1\n",
    "        continue\n",
    "    seen_hashes.add(h)\n",
    "\n",
    "    # Count tokens\n",
    "    n_tokens = len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "    clean_texts.append(text)\n",
    "    total_tokens += n_tokens\n",
    "    stats[\"kept\"] += 1\n",
    "\n",
    "    if stats[\"kept\"] % 2000 == 0:\n",
    "        pct = 100 * total_tokens / TARGET_TOKENS\n",
    "        print(f\"   ...{stats['kept']:,} docs kept, {total_tokens:,} tokens ({pct:.0f}% of budget)\")\n",
    "\n",
    "    # Stop when token budget is met\n",
    "    if total_tokens >= effective_budget:\n",
    "        print(f\"\\n\\u2705 Token budget reached!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n\\U0001f4ca Filtering Results:\")\n",
    "print(f\"   Total scanned:      {stats['total_seen']:,}\")\n",
    "print(f\"   Dropped (short):    {stats['dropped_short']:,}\")\n",
    "print(f\"   Dropped (long):     {stats['dropped_long']:,}\")\n",
    "print(f\"   Dropped (Tamil%):   {stats['dropped_tamil']:,}\")\n",
    "print(f\"   Dropped (repeat):   {stats['dropped_repetition']:,}\")\n",
    "print(f\"   Dropped (dedup):    {stats['dropped_dedup']:,}\")\n",
    "print(f\"   \\u2705 Kept:           {stats['kept']:,} docs\")\n",
    "print(f\"   \\u2705 Total tokens:   {total_tokens:,}\")\n",
    "\n",
    "if total_tokens < TARGET_TOKENS * 0.9:\n",
    "    print(f\"\\n\\u26a0\\ufe0f  Only got {total_tokens:,} tokens ({100 * total_tokens / TARGET_TOKENS:.0f}% of budget).\")\n",
    "    print(f\"   Consider: relaxing filters, adding unverified/synthetic configs, or reducing TARGET_TOKENS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pack Into 1024-Token Blocks\n",
    "\n",
    "**GPT5.2 #5:** Don't pad each doc to max_length — concatenate all docs with EOS separators\n",
    "into a continuous token stream, then split into fixed-length blocks.\n",
    "This is the standard causal LM training format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\U0001f4e6 Packing {len(clean_texts):,} docs into {MAX_SEQ_LENGTH}-token blocks...\")\n",
    "\n",
    "# Step 1: Tokenize all texts, concatenate with EOS separators\n",
    "all_token_ids = []\n",
    "eos_id = tokenizer.eos_token_id\n",
    "\n",
    "for i, text in enumerate(clean_texts):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    all_token_ids.extend(tokens)\n",
    "    all_token_ids.append(eos_id)\n",
    "\n",
    "    if (i + 1) % 5000 == 0:\n",
    "        print(f\"   ...tokenized {i + 1:,}/{len(clean_texts):,} docs\")\n",
    "\n",
    "print(f\"   Total token stream: {len(all_token_ids):,} tokens\")\n",
    "\n",
    "# Step 2: Split into fixed-length blocks (discard partial tail)\n",
    "n_blocks = len(all_token_ids) // MAX_SEQ_LENGTH\n",
    "trimmed = all_token_ids[: n_blocks * MAX_SEQ_LENGTH]\n",
    "blocks = [trimmed[i * MAX_SEQ_LENGTH : (i + 1) * MAX_SEQ_LENGTH] for i in range(n_blocks)]\n",
    "\n",
    "print(f\"\\n\\u2705 Packed into {len(blocks):,} blocks of {MAX_SEQ_LENGTH} tokens\")\n",
    "print(f\"   Total training tokens: {len(blocks) * MAX_SEQ_LENGTH:,}\")\n",
    "print(f\"   Discarded tail:        {len(all_token_ids) - len(trimmed):,} tokens\")\n",
    "\n",
    "# Step 3: Quick sanity — decode a sample block\n",
    "sample_decoded = tokenizer.decode(blocks[0][:100])\n",
    "print(f\"\\n\\U0001f50d Sample block (first 100 tokens):\")\n",
    "print(f\"   Tamil%: {tamil_char_pct(sample_decoded):.0f}%\")\n",
    "print(f\"   Text:   {sample_decoded[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Dataset with Train/Eval Split & Upload to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HF Dataset from packed blocks\n",
    "packed_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": blocks,\n",
    "    \"attention_mask\": [[1] * MAX_SEQ_LENGTH for _ in blocks],\n",
    "    \"labels\": [list(b) for b in blocks],\n",
    "})\n",
    "\n",
    "# Train/eval split (2% for perplexity eval during training)\n",
    "split = packed_dataset.train_test_split(test_size=EVAL_PCT, seed=RANDOM_SEED)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": split[\"train\"],\n",
    "    \"validation\": split[\"test\"],\n",
    "})\n",
    "\n",
    "print(f\"\\U0001f4ca Dataset created:\")\n",
    "print(f\"   Train:      {len(dataset_dict['train']):,} blocks ({len(dataset_dict['train']) * MAX_SEQ_LENGTH:,} tokens)\")\n",
    "print(f\"   Validation: {len(dataset_dict['validation']):,} blocks ({len(dataset_dict['validation']) * MAX_SEQ_LENGTH:,} tokens)\")\n",
    "print(f\"   Block size: {MAX_SEQ_LENGTH} tokens\")\n",
    "print(f\"   Columns:    {list(dataset_dict['train'].column_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to HuggingFace\n",
    "print(f\"\\U0001f4e4 Uploading to {OUTPUT_DATASET}...\")\n",
    "\n",
    "dataset_dict.push_to_hub(\n",
    "    OUTPUT_DATASET,\n",
    "    private=False,\n",
    "    commit_message=(\n",
    "        f\"DAPT data v1.0: {len(blocks):,} packed blocks of {MAX_SEQ_LENGTH} tokens | \"\n",
    "        f\"Source: Sangraha {SANGRAHA_CONFIG}/{SANGRAHA_SPLIT} | \"\n",
    "        f\"Filters: Tamil>={MIN_TAMIL_PCT}%, chars {MIN_DOC_CHARS}-{MAX_DOC_CHARS}, dedup | \"\n",
    "        f\"Tokenizer: {BASE_MODEL}\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"\\n\\u2705 Dataset uploaded: https://huggingface.co/datasets/{OUTPUT_DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the uploaded dataset can be loaded back\n",
    "print(f\"\\U0001f50d Verifying upload by loading back from HF...\")\n",
    "verify_ds = load_dataset(OUTPUT_DATASET)\n",
    "\n",
    "print(f\"   Train:      {len(verify_ds['train']):,} blocks\")\n",
    "print(f\"   Validation: {len(verify_ds['validation']):,} blocks\")\n",
    "\n",
    "# Verify a sample block\n",
    "sample = verify_ds[\"train\"][0]\n",
    "assert len(sample[\"input_ids\"]) == MAX_SEQ_LENGTH, f\"Block size mismatch: {len(sample['input_ids'])}\"\n",
    "assert len(sample[\"labels\"]) == MAX_SEQ_LENGTH, f\"Labels size mismatch: {len(sample['labels'])}\"\n",
    "assert sample[\"input_ids\"] == sample[\"labels\"], \"input_ids and labels should match for causal LM\"\n",
    "\n",
    "sample_text = tokenizer.decode(sample[\"input_ids\"][:50])\n",
    "print(f\"   Sample text: {sample_text[:150]}...\")\n",
    "print(f\"   Tamil%: {tamil_char_pct(sample_text):.0f}%\")\n",
    "\n",
    "print(f\"\\n\\u2705 Verification passed!\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"\\U0001f4cb SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   Dataset:    {OUTPUT_DATASET}\")\n",
    "print(f\"   Source:     Sangraha {SANGRAHA_CONFIG}/{SANGRAHA_SPLIT}\")\n",
    "print(f\"   Docs kept:  {stats['kept']:,} (scanned {stats['total_seen']:,})\")\n",
    "print(f\"   Tokens:     {total_tokens:,} (budget: {TARGET_TOKENS:,})\")\n",
    "print(f\"   Blocks:     {len(blocks):,} x {MAX_SEQ_LENGTH} tokens\")\n",
    "print(f\"   Tokenizer:  {BASE_MODEL}\")\n",
    "print(f\"\\n\\U0001f449 Next step: Run Vazhi_DAPT_v1_0_Tamil.ipynb on Kaggle GPU\")\n",
    "print(f\"   It will load this dataset with:\")\n",
    "print(f'   ds = load_dataset(\"{OUTPUT_DATASET}\")')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
