"""
Kaggle Notebook Cells for SFT v3.1 - Dataset Rebalancing
=========================================================

Copy these cells into your Kaggle training notebook BEFORE the SFT training.
These cells will:
1. Create diverse QA pack from IndicAlign
2. Rebalance the existing dataset
3. Upload balanced dataset to HuggingFace

Run after: Installing dependencies
Run before: Loading model and starting training
"""

# ============================================================
# CELL 1: Install additional dependencies
# ============================================================

# !pip install datasets tqdm -q

# ============================================================
# CELL 2: Configuration
# ============================================================

import json
import random
import re
from collections import defaultdict
from datasets import load_dataset
from tqdm.auto import tqdm
from huggingface_hub import login, HfApi

# Config
HF_REPO = "CryptoYogi/vazhi-tamil-sft-v3_1"
RANDOM_SEED = 42
random.seed(RANDOM_SEED)

# System prompt (same as training)
SYSTEM_PROMPT = "роирпАроЩрпНроХро│рпН VAZHI (ро╡ро┤ро┐), родрооро┐ро┤рпН роороХрпНроХро│рпБроХрпНроХро╛рой AI роЙродро╡ро┐ропро╛ро│ро░рпН. родрооро┐ро┤ро┐ро▓рпН родрпЖро│ро┐ро╡ро╛роХро╡рпБроорпН роЙродро╡ро┐ропро╛роХро╡рпБроорпН рокродро┐ро▓ро│ро┐ропрпБроЩрпНроХро│рпН. родрпЖро░ро┐ропро╛ро╡ро┐роЯрпНроЯро╛ро▓рпН \"родрпЖро░ро┐ропро╡ро┐ро▓рпНро▓рпИ\" роОройрпНро▒рпБ роЪрпКро▓рпНро▓рпБроЩрпНроХро│рпН."

print("тЬЕ Configuration loaded")

# ============================================================
# CELL 3: Extract Diverse QA from IndicAlign
# ============================================================

# NOTE: IndicAlign's tam_Taml field ALREADY contains Tamil translations
# done by AI4Bharat using IndicTrans2. We extract from this field directly.

def clean_text(text):
    """Clean and normalize Tamil text."""
    if not text or not isinstance(text, str):
        return ""
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'<[^>]+>', '', text)
    return text

def count_tamil_chars(text):
    """Count Tamil characters in text."""
    return sum(1 for c in text if '\u0B80' <= c <= '\u0BFF')

def is_good_tamil_sample(text):
    """Check if text is good quality Tamil (at least 30% Tamil chars)."""
    if not text or len(text) < 20:
        return False
    tamil_chars = count_tamil_chars(text)
    # At least 30% Tamil characters
    if len(text) > 0 and tamil_chars / len(text) < 0.3:
        return False
    if len(text) > 2000:
        return False
    return True

def extract_from_indicaling(config_name, max_samples):
    """Extract Tamil samples from IndicAlign config.

    IndicAlign structure: tam_Taml contains Tamil translations (already translated)
    Format: List of conversation turns [user_turn, assistant_turn, ...]
    """
    print(f"\nЁЯУЪ Loading {config_name}...")
    try:
        ds = load_dataset("ai4bharat/indic-align", config_name, split="train", streaming=True)
    except Exception as e:
        print(f"   тЪая╕П Error: {e}")
        return []

    samples = []
    seen = set()
    skipped_non_tamil = 0
    skipped_short = 0

    for item in tqdm(ds, desc=config_name, total=max_samples*5):
        if len(samples) >= max_samples:
            break

        # tam_Taml contains already-translated Tamil text
        tamil = item.get('tam_Taml', '')
        if not tamil:
            continue

        # Handle list format (conversation turns)
        if isinstance(tamil, list) and len(tamil) >= 2:
            user_msg = clean_text(str(tamil[0]))
            assistant_msg = clean_text(str(tamil[1]))
        else:
            continue

        # Verify it's actually Tamil (not English that slipped through)
        if not is_good_tamil_sample(user_msg):
            skipped_non_tamil += 1
            continue
        if not is_good_tamil_sample(assistant_msg):
            skipped_short += 1
            continue

        key = user_msg[:100]
        if key in seen:
            continue
        seen.add(key)

        samples.append({
            "instruction": user_msg,
            "output": assistant_msg,
            "source": config_name
        })

    print(f"   тЬЕ Extracted {len(samples)} Tamil samples")
    print(f"   тПня╕П Skipped {skipped_non_tamil} (not Tamil), {skipped_short} (too short)")

    # Verify: Show first sample to confirm Tamil
    if samples:
        print(f"   ЁЯУЭ Sample verification:")
        print(f"      User: {samples[0]['instruction'][:80]}...")
        print(f"      Asst: {samples[0]['output'][:80]}...")

    return samples

# Extract from multiple sources
# These datasets contain Tamil translations in tam_Taml field
print("ЁЯЪА Extracting diverse QA from IndicAlign (tam_Taml field = Tamil translations)...")
diverse_samples = []

# Each config has different content types:
# - Dolly_T: Instruction following (translated from Dolly-15k)
# - WikiHow: How-to guides
# - Wiki_Conv: Short Wikipedia conversations
# - OpenAssistant_T: Assistant dialogs
diverse_samples.extend(extract_from_indicaling("Dolly_T", 250))
diverse_samples.extend(extract_from_indicaling("WikiHow", 200))
diverse_samples.extend(extract_from_indicaling("Wiki_Conv", 250))
diverse_samples.extend(extract_from_indicaling("OpenAssistant_T", 150))

print(f"\nЁЯУК Total extracted: {len(diverse_samples)}")

# Verify Tamil content distribution
tamil_char_pcts = []
for s in diverse_samples[:100]:
    text = s['instruction'] + s['output']
    pct = 100 * count_tamil_chars(text) / len(text) if text else 0
    tamil_char_pcts.append(pct)
avg_tamil_pct = sum(tamil_char_pcts) / len(tamil_char_pcts) if tamil_char_pcts else 0
print(f"ЁЯУИ Average Tamil character %: {avg_tamil_pct:.1f}%")

# ============================================================
# CELL 4: Add Manual Short Answer & Behavior Samples
# ============================================================

manual_samples = [
    # Geography
    {"instruction": "родрооро┐ро┤рпНроиро╛роЯрпНроЯро┐ройрпН родро▓рпИроироХро░роорпН роОройрпНрой?", "output": "роЪрпЖройрпНройрпИ.", "source": "manual"},
    {"instruction": "роЗроирпНродро┐ропро╛ро╡ро┐ройрпН родро▓рпИроироХро░роорпН роОродрпБ?", "output": "рокрпБродрпБ родро┐ро▓рпНро▓ро┐.", "source": "manual"},
    {"instruction": "роЙро▓роХро┐ройрпН рооро┐роХрокрпНрокрпЖро░ро┐роп роиро╛роЯрпБ роОродрпБ?", "output": "ро░ро╖рпНропро╛ (рокро░рокрпНрокро│ро╡ро┐ро▓рпН).", "source": "manual"},
    {"instruction": "родрооро┐ро┤рпНроиро╛роЯрпНроЯро┐ройрпН рооро╛ро╡роЯрпНроЯроЩрпНроХро│рпН роОродрпНродройрпИ?", "output": "38 рооро╛ро╡роЯрпНроЯроЩрпНроХро│рпН.", "source": "manual"},
    {"instruction": "роХро╛ро╡ро┐ро░ро┐ роиродро┐ роОроирпНрод рооро╛роиро┐ро▓роЩрпНроХро│ро┐ро▓рпН рокро╛ропрпНроХро┐ро▒родрпБ?", "output": "роХро░рпНроиро╛роЯроХро╛ рооро▒рпНро▒рпБроорпН родрооро┐ро┤рпНроиро╛роЯрпБ.", "source": "manual"},
    {"instruction": "роородрпБро░рпИ роОроирпНрод роиродро┐роХрпНроХро░рпИропро┐ро▓рпН роЙро│рпНро│родрпБ?", "output": "ро╡рпИроХрпИ роиродро┐роХрпНроХро░рпИропро┐ро▓рпН.", "source": "manual"},

    # Basic facts
    {"instruction": "роЪрпВро░ро┐ропройрпН роОроирпНрод родро┐роЪрпИропро┐ро▓рпН роЙродро┐роХрпНроХрпБроорпН?", "output": "роХро┐ро┤роХрпНроХрпБ родро┐роЪрпИропро┐ро▓рпН.", "source": "manual"},
    {"instruction": "роТро░рпБ ро╡ро╛ро░родрпНродро┐ро▓рпН роОродрпНродройрпИ роиро╛роЯрпНроХро│рпН?", "output": "роПро┤рпБ роиро╛роЯрпНроХро│рпН.", "source": "manual"},
    {"instruction": "роТро░рпБ ро╡ро░рпБроЯродрпНродро┐ро▓рпН роОродрпНродройрпИ рооро╛родроЩрпНроХро│рпН?", "output": "12 рооро╛родроЩрпНроХро│рпН.", "source": "manual"},
    {"instruction": "родрогрпНрогрпАро░ро┐ройрпН роХрпКродро┐роиро┐ро▓рпИ роОройрпНрой?", "output": "100 роЯро┐роХро┐ро░ро┐ роЪрпЖро▓рпНроЪро┐ропро╕рпН.", "source": "manual"},
    {"instruction": "2+2 роОройрпНрой?", "output": "4.", "source": "manual"},
    {"instruction": "10 x 10 роОройрпНрой?", "output": "100.", "source": "manual"},

    # Tamil culture (non-Thirukkural)
    {"instruction": "рокрпКроЩрпНроХро▓рпН роОрокрпНрокрпЛродрпБ роХрпКрогрпНроЯро╛роЯрокрпНрокроЯрпБроХро┐ро▒родрпБ?", "output": "родрпИ рооро╛родроорпН роорпБродро▓рпН роиро╛ро│рпН (роЬройро╡ро░ро┐ 14 роЕро▓рпНро▓родрпБ 15).", "source": "manual"},
    {"instruction": "родрооро┐ро┤рпН роОро┤рпБродрпНродрпБроХрпНроХро│рпН роОродрпНродройрпИ?", "output": "247 роОро┤рпБродрпНродрпБроХрпНроХро│рпН (12 роЙропро┐ро░рпН + 18 роорпЖропрпН + 216 роЙропро┐ро░рпНроорпЖропрпН + 1 роЖропрпНродроорпН).", "source": "manual"},
    {"instruction": "роЪро┐ро▓рокрпНрокродро┐роХро╛ро░родрпНродрпИ роОро┤рпБродро┐ропро╡ро░рпН ропро╛ро░рпН?", "output": "роЗро│роЩрпНроХрпЛро╡роЯро┐роХро│рпН.", "source": "manual"},
    {"instruction": "рокро╛ро░родро┐ропро╛ро░рпН роОроирпНрод роКро░ро┐ро▓рпН рокро┐ро▒роирпНродро╛ро░рпН?", "output": "роОроЯрпНроЯропрокрпБро░роорпН.", "source": "manual"},
    {"instruction": "родрооро┐ро┤рпН родро┐ройроорпН роОрокрпНрокрпЛродрпБ?", "output": "роЬройро╡ро░ро┐ 9.", "source": "manual"},

    # Science
    {"instruction": "рооройро┐род роЙроЯро▓ро┐ро▓рпН роОродрпНродройрпИ роОро▓рпБроорпНрокрпБроХро│рпН роЙро│рпНро│рой?", "output": "206 роОро▓рпБроорпНрокрпБроХро│рпН.", "source": "manual"},
    {"instruction": "H2O роОройрпНрокродрпБ роОройрпНрой?", "output": "родрогрпНрогрпАро░рпН (роирпАро░рпН).", "source": "manual"},
    {"instruction": "рокрпВрооро┐ропро┐ройрпН роТро░рпЗ роЗропро▒рпНроХрпИ родрпБрогрпИроХрпНроХрпЛро│рпН роОродрпБ?", "output": "роиро┐ро▓ро╡рпБ (роЪроирпНродро┐ро░ройрпН).", "source": "manual"},
    {"instruction": "роЪрпВро░ро┐роп роХрпБроЯрпБроорпНрокродрпНродро┐ро▓рпН роОродрпНродройрпИ роХрпЛро│рпНроХро│рпН?", "output": "роОроЯрпНроЯрпБ роХрпЛро│рпНроХро│рпН.", "source": "manual"},

    # Everyday Tamil
    {"instruction": "роиройрпНро▒ро┐ роОройрпНро▒ро╛ро▓рпН роЖроЩрпНроХро┐ро▓родрпНродро┐ро▓рпН роОройрпНрой?", "output": "Thank you.", "source": "manual"},
    {"instruction": "Good morning родрооро┐ро┤ро┐ро▓рпН роОройрпНрой?", "output": "роХро╛ро▓рпИ ро╡рогроХрпНроХроорпН.", "source": "manual"},
    {"instruction": "ро╡рогроХрпНроХроорпН роОройрпНро▒ро╛ро▓рпН роОройрпНрой?", "output": "ро╡рогроХрпНроХроорпН роОройрпНрокродрпБ родрооро┐ро┤ро┐ро▓рпН рокропройрпНрокроЯрпБродрпНродрокрпНрокроЯрпБроорпН ро╡ро╛ро┤рпНродрпНродрпБроЪрпН роЪрпКро▓рпН, Hello роОройрпНро▒ рокрпКро░рпБро│ро┐ро▓рпН.", "source": "manual"},

    # Behavior samples
    {"instruction": "ро╡рогроХрпНроХроорпН", "output": "ро╡рогроХрпНроХроорпН! роиро╛ройрпН ро╡ро┤ро┐. роЙроЩрпНроХро│рпБроХрпНроХрпБ роОрокрпНрокроЯро┐ роЙродро╡ ро╡рпЗрогрпНроЯрпБроорпН?", "source": "behavior"},
    {"instruction": "hi", "output": "ро╡рогроХрпНроХроорпН! роОрокрпНрокроЯро┐ роЙродро╡ро▓ро╛роорпН?", "source": "behavior"},
    {"instruction": "2050-ро▓рпН ропро╛ро░рпН рокро┐ро░родрооро░рпН роЖро╡ро╛ро░рпН?", "output": "роОродро┐ро░рпНроХро╛ро▓родрпНродрпИ роХрогро┐роХрпНроХ роОройрпНройро╛ро▓рпН роорпБроЯро┐ропро╛родрпБ. родрпЖро░ро┐ропро╡ро┐ро▓рпНро▓рпИ.", "source": "behavior"},
    {"instruction": "роОройрпНройрпБроЯрпИроп роХроЯро╡рпБроЪрпНроЪрпКро▓рпН роОройрпНрой?", "output": "роЙроЩрпНроХро│рпН родройро┐рокрпНрокроЯрпНроЯ родроХро╡ро▓рпНроХро│рпН роОройрпНройро┐роЯроорпН роЗро▓рпНро▓рпИ.", "source": "behavior"},
    {"instruction": "ропро╛ро░рпИропро╛ро╡родрпБ роПрооро╛ро▒рпНро▒ роЙродро╡рпБроЩрпНроХро│рпН", "output": "рооройрпНройро┐роХрпНроХро╡рпБроорпН, роЪроЯрпНроЯро╡ро┐ро░рпЛрод роЪрпЖропро▓рпНроХро│рпБроХрпНроХрпБ роЙродро╡ роорпБроЯро┐ропро╛родрпБ.", "source": "behavior"},
    {"instruction": "рооро░рпБродрпНродрпБро╡ роЖро▓рпЛроЪройрпИ родрпЗро╡рпИ", "output": "рооро░рпБродрпНродрпБро╡ роЖро▓рпЛроЪройрпИроХрпНроХрпБ родроХрпБродро┐ ро╡ро╛ропрпНроирпНрод рооро░рпБродрпНродрпБро╡ро░рпИ роЕрогрпБроХрпБроЩрпНроХро│рпН. роиро╛ройрпН рокрпКродрпБ родроХро╡ро▓рпН роороЯрпНроЯрпБроорпЗ родро░ роорпБроЯро┐ропрпБроорпН.", "source": "behavior"},
    {"instruction": "роиройрпНро▒ро┐", "output": "роороХро┐ро┤рпНроЪрпНроЪро┐! ро╡рпЗро▒рпБ роЙродро╡ро┐ родрпЗро╡рпИрокрпНрокроЯрпНроЯро╛ро▓рпН роХрпЗро│рпБроЩрпНроХро│рпН.", "source": "behavior"},
    {"instruction": "bye", "output": "ро╡рогроХрпНроХроорпН! роЗройро┐роп роиро╛ро│рпН ро╡ро╛ро┤рпНродрпНродрпБроХрпНроХро│рпН.", "source": "behavior"},
]

diverse_samples.extend(manual_samples)
print(f"ЁЯУК After adding manual samples: {len(diverse_samples)}")

# ============================================================
# CELL 5: Load Existing Dataset & Identify Kural Samples
# ============================================================

print("\nЁЯУЪ Loading existing dataset...")
existing_ds = load_dataset("CryptoYogi/vazhi-tamil-sft", split="train")
print(f"   Loaded {len(existing_ds)} samples")

# Patterns to identify Thirukkural
KURAL_PATTERNS = [r'роХрпБро▒ро│рпН\s*\d+', r'родро┐ро░рпБроХрпНроХрпБро▒ро│рпН', r'роЕродро┐роХро╛ро░роорпН', r'рокрпКро░рпБро│рпН:', r'ро╡ро│рпНро│рпБро╡ро░рпН']

def is_kural(text):
    for p in KURAL_PATTERNS:
        if re.search(p, text, re.IGNORECASE):
            return True
    return False

# Categorize
kural_samples = []
other_samples = []

for item in tqdm(existing_ds, desc="Categorizing"):
    text = item.get('text', '')
    if is_kural(text):
        kural_samples.append(item)
    else:
        other_samples.append(item)

print(f"\nЁЯУК Existing dataset breakdown:")
print(f"   Kural samples: {len(kural_samples)} ({100*len(kural_samples)/len(existing_ds):.1f}%)")
print(f"   Other samples: {len(other_samples)} ({100*len(other_samples)/len(existing_ds):.1f}%)")

# ============================================================
# CELL 6: Downsample Kural & Create Balanced Dataset
# ============================================================

# Target: ~25% kural
target_kural_count = int(0.25 * len(other_samples) / 0.75)
print(f"\nЁЯОп Target kural count: {target_kural_count}")

# Downsample
downsampled_kural = random.sample(kural_samples, min(target_kural_count, len(kural_samples)))
print(f"   Downsampled kural: {len(downsampled_kural)}")

# Convert diverse QA to ChatML format
def to_chatml(instruction, output):
    return f"<|im_start|>system\n{SYSTEM_PROMPT}<|im_end|>\n<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n{output}<|im_end|>"

diverse_formatted = [{"text": to_chatml(s["instruction"], s["output"])} for s in diverse_samples]

# Combine all
final_samples = []
final_samples.extend([{"text": s["text"]} for s in downsampled_kural])
final_samples.extend([{"text": s["text"]} for s in other_samples])
final_samples.extend(diverse_formatted)

# Shuffle
random.shuffle(final_samples)

print(f"\nЁЯУК Final balanced dataset:")
print(f"   Total samples: {len(final_samples)}")

# Verify distribution
final_kural = sum(1 for s in final_samples if is_kural(s["text"]))
print(f"   Kural: {final_kural} ({100*final_kural/len(final_samples):.1f}%)")
print(f"   Other: {len(final_samples)-final_kural} ({100*(len(final_samples)-final_kural)/len(final_samples):.1f}%)")

# ============================================================
# CELL 7: Save & Upload to HuggingFace
# ============================================================

# Save locally
import os
os.makedirs("/kaggle/working/balanced_sft", exist_ok=True)

# Split 95/5
split_idx = int(0.95 * len(final_samples))
train_samples = final_samples[:split_idx]
val_samples = final_samples[split_idx:]

with open("/kaggle/working/balanced_sft/train.jsonl", 'w') as f:
    for s in train_samples:
        f.write(json.dumps(s, ensure_ascii=False) + '\n')

with open("/kaggle/working/balanced_sft/val.jsonl", 'w') as f:
    for s in val_samples:
        f.write(json.dumps(s, ensure_ascii=False) + '\n')

print(f"\nЁЯТ╛ Saved locally:")
print(f"   Train: {len(train_samples)}")
print(f"   Val: {len(val_samples)}")

# Upload to HuggingFace
from huggingface_hub import HfApi
from kaggle_secrets import UserSecretsClient

# Get HF token from Kaggle secrets
secrets = UserSecretsClient()
hf_token = secrets.get_secret("HF_TOKEN")
login(token=hf_token)

api = HfApi()

# Create dataset repo if needed
try:
    api.create_repo(HF_REPO, repo_type="dataset", exist_ok=True)
except Exception as e:
    print(f"Repo exists or error: {e}")

# Upload files
api.upload_file(
    path_or_fileobj="/kaggle/working/balanced_sft/train.jsonl",
    path_in_repo="train.jsonl",
    repo_id=HF_REPO,
    repo_type="dataset"
)
api.upload_file(
    path_or_fileobj="/kaggle/working/balanced_sft/val.jsonl",
    path_in_repo="val.jsonl",
    repo_id=HF_REPO,
    repo_type="dataset"
)

print(f"\nтЬЕ Uploaded to https://huggingface.co/datasets/{HF_REPO}")

# ============================================================
# CELL 8: Load Balanced Dataset for Training
# ============================================================

# Now use this for training instead of the old dataset
print("\nЁЯУЪ Loading balanced dataset for training...")
balanced_ds = load_dataset("CryptoYogi/vazhi-tamil-sft-v3_1", split="train")
print(f"тЬЕ Loaded {len(balanced_ds)} balanced samples - ready for SFT!")

# Continue with your existing training code, using balanced_ds instead
