{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI GGUF Diagnostic v2 (Using Unsloth)\n",
    "\n",
    "**CRITICAL FIX**: Must use Unsloth with `load_in_4bit=True` - same as training!\n",
    "\n",
    "**Problem Found**: Training used Unsloth 4-bit, but our first diagnostic used standard transformers float16.\n",
    "\n",
    "**Test Question**: திருக்குறளின் முதல் குறள் என்ன?\n",
    "\n",
    "**Expected**: அகர முதல எழுத்தெல்லாம் ஆதி பகவன்..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth - SAME as training!\n",
    "!pip install unsloth\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# EXACT same config as training\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "LORA_ADAPTER = \"CryptoYogi/vazhi-lora\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "# EXACT same system prompt as training\n",
    "SYSTEM_PROMPT = \"\"\"நீங்கள் VAZHI (வழி), தமிழ் மக்களுக்கான AI உதவியாளர். தமிழில் தெளிவாகவும் உதவியாகவும் பதிலளியுங்கள். You can respond in Tamil, Tanglish, or English based on how the user asks.\"\"\"\n",
    "\n",
    "EXPECTED_KEYWORDS = [\"அகர\", \"முதல\", \"எழுத்தெல்லாம்\", \"ஆதி\", \"பகவன்\"]\n",
    "\n",
    "def vazhi_chat(model, tokenizer, question, max_tokens=512):\n",
    "    \"\"\"Chat function - EXACT same as training notebook\"\"\"\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    if \"<|im_start|>assistant\" in response:\n",
    "        response = response.split(\"<|im_start|>assistant\")[-1]\n",
    "    response = response.replace(\"<|im_end|>\", \"\").replace(\"<|im_start|>\", \"\").strip()\n",
    "    return response\n",
    "\n",
    "def test_checkpoint(model, tokenizer, name):\n",
    "    \"\"\"Test and validate a checkpoint\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    answer = vazhi_chat(model, tokenizer, \"திருக்குறளின் முதல் குறள் என்ன?\")\n",
    "    print(f\"\\nResponse:\\n{answer}\\n\")\n",
    "    \n",
    "    found = [kw for kw in EXPECTED_KEYWORDS if kw in answer]\n",
    "    print(f\"Keywords: {len(found)}/{len(EXPECTED_KEYWORDS)} - {found}\")\n",
    "    \n",
    "    if len(found) >= 3:\n",
    "        print(\"✅ PASS\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ FAIL\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1: Load with Unsloth (SAME as training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model - EXACT same as training\n",
    "print(\"Loading base model with Unsloth (load_in_4bit=True)...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,  # CRITICAL - same as training!\n",
    ")\n",
    "print(f\"Model loaded. Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA adapter\n",
    "from peft import PeftModel\n",
    "print(\"Loading LoRA adapter from HuggingFace...\")\n",
    "model = PeftModel.from_pretrained(model, LORA_ADAPTER)\n",
    "print(\"LoRA loaded!\")\n",
    "\n",
    "# Set to inference mode - CRITICAL!\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"Inference mode enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Checkpoint 1\n",
    "cp1 = test_checkpoint(model, tokenizer, \"Checkpoint 1: LoRA + Unsloth 4-bit (same as training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 2: Merge LoRA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging LoRA into base model...\")\n",
    "model = model.merge_and_unload()\n",
    "print(\"Merged!\")\n",
    "\n",
    "# Test Checkpoint 2\n",
    "cp2 = test_checkpoint(model, tokenizer, \"Checkpoint 2: Merged (still 4-bit Unsloth)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 3: Save to float16 HuggingFace format\n",
    "\n",
    "This is needed for GGUF conversion. But 4-bit models can't be directly converted.\n",
    "We need to reload in float16 for proper saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload in float16 for proper merge and save\n",
    "print(\"Reloading base model in float16 (for proper GGUF conversion)...\")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "print(\"Base model loaded in float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and merge LoRA in float16\n",
    "from peft import PeftModel\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, LORA_ADAPTER, torch_dtype=torch.float16)\n",
    "print(\"Merging...\")\n",
    "model = model.merge_and_unload()\n",
    "print(\"Merged in float16!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Checkpoint 3 - merged float16 model\n",
    "def test_hf_model(model, tokenizer, name):\n",
    "    \"\"\"Test HuggingFace model (non-Unsloth)\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "திருக்குறளின் முதல் குறள் என்ன?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    if \"<|im_start|>assistant\" in response:\n",
    "        answer = response.split(\"<|im_start|>assistant\")[-1]\n",
    "    else:\n",
    "        answer = response\n",
    "    answer = answer.replace(\"<|im_end|>\", \"\").replace(\"<|im_start|>\", \"\").strip()\n",
    "    \n",
    "    print(f\"\\nResponse:\\n{answer}\\n\")\n",
    "    \n",
    "    found = [kw for kw in EXPECTED_KEYWORDS if kw in answer]\n",
    "    print(f\"Keywords: {len(found)}/{len(EXPECTED_KEYWORDS)} - {found}\")\n",
    "    \n",
    "    if len(found) >= 3:\n",
    "        print(\"✅ PASS\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ FAIL\")\n",
    "        return False\n",
    "\n",
    "cp3 = test_hf_model(model, tokenizer, \"Checkpoint 3: Merged float16 HF model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged model\n",
    "MERGED_OUTPUT = \"./vazhi-merged-f16\"\n",
    "print(f\"Saving to {MERGED_OUTPUT}...\")\n",
    "model.save_pretrained(MERGED_OUTPUT, safe_serialization=True)\n",
    "tokenizer.save_pretrained(MERGED_OUTPUT)\n",
    "print(\"Saved!\")\n",
    "!ls -lh {MERGED_OUTPUT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "!cd llama.cpp && mkdir -p build && cd build && cmake .. && make -j4\n",
    "!pip install -q -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 4: Convert to GGUF F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory before conversion\n",
    "del model\n",
    "del base_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Converting to GGUF F16...\")\n",
    "!python llama.cpp/convert_hf_to_gguf.py {MERGED_OUTPUT} --outfile vazhi-f16.gguf --outtype f16\n",
    "!ls -lh vazhi-f16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test GGUF F16\nprint(\"\\n\" + \"=\"*60)\nprint(\"Testing: Checkpoint 4: GGUF F16\")\nprint(\"=\"*60)\n\n!./llama.cpp/build/bin/llama-cli \\\n    -m vazhi-f16.gguf \\\n    -p \"<|im_start|>system\\nநீங்கள் VAZHI (வழி), தமிழ் மக்களுக்கான AI உதவியாளர். தமிழில் தெளிவாகவும் உதவியாகவும் பதிலளியுங்கள். You can respond in Tamil, Tanglish, or English based on how the user asks.<|im_end|>\\n<|im_start|>user\\nதிருக்குறளின் முதல் குறள் என்ன?<|im_end|>\\n<|im_start|>assistant\\n\" \\\n    -n 150 \\\n    --temp 0.7 \\\n    -ngl 0 \\\n    --stop \"<|im_end|>\" \\\n    2>&1 | tail -30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 5: Quantize to Q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quantizing to Q8_0...\")\n",
    "!./llama.cpp/build/bin/llama-quantize vazhi-f16.gguf vazhi-q8_0.gguf q8_0\n",
    "!ls -lh vazhi-q8_0.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test Q8_0\nprint(\"\\n\" + \"=\"*60)\nprint(\"Testing: Checkpoint 5: GGUF Q8_0\")\nprint(\"=\"*60)\n\n!./llama.cpp/build/bin/llama-cli \\\n    -m vazhi-q8_0.gguf \\\n    -p \"<|im_start|>system\\nநீங்கள் VAZHI (வழி), தமிழ் மக்களுக்கான AI உதவியாளர். தமிழில் தெளிவாகவும் உதவியாகவும் பதிலளியுங்கள். You can respond in Tamil, Tanglish, or English based on how the user asks.<|im_end|>\\n<|im_start|>user\\nதிருக்குறளின் முதல் குறள் என்ன?<|im_end|>\\n<|im_start|>assistant\\n\" \\\n    -n 150 \\\n    --temp 0.7 \\\n    -ngl 0 \\\n    --stop \"<|im_end|>\" \\\n    2>&1 | tail -30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 6: Quantize to Q4_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quantizing to Q4_K_M...\")\n",
    "!./llama.cpp/build/bin/llama-quantize vazhi-f16.gguf vazhi-q4_k_m.gguf q4_k_m\n",
    "!ls -lh vazhi-*.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test Q4_K_M\nprint(\"\\n\" + \"=\"*60)\nprint(\"Testing: Checkpoint 6: GGUF Q4_K_M\")\nprint(\"=\"*60)\n\n!./llama.cpp/build/bin/llama-cli \\\n    -m vazhi-q4_k_m.gguf \\\n    -p \"<|im_start|>system\\nநீங்கள் VAZHI (வழி), தமிழ் மக்களுக்கான AI உதவியாளர். தமிழில் தெளிவாகவும் உதவியாகவும் பதிலளியுங்கள். You can respond in Tamil, Tanglish, or English based on how the user asks.<|im_end|>\\n<|im_start|>user\\nதிருக்குறளின் முதல் குறள் என்ன?<|im_end|>\\n<|im_start|>assistant\\n\" \\\n    -n 150 \\\n    --temp 0.7 \\\n    -ngl 0 \\\n    --stop \"<|im_end|>\" \\\n    2>&1 | tail -30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════╗\n",
    "║                    DIAGNOSTIC SUMMARY                        ║\n",
    "╠══════════════════════════════════════════════════════════════╣\n",
    "║                                                              ║\n",
    "║  Review outputs above and fill in:                          ║\n",
    "║                                                              ║\n",
    "║  | Checkpoint | Stage                  | Result |            ║\n",
    "║  |------------|------------------------|--------|            ║\n",
    "║  | 1          | LoRA + Unsloth 4-bit   |   ?    |            ║\n",
    "║  | 2          | Merged Unsloth 4-bit   |   ?    |            ║\n",
    "║  | 3          | Merged HF float16      |   ?    |            ║\n",
    "║  | 4          | GGUF F16               |   ?    |            ║\n",
    "║  | 5          | GGUF Q8_0              |   ?    |            ║\n",
    "║  | 6          | GGUF Q4_K_M            |   ?    |            ║\n",
    "║                                                              ║\n",
    "║  First FAIL checkpoint = where quality loss happens         ║\n",
    "║                                                              ║\n",
    "╚══════════════════════════════════════════════════════════════╝\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nFile sizes:\")\n",
    "!ls -lh vazhi-*.gguf 2>/dev/null || echo \"No GGUF files yet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload best working model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run to upload the best working model\n",
    "# from huggingface_hub import HfApi\n",
    "# api = HfApi()\n",
    "# \n",
    "# # Upload Q8_0 if it works\n",
    "# print(\"Uploading Q8_0...\")\n",
    "# api.upload_file(\n",
    "#     path_or_fileobj=\"vazhi-q8_0.gguf\",\n",
    "#     path_in_repo=\"vazhi-q8_0.gguf\",\n",
    "#     repo_id=\"CryptoYogi/vazhi-gguf\",\n",
    "#     repo_type=\"model\",\n",
    "# )\n",
    "# print(\"Uploaded!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}