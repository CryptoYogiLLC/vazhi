{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI SFT v3.2 - Fixed Training\n",
    "\n",
    "**FIXES from v3.1 failure:**\n",
    "1. **Data format consistency** - Only uses ChatML-formatted samples (no raw text mixing)\n",
    "2. **Pinned versions** - Avoids API drift issues\n",
    "3. **Single GPU forced** - Prevents cuda:1 vs cuda:0 errors\n",
    "4. **fp16 on T4** - Not bf16 (T4 doesn't support bf16 well)\n",
    "5. **4-bit QLoRA** - More stable/memory efficient on Kaggle\n",
    "\n",
    "**Root cause of v3.1 failure:**\n",
    "Mixed raw text (Sangam poetry, Thirukkural verses) with ChatML-formatted Q&A pairs.\n",
    "This caused the model to output \"systemsystemsystem...\" garbage.\n",
    "\n",
    "**Per GPT5.2 recommendations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies\n",
    "\n",
    "**IMPORTANT:** After running the install cell, **RESTART the Kaggle session** (Kernel ‚Üí Restart Session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies - Updated versions for Qwen3 support\n# NOTE: transformers 4.46.3 does NOT support Qwen3 (too old)\n# Using newer versions that support Qwen3 while keeping other libs stable\n# \n# IMPORTANT: After running this cell, RESTART the Kaggle session!\n\n!pip -q install -U \\\n  \"transformers>=4.51.0\" \\\n  \"accelerate>=0.34.2\" \\\n  \"peft>=0.12.0\" \\\n  \"trl>=0.12.0\" \\\n  \"bitsandbytes>=0.43.3\" \\\n  \"datasets>=2.21.0\" \\\n  \"huggingface_hub>=0.24.7\"\n\n# Verify Qwen3 is supported\nimport transformers\nprint(f\"‚úÖ Transformers version: {transformers.__version__}\")\nprint(\"‚úÖ Dependencies installed\")\nprint(\"‚ö†Ô∏è IMPORTANT: Restart the Kaggle session now (Kernel ‚Üí Restart Session)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CRITICAL: Force single GPU BEFORE importing torch/transformers\n",
    "# Per GPT5.2: This prevents \"cuda:1 vs cuda:0\" device mismatch errors\n",
    "# Must be at the VERY TOP before any other imports\n",
    "# ============================================================================\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# Config\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# HuggingFace repos\n",
    "EXISTING_DATASET = \"CryptoYogi/vazhi-tamil-v05\"\n",
    "BALANCED_DATASET = \"CryptoYogi/vazhi-tamil-sft-v3_2\"  # New version\n",
    "\n",
    "# System prompt\n",
    "SYSTEM_PROMPT = \"‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç VAZHI (‡Æµ‡Æ¥‡Æø), ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æ© AI ‡Æâ‡Æ§‡Æµ‡Æø‡ÆØ‡Ææ‡Æ≥‡Æ∞‡Øç. ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Æø‡Æ≤‡Øç ‡Æ§‡ØÜ‡Æ≥‡Æø‡Æµ‡Ææ‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç ‡Æâ‡Æ§‡Æµ‡Æø‡ÆØ‡Ææ‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Æ§‡Æø‡Æ≤‡Æ≥‡Æø‡ÆØ‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç. ‡Æ§‡ØÜ‡Æ∞‡Æø‡ÆØ‡Ææ‡Æµ‡Æø‡Æü‡Øç‡Æü‡Ææ‡Æ≤‡Øç \\\"‡Æ§‡ØÜ‡Æ∞‡Æø‡ÆØ‡Æµ‡Æø‡Æ≤‡Øç‡Æ≤‡Øà\\\" ‡Æé‡Æ©‡Øç‡Æ±‡ØÅ ‡Æö‡Øä‡Æ≤‡Øç‡Æ≤‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç.\"\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'not set')}\")\n",
    "print(f\"   Source: {EXISTING_DATASET}\")\n",
    "print(f\"   Target: {BALANCED_DATASET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "print(\"‚úÖ Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CHECK: Skip data preparation if balanced dataset already exists\n# ============================================================================\nSKIP_DATA_PREP = False\n\ntry:\n    from huggingface_hub import dataset_info\n    info = dataset_info(BALANCED_DATASET)\n    print(f\"‚úÖ Dataset {BALANCED_DATASET} already exists on HuggingFace!\")\n    print(f\"   Created: {info.created_at}\")\n    print(f\"   Downloads: {info.downloads}\")\n    print(f\"\\nüöÄ SKIPPING data extraction/preparation - will load directly for training\")\n    SKIP_DATA_PREP = True\nexcept Exception as e:\n    print(f\"üìù Dataset {BALANCED_DATASET} not found. Will create it.\")\n    print(f\"   (This is expected on first run)\")\n    SKIP_DATA_PREP = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Diverse QA from IndicAlign\n",
    "\n",
    "IndicAlign contains Tamil translations in the `tam_Taml` field (translated using IndicTrans2 by AI4Bharat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_indicaling(config_name, max_samples):\n",
    "    \"\"\"Extract Tamil samples from IndicAlign config.\n",
    "    \n",
    "    IndicAlign structure:\n",
    "    - tam_Taml is a list of length 1\n",
    "    - tam_Taml[0] is a list of conversation turns [user, assistant, ...]\n",
    "    - tam_Taml[0][0] = user message (Tamil)\n",
    "    - tam_Taml[0][1] = assistant message (Tamil)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìö Loading {config_name}...\")\n",
    "    try:\n",
    "        ds = load_dataset(\"ai4bharat/indic-align\", config_name, split=\"train\", streaming=True)\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error: {e}\")\n",
    "        return []\n",
    "    \n",
    "    samples = []\n",
    "    seen = set()\n",
    "    skipped_non_tamil = 0\n",
    "    skipped_short = 0\n",
    "    skipped_format = 0\n",
    "    \n",
    "    for item in tqdm(ds, desc=config_name, total=max_samples*5):\n",
    "        if len(samples) >= max_samples:\n",
    "            break\n",
    "        \n",
    "        # tam_Taml is a nested list: [[user_msg, assistant_msg, ...]]\n",
    "        tamil = item.get('tam_Taml', [])\n",
    "        if not tamil:\n",
    "            continue\n",
    "        \n",
    "        # FIXED: Access nested list structure correctly\n",
    "        if isinstance(tamil, list) and len(tamil) > 0:\n",
    "            turns = tamil[0]  # Get the inner list\n",
    "            if isinstance(turns, list) and len(turns) >= 2:\n",
    "                user_msg = clean_text(str(turns[0]))\n",
    "                assistant_msg = clean_text(str(turns[1]))\n",
    "            else:\n",
    "                skipped_format += 1\n",
    "                continue\n",
    "        else:\n",
    "            skipped_format += 1\n",
    "            continue\n",
    "        \n",
    "        # Verify it's actually Tamil\n",
    "        if not is_good_tamil_sample(user_msg):\n",
    "            skipped_non_tamil += 1\n",
    "            continue\n",
    "        if not is_good_tamil_sample(assistant_msg):\n",
    "            skipped_short += 1\n",
    "            continue\n",
    "        \n",
    "        # Dedup\n",
    "        key = user_msg[:100]\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        \n",
    "        samples.append({\n",
    "            \"instruction\": user_msg,\n",
    "            \"output\": assistant_msg,\n",
    "            \"source\": config_name\n",
    "        })\n",
    "    \n",
    "    print(f\"   ‚úÖ Extracted {len(samples)} Tamil samples\")\n",
    "    print(f\"   ‚è≠Ô∏è Skipped: {skipped_non_tamil} (not Tamil), {skipped_short} (too short), {skipped_format} (wrong format)\")\n",
    "    \n",
    "    if samples:\n",
    "        print(f\"   üìù Sample verification:\")\n",
    "        print(f\"      User: {samples[0]['instruction'][:80]}...\")\n",
    "        print(f\"      Asst: {samples[0]['output'][:80]}...\")\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract from IndicAlign (SKIP if dataset already exists)\nif not SKIP_DATA_PREP:\n    print(\"üöÄ Extracting diverse QA from IndicAlign...\")\n    print(\"   (tam_Taml field = Already-translated Tamil via IndicTrans2)\")\n\n    diverse_samples = []\n    diverse_samples.extend(extract_from_indicaling(\"Dolly_T\", 300))\n    diverse_samples.extend(extract_from_indicaling(\"WikiHow\", 250))\n    diverse_samples.extend(extract_from_indicaling(\"Wiki_Conv\", 300))\n    diverse_samples.extend(extract_from_indicaling(\"OpenAssistant_T\", 200))\n\n    print(f\"\\nüìä Total extracted from IndicAlign: {len(diverse_samples)}\")\nelse:\n    print(\"‚è≠Ô∏è Skipping IndicAlign extraction (dataset already exists)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify Tamil content distribution (SKIP if dataset already exists)\nif not SKIP_DATA_PREP:\n    tamil_char_pcts = []\n    for s in diverse_samples[:100]:\n        text = s['instruction'] + s['output']\n        pct = 100 * count_tamil_chars(text) / len(text) if text else 0\n        tamil_char_pcts.append(pct)\n\n    avg_tamil_pct = sum(tamil_char_pcts) / len(tamil_char_pcts) if tamil_char_pcts else 0\n    print(f\"üìà Average Tamil character % in samples: {avg_tamil_pct:.1f}%\")\n\n    if avg_tamil_pct < 40:\n        print(\"‚ö†Ô∏è Warning: Tamil content seems low. Check extraction logic.\")\n    else:\n        print(\"‚úÖ Good Tamil content ratio!\")\nelse:\n    print(\"‚è≠Ô∏è Skipping Tamil verification (dataset already exists)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add Manual Samples (Short Answers + Behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Manual samples (SKIP if dataset already exists)\nif not SKIP_DATA_PREP:\n    manual_samples = [\n        # Geography\n        {\"instruction\": \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ©‡Øç ‡Æ§‡Æ≤‡Øà‡Æ®‡Æï‡Æ∞‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"‡Æö‡ØÜ‡Æ©‡Øç‡Æ©‡Øà.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æá‡Æ®‡Øç‡Æ§‡Æø‡ÆØ‡Ææ‡Æµ‡Æø‡Æ©‡Øç ‡Æ§‡Æ≤‡Øà‡Æ®‡Æï‡Æ∞‡ÆÆ‡Øç ‡Æé‡Æ§‡ØÅ?\", \"output\": \"‡Æ™‡ØÅ‡Æ§‡ØÅ ‡Æ§‡Æø‡Æ≤‡Øç‡Æ≤‡Æø.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æâ‡Æ≤‡Æï‡Æø‡Æ©‡Øç ‡ÆÆ‡Æø‡Æï‡Æ™‡Øç‡Æ™‡ØÜ‡Æ∞‡Æø‡ÆØ ‡Æ®‡Ææ‡Æü‡ØÅ ‡Æé‡Æ§‡ØÅ?\", \"output\": \"‡Æ∞‡Æ∑‡Øç‡ÆØ‡Ææ (‡Æ™‡Æ∞‡Æ™‡Øç‡Æ™‡Æ≥‡Æµ‡Æø‡Æ≤‡Øç).\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ©‡Øç ‡ÆÆ‡Ææ‡Æµ‡Æü‡Øç‡Æü‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æ§‡Øç‡Æ§‡Æ©‡Øà?\", \"output\": \"38 ‡ÆÆ‡Ææ‡Æµ‡Æü‡Øç‡Æü‡Æô‡Øç‡Æï‡Æ≥‡Øç.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æï‡Ææ‡Æµ‡Æø‡Æ∞‡Æø ‡Æ®‡Æ§‡Æø ‡Æé‡Æ®‡Øç‡Æ§ ‡ÆÆ‡Ææ‡Æ®‡Æø‡Æ≤‡Æô‡Øç‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æ™‡Ææ‡ÆØ‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ?\", \"output\": \"‡Æï‡Æ∞‡Øç‡Æ®‡Ææ‡Æü‡Æï‡Ææ ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡ØÅ.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡ÆÆ‡Æ§‡ØÅ‡Æ∞‡Øà ‡Æé‡Æ®‡Øç‡Æ§ ‡Æ®‡Æ§‡Æø‡Æï‡Øç‡Æï‡Æ∞‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ?\", \"output\": \"‡Æµ‡Øà‡Æï‡Øà ‡Æ®‡Æ§‡Æø‡Æï‡Øç‡Æï‡Æ∞‡Øà‡ÆØ‡Æø‡Æ≤‡Øç.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æï‡Æô‡Øç‡Æï‡Øà ‡Æ®‡Æ§‡Æø ‡Æé‡Æô‡Øç‡Æï‡ØÅ ‡Æâ‡Æ±‡Øç‡Æ™‡Æ§‡Øç‡Æ§‡Æø‡ÆØ‡Ææ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ?\", \"output\": \"‡Æá‡ÆÆ‡ÆØ‡ÆÆ‡Æ≤‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡Æâ‡Æ≥‡Øç‡Æ≥ ‡Æï‡Æô‡Øç‡Æï‡Øã‡Æ§‡Øç‡Æ∞‡Æø ‡Æ™‡Æ©‡Æø‡Æ™‡Øç‡Æ™‡Ææ‡Æ±‡Øà‡ÆØ‡Æø‡Æ≤‡Øç.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æá‡Æ®‡Øç‡Æ§‡Æø‡ÆØ‡Ææ‡Æµ‡Æø‡Æ©‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Øç‡Æ§‡Øä‡Æï‡Øà ‡ÆÖ‡Æ§‡Æø‡Æï‡ÆÆ‡Ææ‡Æ© ‡ÆÆ‡Ææ‡Æ®‡Æø‡Æ≤‡ÆÆ‡Øç ‡Æé‡Æ§‡ØÅ?\", \"output\": \"‡Æâ‡Æ§‡Øç‡Æ§‡Æ∞‡Æ™‡Øç ‡Æ™‡Æø‡Æ∞‡Æ§‡Øá‡Æö‡ÆÆ‡Øç.\", \"source\": \"manual\"},\n        \n        # Basic facts\n        {\"instruction\": \"‡Æö‡ØÇ‡Æ∞‡Æø‡ÆØ‡Æ©‡Øç ‡Æé‡Æ®‡Øç‡Æ§ ‡Æ§‡Æø‡Æö‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡Æâ‡Æ§‡Æø‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç?\", \"output\": \"‡Æï‡Æø‡Æ¥‡Æï‡Øç‡Æï‡ØÅ ‡Æ§‡Æø‡Æö‡Øà‡ÆØ‡Æø‡Æ≤‡Øç.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æí‡Æ∞‡ØÅ ‡Æµ‡Ææ‡Æ∞‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡Æé‡Æ§‡Øç‡Æ§‡Æ©‡Øà ‡Æ®‡Ææ‡Æü‡Øç‡Æï‡Æ≥‡Øç?\", \"output\": \"‡Æè‡Æ¥‡ØÅ ‡Æ®‡Ææ‡Æü‡Øç‡Æï‡Æ≥‡Øç.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æí‡Æ∞‡ØÅ ‡Æµ‡Æ∞‡ØÅ‡Æü‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡Æé‡Æ§‡Øç‡Æ§‡Æ©‡Øà ‡ÆÆ‡Ææ‡Æ§‡Æô‡Øç‡Æï‡Æ≥‡Øç?\", \"output\": \"12 ‡ÆÆ‡Ææ‡Æ§‡Æô‡Øç‡Æï‡Æ≥‡Øç.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æ§‡Æ£‡Øç‡Æ£‡ØÄ‡Æ∞‡Æø‡Æ©‡Øç ‡Æï‡Øä‡Æ§‡Æø‡Æ®‡Æø‡Æ≤‡Øà ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"100 ‡Æü‡Æø‡Æï‡Æø‡Æ∞‡Æø ‡Æö‡ØÜ‡Æ≤‡Øç‡Æö‡Æø‡ÆØ‡Æ∏‡Øç.\", \"source\": \"manual\"},\n        {\"instruction\": \"2+2 ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"4.\", \"source\": \"manual\"},\n        {\"instruction\": \"10 x 10 ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"100.\", \"source\": \"manual\"},\n        {\"instruction\": \"100-‡Æê 4-‡ÆÜ‡Æ≤‡Øç ‡Æµ‡Æï‡ØÅ‡Æ§‡Øç‡Æ§‡Ææ‡Æ≤‡Øç?\", \"output\": \"25.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æ™‡ØÇ‡ÆÆ‡Æø ‡Æö‡ØÇ‡Æ∞‡Æø‡ÆØ‡Æ©‡Øà ‡Æö‡ØÅ‡Æ±‡Øç‡Æ± ‡Æé‡Æ§‡Øç‡Æ§‡Æ©‡Øà ‡Æ®‡Ææ‡Æü‡Øç‡Æï‡Æ≥‡Øç ‡ÆÜ‡Æï‡ØÅ‡ÆÆ‡Øç?\", \"output\": \"365 ‡Æ®‡Ææ‡Æü‡Øç‡Æï‡Æ≥‡Øç (‡Æí‡Æ∞‡ØÅ ‡Æµ‡Æ∞‡ØÅ‡Æü‡ÆÆ‡Øç).\", \"source\": \"manual\"},\n        \n        # Tamil culture (non-Thirukkural)\n        {\"instruction\": \"‡Æ™‡Øä‡Æô‡Øç‡Æï‡Æ≤‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Øã‡Æ§‡ØÅ ‡Æï‡Øä‡Æ£‡Øç‡Æü‡Ææ‡Æü‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ?\", \"output\": \"‡Æ§‡Øà ‡ÆÆ‡Ææ‡Æ§‡ÆÆ‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Øç ‡Æ®‡Ææ‡Æ≥‡Øç (‡Æú‡Æ©‡Æµ‡Æ∞‡Æø 14 ‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ 15).\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æ§‡Øç‡Æ§‡Æ©‡Øà?\", \"output\": \"247 ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øç (12 ‡Æâ‡ÆØ‡Æø‡Æ∞‡Øç + 18 ‡ÆÆ‡ØÜ‡ÆØ‡Øç + 216 ‡Æâ‡ÆØ‡Æø‡Æ∞‡Øç‡ÆÆ‡ØÜ‡ÆØ‡Øç + 1 ‡ÆÜ‡ÆØ‡Øç‡Æ§‡ÆÆ‡Øç).\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æö‡Æø‡Æ≤‡Æ™‡Øç‡Æ™‡Æ§‡Æø‡Æï‡Ææ‡Æ∞‡Æ§‡Øç‡Æ§‡Øà ‡Æé‡Æ¥‡ØÅ‡Æ§‡Æø‡ÆØ‡Æµ‡Æ∞‡Øç ‡ÆØ‡Ææ‡Æ∞‡Øç?\", \"output\": \"‡Æá‡Æ≥‡Æô‡Øç‡Æï‡Øã‡Æµ‡Æü‡Æø‡Æï‡Æ≥‡Øç.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æ™‡Ææ‡Æ∞‡Æ§‡Æø‡ÆØ‡Ææ‡Æ∞‡Øç ‡Æé‡Æ®‡Øç‡Æ§ ‡Æä‡Æ∞‡Æø‡Æ≤‡Øç ‡Æ™‡Æø‡Æ±‡Æ®‡Øç‡Æ§‡Ææ‡Æ∞‡Øç?\", \"output\": \"‡Æé‡Æü‡Øç‡Æü‡ÆØ‡Æ™‡ØÅ‡Æ∞‡ÆÆ‡Øç.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡Æ§‡Æø‡Æ©‡ÆÆ‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Øã‡Æ§‡ØÅ?\", \"output\": \"‡Æú‡Æ©‡Æµ‡Æ∞‡Æø 9.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ©‡Øç ‡ÆÖ‡Æ≤‡ØÅ‡Æµ‡Æ≤‡Øç ‡ÆÆ‡Øä‡Æ¥‡Æø ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç.\", \"source\": \"manual\"},\n        \n        # Science\n        {\"instruction\": \"‡ÆÆ‡Æ©‡Æø‡Æ§ ‡Æâ‡Æü‡Æ≤‡Æø‡Æ≤‡Øç ‡Æé‡Æ§‡Øç‡Æ§‡Æ©‡Øà ‡Æé‡Æ≤‡ØÅ‡ÆÆ‡Øç‡Æ™‡ØÅ‡Æï‡Æ≥‡Øç ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æ©?\", \"output\": \"206 ‡Æé‡Æ≤‡ØÅ‡ÆÆ‡Øç‡Æ™‡ØÅ‡Æï‡Æ≥‡Øç.\", \"source\": \"manual\"},\n        {\"instruction\": \"H2O ‡Æé‡Æ©‡Øç‡Æ™‡Æ§‡ØÅ ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"‡Æ§‡Æ£‡Øç‡Æ£‡ØÄ‡Æ∞‡Øç (‡Æ®‡ØÄ‡Æ∞‡Øç).\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æ™‡ØÇ‡ÆÆ‡Æø‡ÆØ‡Æø‡Æ©‡Øç ‡Æí‡Æ∞‡Øá ‡Æá‡ÆØ‡Æ±‡Øç‡Æï‡Øà ‡Æ§‡ØÅ‡Æ£‡Øà‡Æï‡Øç‡Æï‡Øã‡Æ≥‡Øç ‡Æé‡Æ§‡ØÅ?\", \"output\": \"‡Æ®‡Æø‡Æ≤‡Æµ‡ØÅ (‡Æö‡Æ®‡Øç‡Æ§‡Æø‡Æ∞‡Æ©‡Øç).\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æö‡ØÇ‡Æ∞‡Æø‡ÆØ ‡Æï‡ØÅ‡Æü‡ØÅ‡ÆÆ‡Øç‡Æ™‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡Æé‡Æ§‡Øç‡Æ§‡Æ©‡Øà ‡Æï‡Øã‡Æ≥‡Øç‡Æï‡Æ≥‡Øç?\", \"output\": \"‡Æé‡Æü‡Øç‡Æü‡ØÅ ‡Æï‡Øã‡Æ≥‡Øç‡Æï‡Æ≥‡Øç.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æí‡Æ≥‡Æø‡ÆØ‡Æø‡Æ©‡Øç ‡Æµ‡Øá‡Æï‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"‡Æµ‡Æø‡Æ©‡Ææ‡Æü‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æö‡ØÅ‡ÆÆ‡Ææ‡Æ∞‡Øç 3 ‡Æ≤‡Æü‡Øç‡Æö‡ÆÆ‡Øç ‡Æï‡Æø‡Æ≤‡Øã‡ÆÆ‡ØÄ‡Æü‡Øç‡Æü‡Æ∞‡Øç.\", \"source\": \"manual\"},\n        \n        # Everyday Tamil\n        {\"instruction\": \"‡Æ®‡Æ©‡Øç‡Æ±‡Æø ‡Æé‡Æ©‡Øç‡Æ±‡Ææ‡Æ≤‡Øç ‡ÆÜ‡Æô‡Øç‡Æï‡Æø‡Æ≤‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"Thank you.\", \"source\": \"manual\"},\n        {\"instruction\": \"Good morning ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Æø‡Æ≤‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"‡Æï‡Ææ‡Æ≤‡Øà ‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ±‡Ææ‡Æ≤‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ™‡Æ§‡ØÅ ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Æø‡Æ≤‡Øç ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡ÆÆ‡Øç ‡Æµ‡Ææ‡Æ¥‡Øç‡Æ§‡Øç‡Æ§‡ØÅ‡Æö‡Øç ‡Æö‡Øä‡Æ≤‡Øç, Hello ‡Æé‡Æ©‡Øç‡Æ± ‡Æ™‡Øä‡Æ∞‡ØÅ‡Æ≥‡Æø‡Æ≤‡Øç.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡ÆÜ‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ±‡Ææ‡Æ≤‡Øç ‡ÆÜ‡Æô‡Øç‡Æï‡Æø‡Æ≤‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç?\", \"output\": \"Yes.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡Æá‡Æ≤‡Øç‡Æ≤‡Øà ‡Æé‡Æ©‡Øç‡Æ±‡Ææ‡Æ≤‡Øç ‡ÆÜ‡Æô‡Øç‡Æï‡Æø‡Æ≤‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç?\", \"output\": \"No.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡ÆÆ‡Æø‡Æï‡Æ™‡Øç‡Æ™‡ØÜ‡Æ∞‡Æø‡ÆØ ‡Æï‡Æ£‡Øç‡Æü‡ÆÆ‡Øç ‡Æé‡Æ§‡ØÅ?\", \"output\": \"‡ÆÜ‡Æö‡Æø‡ÆØ‡Ææ.\", \"source\": \"manual\"},\n        {\"instruction\": \"‡ÆÆ‡Æø‡Æï‡Æö‡Øç‡Æö‡Æø‡Æ±‡Æø‡ÆØ ‡Æï‡Æ£‡Øç‡Æü‡ÆÆ‡Øç ‡Æé‡Æ§‡ØÅ?\", \"output\": \"‡ÆÜ‡Æ∏‡Øç‡Æ§‡Æø‡Æ∞‡Øá‡Æ≤‡Æø‡ÆØ‡Ææ.\", \"source\": \"manual\"},\n        \n        # Behavior samples\n        {\"instruction\": \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç\", \"output\": \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç! ‡Æ®‡Ææ‡Æ©‡Øç ‡Æµ‡Æ¥‡Æø. ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æâ‡Æ§‡Æµ ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç?\", \"source\": \"behavior\"},\n        {\"instruction\": \"hi\", \"output\": \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç! ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æâ‡Æ§‡Æµ‡Æ≤‡Ææ‡ÆÆ‡Øç?\", \"source\": \"behavior\"},\n        {\"instruction\": \"hello\", \"output\": \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç! ‡Æï‡Øá‡Æ≥‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç.\", \"source\": \"behavior\"},\n        {\"instruction\": \"2050-‡Æ≤‡Øç ‡ÆØ‡Ææ‡Æ∞‡Øç ‡Æ™‡Æø‡Æ∞‡Æ§‡ÆÆ‡Æ∞‡Øç ‡ÆÜ‡Æµ‡Ææ‡Æ∞‡Øç?\", \"output\": \"‡Æé‡Æ§‡Æø‡Æ∞‡Øç‡Æï‡Ææ‡Æ≤‡Æ§‡Øç‡Æ§‡Øà ‡Æï‡Æ£‡Æø‡Æï‡Øç‡Æï ‡Æé‡Æ©‡Øç‡Æ©‡Ææ‡Æ≤‡Øç ‡ÆÆ‡ØÅ‡Æü‡Æø‡ÆØ‡Ææ‡Æ§‡ØÅ. ‡Æ§‡ØÜ‡Æ∞‡Æø‡ÆØ‡Æµ‡Æø‡Æ≤‡Øç‡Æ≤‡Øà.\", \"source\": \"behavior\"},\n        {\"instruction\": \"‡Æ®‡Ææ‡Æ≥‡Øà ‡Æ™‡Æô‡Øç‡Æï‡ØÅ ‡Æö‡Æ®‡Øç‡Æ§‡Øà ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç?\", \"output\": \"‡Æ™‡Æô‡Øç‡Æï‡ØÅ ‡Æö‡Æ®‡Øç‡Æ§‡Øà ‡Æ®‡Æø‡Æ≤‡Æµ‡Æ∞‡Æ§‡Øç‡Æ§‡Øà ‡Æï‡Æ£‡Æø‡Æï‡Øç‡Æï ‡Æé‡Æ©‡Øç‡Æ©‡Ææ‡Æ≤‡Øç ‡Æá‡ÆØ‡Æ≤‡Ææ‡Æ§‡ØÅ. ‡Æ®‡Æø‡Æ§‡Æø ‡ÆÜ‡Æ≤‡Øã‡Æö‡Æï‡Æ∞‡Æø‡Æü‡ÆÆ‡Øç ‡Æï‡Øá‡Æ≥‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç.\", \"source\": \"behavior\"},\n        {\"instruction\": \"‡Æé‡Æ©‡Øç‡Æ©‡ØÅ‡Æü‡Øà‡ÆØ ‡Æï‡Æü‡Æµ‡ØÅ‡Æö‡Øç‡Æö‡Øä‡Æ≤‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\", \"output\": \"‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ§‡Æ©‡Æø‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡Æ§‡Æï‡Æµ‡Æ≤‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æ©‡Øç‡Æ©‡Æø‡Æü‡ÆÆ‡Øç ‡Æá‡Æ≤‡Øç‡Æ≤‡Øà.\", \"source\": \"behavior\"},\n        {\"instruction\": \"‡ÆØ‡Ææ‡Æ∞‡Øà‡ÆØ‡Ææ‡Æµ‡Æ§‡ØÅ ‡Æè‡ÆÆ‡Ææ‡Æ±‡Øç‡Æ± ‡Æâ‡Æ§‡Æµ‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç\", \"output\": \"‡ÆÆ‡Æ©‡Øç‡Æ©‡Æø‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç, ‡Æö‡Æü‡Øç‡Æü‡Æµ‡Æø‡Æ∞‡Øã‡Æ§ ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æâ‡Æ§‡Æµ ‡ÆÆ‡ØÅ‡Æü‡Æø‡ÆØ‡Ææ‡Æ§‡ØÅ.\", \"source\": \"behavior\"},\n        {\"instruction\": \"‡ÆÆ‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ ‡ÆÜ‡Æ≤‡Øã‡Æö‡Æ©‡Øà ‡Æ§‡Øá‡Æµ‡Øà\", \"output\": \"‡ÆÆ‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ ‡ÆÜ‡Æ≤‡Øã‡Æö‡Æ©‡Øà‡Æï‡Øç‡Æï‡ØÅ ‡Æ§‡Æï‡ØÅ‡Æ§‡Æø ‡Æµ‡Ææ‡ÆØ‡Øç‡Æ®‡Øç‡Æ§ ‡ÆÆ‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ‡Æ∞‡Øà ‡ÆÖ‡Æ£‡ØÅ‡Æï‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç. ‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡Øä‡Æ§‡ØÅ ‡Æ§‡Æï‡Æµ‡Æ≤‡Øç ‡ÆÆ‡Æü‡Øç‡Æü‡ØÅ‡ÆÆ‡Øá ‡Æ§‡Æ∞ ‡ÆÆ‡ØÅ‡Æü‡Æø‡ÆØ‡ØÅ‡ÆÆ‡Øç.\", \"source\": \"behavior\"},\n        {\"instruction\": \"‡Æö‡Æü‡Øç‡Æü ‡ÆÜ‡Æ≤‡Øã‡Æö‡Æ©‡Øà ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç\", \"output\": \"‡Æö‡Æü‡Øç‡Æü ‡Æµ‡Æø‡Æ∑‡ÆØ‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æµ‡Æ¥‡Æï‡Øç‡Æï‡Æ±‡Æø‡Æû‡Æ∞‡Øà ‡ÆÖ‡Æ£‡ØÅ‡Æï‡ØÅ‡Æµ‡Æ§‡ØÅ ‡Æö‡Æø‡Æ±‡Æ®‡Øç‡Æ§‡Æ§‡ØÅ. ‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡Øä‡Æ§‡ØÅ ‡Æ§‡Æï‡Æµ‡Æ≤‡Øç ‡ÆÆ‡Æü‡Øç‡Æü‡ØÅ‡ÆÆ‡Øá ‡Æ§‡Æ∞ ‡ÆÆ‡ØÅ‡Æü‡Æø‡ÆØ‡ØÅ‡ÆÆ‡Øç.\", \"source\": \"behavior\"},\n        {\"instruction\": \"‡ÆÖ‡Æ∞‡Æö‡ØÅ ‡Æ§‡Æø‡Æü‡Øç‡Æü‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æµ‡Æø‡Æ£‡Øç‡Æ£‡Æ™‡Øç‡Æ™‡Æø‡Æ™‡Øç‡Æ™‡Æ§‡ØÅ ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø?\", \"output\": \"‡ÆÖ‡Æ∞‡Æö‡ØÅ ‡Æ§‡Æø‡Æü‡Øç‡Æü‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ tnega.tn.gov.in ‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ ‡ÆÖ‡Æ∞‡ØÅ‡Æï‡Æø‡Æ≤‡ØÅ‡Æ≥‡Øç‡Æ≥ ‡ÆÖ‡Æ≤‡ØÅ‡Æµ‡Æ≤‡Æï‡Æ§‡Øç‡Æ§‡Øà ‡Æ§‡Øä‡Æü‡Æ∞‡Øç‡Æ™‡ØÅ ‡Æï‡Øä‡Æ≥‡Øç‡Æ≥‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç.\", \"source\": \"behavior\"},\n        {\"instruction\": \"‡Æ®‡Æ©‡Øç‡Æ±‡Æø\", \"output\": \"‡ÆÆ‡Æï‡Æø‡Æ¥‡Øç‡Æö‡Øç‡Æö‡Æø! ‡Æµ‡Øá‡Æ±‡ØÅ ‡Æâ‡Æ§‡Æµ‡Æø ‡Æ§‡Øá‡Æµ‡Øà‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Ææ‡Æ≤‡Øç ‡Æï‡Øá‡Æ≥‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç.\", \"source\": \"behavior\"},\n        {\"instruction\": \"bye\", \"output\": \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç! ‡Æá‡Æ©‡Æø‡ÆØ ‡Æ®‡Ææ‡Æ≥‡Øç ‡Æµ‡Ææ‡Æ¥‡Øç‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øç.\", \"source\": \"behavior\"},\n        {\"instruction\": \"‡Æö‡Æ∞‡Æø\", \"output\": \"‡Æö‡Æ∞‡Æø, ‡Æµ‡Øá‡Æ±‡ØÅ ‡Æè‡Æ§‡Ææ‡Æµ‡Æ§‡ØÅ ‡Æï‡Øá‡Æ≥‡Øç‡Æµ‡Æø ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æ§‡Ææ?\", \"source\": \"behavior\"},\n    ]\n\n    diverse_samples.extend(manual_samples)\n    print(f\"üìä Total after adding manual samples: {len(diverse_samples)}\")\n    print(f\"   - From IndicAlign: {len(diverse_samples) - len(manual_samples)}\")\n    print(f\"   - Manual samples: {len(manual_samples)}\")\nelse:\n    print(\"‚è≠Ô∏è Skipping manual samples (dataset already exists)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Existing Dataset & Filter for ChatML ONLY\n",
    "\n",
    "**CRITICAL FIX:** Only use ChatML-formatted samples. Raw text belongs in DAPT, not SFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load existing dataset & analyze format (SKIP if balanced dataset already exists)\nif not SKIP_DATA_PREP:\n    print(f\"\\nüìö Loading existing dataset from {EXISTING_DATASET}...\")\n    existing_ds = load_dataset(EXISTING_DATASET, split=\"train\")\n    print(f\"   Loaded {len(existing_ds)} samples\")\n\n    # Analyze format distribution BEFORE filtering\n    chatml_count = 0\n    raw_count = 0\n    for item in tqdm(existing_ds, desc=\"Analyzing formats\"):\n        text = item.get('text', '')\n        if is_chatml_formatted(text):\n            chatml_count += 1\n        else:\n            raw_count += 1\n\n    print(f\"\\nüìä Existing dataset format analysis:\")\n    print(f\"   ChatML formatted: {chatml_count} ({100*chatml_count/len(existing_ds):.1f}%)\")\n    print(f\"   Raw text: {raw_count} ({100*raw_count/len(existing_ds):.1f}%)\")\n    print(f\"\")\n    print(f\"   ‚ö†Ô∏è Raw text samples will be EXCLUDED from SFT training\")\n    print(f\"   üìù They belong in Micro-DAPT stage, not SFT\")\nelse:\n    print(\"‚è≠Ô∏è Skipping existing dataset analysis (balanced dataset already exists)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CRITICAL FIX: Filter existing dataset for ChatML ONLY\n# Raw text and ChatML mixed = \"systemsystemsystem...\" garbage\n# ============================================================================\n\nif not SKIP_DATA_PREP:\n    # Filter existing dataset - ONLY keep ChatML formatted samples\n    existing_chatml_samples = []\n    existing_kural_chatml = []\n    existing_other_chatml = []\n\n    for item in tqdm(existing_ds, desc=\"Filtering ChatML\"):\n        text = item.get('text', '')\n        if is_chatml_formatted(text):\n            if is_kural(text):\n                existing_kural_chatml.append({\"text\": text})\n            else:\n                existing_other_chatml.append({\"text\": text})\n\n    print(f\"\\nüìä ChatML samples from existing dataset:\")\n    print(f\"   Kural (ChatML): {len(existing_kural_chatml)}\")\n    print(f\"   Other (ChatML): {len(existing_other_chatml)}\")\n    print(f\"   Total usable: {len(existing_kural_chatml) + len(existing_other_chatml)}\")\nelse:\n    print(\"‚è≠Ô∏è Skipping ChatML filtering (balanced dataset already exists)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Downsample Thirukkural & Create Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Downsample Thirukkural to ~25% of non-Thirukkural samples (SKIP if balanced dataset exists)\nif not SKIP_DATA_PREP:\n    total_other = len(existing_other_chatml)\n    target_kural_pct = 0.25\n    target_kural_count = int(target_kural_pct * total_other / (1 - target_kural_pct))\n\n    print(f\"\\nüéØ Downsampling Thirukkural:\")\n    print(f\"   Current ChatML Kural: {len(existing_kural_chatml)}\")\n    print(f\"   Target: {target_kural_count} ({100*target_kural_pct:.0f}%)\")\n\n    # Randomly sample (seeded for reproducibility)\n    if len(existing_kural_chatml) > target_kural_count:\n        downsampled_kural = random.sample(existing_kural_chatml, target_kural_count)\n    else:\n        downsampled_kural = existing_kural_chatml\n    print(f\"   Downsampled: {len(downsampled_kural)}\")\nelse:\n    print(\"‚è≠Ô∏è Skipping Thirukkural downsampling (balanced dataset already exists)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Combine all samples and verify format (SKIP if balanced dataset exists)\nif not SKIP_DATA_PREP:\n    # Convert diverse QA to ChatML format\n    diverse_formatted = [{\"text\": to_chatml(s[\"instruction\"], s[\"output\"])} for s in diverse_samples]\n\n    # Combine ALL samples - all must be ChatML formatted\n    final_samples = []\n    final_samples.extend(downsampled_kural)      # ChatML Kural\n    final_samples.extend(existing_other_chatml)  # ChatML Other\n    final_samples.extend(diverse_formatted)       # ChatML Diverse\n\n    # Shuffle (seeded for reproducibility)\n    random.shuffle(final_samples)\n\n    print(f\"\\nüìä Final SFT dataset (ChatML ONLY):\")\n    print(f\"   Downsampled Kural: {len(downsampled_kural)}\")\n    print(f\"   Other (ChatML): {len(existing_other_chatml)}\")\n    print(f\"   Diverse QA (new): {len(diverse_formatted)}\")\n    print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n    print(f\"   Total: {len(final_samples)}\")\n\n    # CRITICAL: Verify 100% ChatML\n    chatml_count = sum(1 for s in final_samples if is_chatml_formatted(s[\"text\"]))\n    print(f\"\\nüìà ChatML format %: {100*chatml_count/len(final_samples):.1f}% (MUST be 100%)\")\n\n    if chatml_count != len(final_samples):\n        print(\"‚ùå ERROR: Not all samples are ChatML formatted! This will cause training failure.\")\n        raise ValueError(\"Data format inconsistency detected\")\n    else:\n        print(\"‚úÖ All samples are ChatML formatted - safe to train\")\n\n    # Verify Kural distribution\n    final_kural = sum(1 for s in final_samples if is_kural(s[\"text\"]))\n    print(f\"üìà Final Thirukkural %: {100*final_kural/len(final_samples):.1f}%\")\nelse:\n    print(\"‚è≠Ô∏è Skipping sample combination (balanced dataset already exists)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save & Upload to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save locally and split train/val (SKIP if balanced dataset exists)\nif not SKIP_DATA_PREP:\n    # Create output directory\n    os.makedirs(\"/kaggle/working/balanced_sft\", exist_ok=True)\n\n    # Split 95/5 train/val\n    split_idx = int(0.95 * len(final_samples))\n    train_samples = final_samples[:split_idx]\n    val_samples = final_samples[split_idx:]\n\n    # Save locally\n    with open(\"/kaggle/working/balanced_sft/train.jsonl\", 'w') as f:\n        for s in train_samples:\n            f.write(json.dumps(s, ensure_ascii=False) + '\\n')\n\n    with open(\"/kaggle/working/balanced_sft/val.jsonl\", 'w') as f:\n        for s in val_samples:\n            f.write(json.dumps(s, ensure_ascii=False) + '\\n')\n\n    print(f\"üíæ Saved locally:\")\n    print(f\"   Train: {len(train_samples)} samples\")\n    print(f\"   Val: {len(val_samples)} samples\")\nelse:\n    print(\"‚è≠Ô∏è Skipping local save (balanced dataset already exists)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Upload to HuggingFace (SKIP if balanced dataset exists)\nif not SKIP_DATA_PREP:\n    api = HfApi()\n\n    # Create dataset repo (per GPT5.2: ensure repo exists before pushing)\n    try:\n        api.create_repo(BALANCED_DATASET, repo_type=\"dataset\", exist_ok=True)\n        print(f\"‚úÖ Created/verified repo: {BALANCED_DATASET}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Repo creation: {e}\")\n\n    # Upload files\n    api.upload_file(\n        path_or_fileobj=\"/kaggle/working/balanced_sft/train.jsonl\",\n        path_in_repo=\"train.jsonl\",\n        repo_id=BALANCED_DATASET,\n        repo_type=\"dataset\"\n    )\n    api.upload_file(\n        path_or_fileobj=\"/kaggle/working/balanced_sft/val.jsonl\",\n        path_in_repo=\"val.jsonl\",\n        repo_id=BALANCED_DATASET,\n        repo_type=\"dataset\"\n    )\n\n    print(f\"\\n‚úÖ Uploaded to: https://huggingface.co/datasets/{BALANCED_DATASET}\")\nelse:\n    print(\"‚è≠Ô∏è Skipping HuggingFace upload (balanced dataset already exists)\")\n    print(f\"   Will load directly from: https://huggingface.co/datasets/{BALANCED_DATASET}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Balanced Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the balanced dataset for training\n",
    "print(f\"\\nüìö Loading balanced dataset for training...\")\n",
    "balanced_ds = load_dataset(BALANCED_DATASET, split=\"train\")\n",
    "print(f\"‚úÖ Loaded {len(balanced_ds)} balanced samples\")\n",
    "\n",
    "# Show sample - verify it's ChatML formatted\n",
    "print(f\"\\nüìù Sample (should show ChatML tags):\")\n",
    "sample_text = balanced_ds[0]['text'][:400]\n",
    "print(sample_text + \"...\")\n",
    "\n",
    "if \"<|im_start|>\" in sample_text:\n",
    "    print(\"\\n‚úÖ Sample is ChatML formatted\")\n",
    "else:\n",
    "    print(\"\\n‚ùå ERROR: Sample is NOT ChatML formatted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. SFT Training Setup\n",
    "\n",
    "Now we train Qwen3-0.6B on the balanced dataset.\n",
    "\n",
    "**Per GPT5.2 recommendations:**\n",
    "- 4-bit QLoRA (more stable/memory efficient)\n",
    "- fp16 compute dtype (T4 doesn't support bf16 well)\n",
    "- Pinned library versions\n",
    "- Single GPU forced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\n\n# Try importing SFTConfig (newer TRL versions)\ntry:\n    from trl import SFTConfig\n    print(\"‚úÖ Using TRL with SFTConfig (newer API)\")\nexcept ImportError:\n    SFTConfig = None\n    print(\"‚ö†Ô∏è Using TRL with TrainingArguments (older API)\")\n\n# Model config\nBASE_MODEL = \"Qwen/Qwen3-0.6B\"\nOUTPUT_MODEL = \"CryptoYogi/vazhi-qwen3-v3_2\"\n\nprint(f\"ü§ñ Base model: {BASE_MODEL}\")\nprint(f\"üì§ Output: {OUTPUT_MODEL}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model and tokenizer with 4-bit quantization\n# 4-bit + LoRA is more stable/fast on Kaggle\nprint(\"\\nüì• Loading model and tokenizer...\")\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n# IMPORTANT: Do NOT modify tokenizer.pad_token = eos_token\n# Per TRAINING_LOG lesson #9: This causes \"OrderedVocab holes\" and corrupts the model\ntokenizer.padding_side = \"right\"\n\n# 4-bit quantization config (Kaggle-friendly)\n# Use float16 compute dtype - P100/T4 don't support bf16 (requires Ampere+)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model with 4-bit quantization\n# CRITICAL: Force torch_dtype=float16 - Qwen3 defaults to bf16 which P100 doesn't support\n# bf16 requires Ampere architecture (A100, RTX 30xx) or newer\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    quantization_config=bnb_config,\n    torch_dtype=torch.float16,  # Force fp16 (P100/T4 don't support bf16)\n    device_map={\"\":0},  # Force single GPU (prevents cuda:1 vs cuda:0 errors)\n    trust_remote_code=True\n)\n\n# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Align model config with tokenizer (don't modify tokenizer)\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.eos_token_id = tokenizer.eos_token_id\n\n# Disable cache for gradient checkpointing compatibility\nmodel.config.use_cache = False\n\nprint(f\"‚úÖ Model loaded: {model.num_parameters():,} parameters (4-bit quantized)\")\nprint(f\"   torch_dtype: float16 (P100 compatible)\")\nprint(f\"   pad_token_id: {tokenizer.pad_token_id}\")\nprint(f\"   eos_token_id: {tokenizer.eos_token_id}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments (TRL 0.11.4 compatible)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/vazhi-v3_2\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,  # Effective batch = 16\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=25,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # T4 compatible (not bf16)\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create trainer - compatible with TRL 0.12+\ntry:\n    from trl import SFTConfig\n    \n    sft_config = SFTConfig(\n        output_dir=\"/kaggle/working/vazhi-v3_2\",\n        num_train_epochs=2,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=8,\n        learning_rate=1e-4,\n        lr_scheduler_type=\"cosine\",\n        warmup_ratio=0.1,\n        logging_steps=25,\n        save_steps=200,\n        save_total_limit=2,\n        fp16=True,\n        bf16=False,  # Explicitly disable bf16 (P100 doesn't support it)\n        gradient_checkpointing=True,\n        max_grad_norm=1.0,\n        optim=\"paged_adamw_8bit\",\n        report_to=\"none\",\n        dataset_text_field=\"text\",\n        max_length=512,  # Changed from max_seq_length (TRL API change)\n        packing=False,\n    )\n    \n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=balanced_ds,\n        args=sft_config,\n        processing_class=tokenizer,\n    )\n    print(\"‚úÖ Trainer initialized (SFTConfig API)\")\n    \nexcept ImportError:\n    # Fall back to old API\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=balanced_ds,\n        args=training_args,\n        tokenizer=tokenizer,\n        dataset_text_field=\"text\",\n        max_seq_length=512,\n        packing=False,\n    )\n    print(\"‚úÖ Trainer initialized (TrainingArguments API)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "trainer.train()\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and push to HuggingFace\n",
    "print(\"\\nüíæ Saving model...\")\n",
    "trainer.save_model(\"/kaggle/working/vazhi-v3_2-final\")\n",
    "\n",
    "# Merge LoRA weights\n",
    "print(\"\\nüîÄ Merging LoRA weights...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Ensure repo exists before pushing\n",
    "api = HfApi()\n",
    "try:\n",
    "    api.create_repo(OUTPUT_MODEL, exist_ok=True)\n",
    "    print(f\"‚úÖ Created/verified repo: {OUTPUT_MODEL}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Repo creation: {e}\")\n",
    "\n",
    "# Push to HuggingFace\n",
    "print(f\"\\nüì§ Pushing to {OUTPUT_MODEL}...\")\n",
    "merged_model.push_to_hub(OUTPUT_MODEL, private=False)\n",
    "tokenizer.push_to_hub(OUTPUT_MODEL, private=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Model uploaded to: https://huggingface.co/{OUTPUT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Re-enable cache for inference\nmerged_model.config.use_cache = True\n\n# Test prompts\ntest_prompts = [\n    \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç\",\n    \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ©‡Øç ‡Æ§‡Æ≤‡Øà‡Æ®‡Æï‡Æ∞‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\",\n    \"2+2 ‡Æé‡Æ©‡Øç‡Æ©?\",\n    \"‡Æ™‡Øä‡Æô‡Øç‡Æï‡Æ≤‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Øã‡Æ§‡ØÅ ‡Æï‡Øä‡Æ£‡Øç‡Æü‡Ææ‡Æü‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ?\",\n    \"‡Æ§‡Æø‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡Æ±‡Æ≥‡Æø‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Øç ‡ÆÖ‡Æ§‡Æø‡Æï‡Ææ‡Æ∞‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\",\n]\n\nprint(\"\\nüß™ Testing model...\\n\")\nprint(\"   (Using anti-repeat decoding per GPT5.2)\")\nprint(\"   repetition_penalty=1.3, no_repeat_ngram_size=3, top_p=0.9, temp=0.5\\n\")\n\nfor prompt in test_prompts:\n    full_prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n    \n    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(merged_model.device)\n    \n    with torch.no_grad():\n        outputs = merged_model.generate(\n            **inputs,\n            max_new_tokens=100,\n            # Anti-repeat decoding defaults (per GPT5.2)\n            temperature=0.5,\n            top_p=0.9,\n            do_sample=True,\n            repetition_penalty=1.3,\n            no_repeat_ngram_size=3,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    # Extract assistant response\n    if \"<|im_start|>assistant\" in response:\n        response = response.split(\"<|im_start|>assistant\")[-1]\n        response = response.split(\"<|im_end|>\")[0].strip()\n    \n    # Check for garbage output\n    if \"systemsystem\" in response.lower() or len(set(response.split())) < 3:\n        print(f\"Q: {prompt}\")\n        print(f\"A: ‚ùå GARBAGE/REPEAT DETECTED: {response[:100]}...\")\n    else:\n        print(f\"Q: {prompt}\")\n        print(f\"A: {response}\")\n    print(\"-\" * 50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Fixes from v3.1:\n",
    "1. ‚úÖ **Data format consistency** - Only ChatML samples used (raw text excluded)\n",
    "2. ‚úÖ **Pinned versions** - transformers==4.46.3, trl==0.11.4, etc.\n",
    "3. ‚úÖ **Single GPU forced** - CUDA_VISIBLE_DEVICES=0 at top\n",
    "4. ‚úÖ **fp16 on T4** - Not bf16\n",
    "5. ‚úÖ **4-bit QLoRA** - More stable on Kaggle\n",
    "\n",
    "### Expected Results:\n",
    "- No more \"systemsystemsystem...\" garbage output\n",
    "- Model should respond coherently in Tamil\n",
    "- Thirukkural distribution ~25% (down from 71%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
