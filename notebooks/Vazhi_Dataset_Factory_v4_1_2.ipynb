{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI Dataset Factory v4.1.2 — Stage 2 + 3 (GPU)\n",
    "\n",
    "**Resumes from HF checkpoint** — Stage 1 already completed in v4.1 (CPU).\n",
    "\n",
    "```\n",
    "Stage 1 (DONE): 37,947 raw samples → CryptoYogi/vazhi-raw-tamil-qa-v1\n",
    "┌─ Stage 2: CURATE (this notebook, GPU) ─────────────────────────┐\n",
    "│ Pass 1 (CPU): lang-id → heuristics → dedup → toxicity          │\n",
    "│ Pass 2 (GPU): perplexity + IndicSBERT → composite score        │\n",
    "│ → Upload curated to HF: CryptoYogi/vazhi-curated-tamil-qa-v1   │\n",
    "├─ Stage 3: COMPOSE (CPU) ────────────────────────────────────────┤\n",
    "│ Filter → ChatML → absolute count targets → stratified split     │\n",
    "│ → Upload final SFT to HF: CryptoYogi/vazhi-tamil-sft-v4_1      │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Sources (8 total, from Stage 1):**\n",
    "- **IndicAlign** (5 subsets): Dolly_T, WikiHow, Indic_ShareLlama, HHRLHF_T, Toxic_Matrix\n",
    "- **Local**: vazhi-packs (3,007), handcrafted (147), general (8,793)\n",
    "\n",
    "**Fixes in v4.1.2:**\n",
    "- Source-aware filtering: vazhi_packs/handcrafted bypass lang-id and tamil_pct filters\n",
    "- fasttext NumPy 2.x patch (np.array copy=False → np.asarray)\n",
    "- GPU runtime required for perplexity + SBERT scoring\n",
    "\n",
    "**Run on:** Colab T4/A100 or Kaggle P100 (GPU required)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 1. Config & Dependencies\n!pip install -q datasets huggingface_hub fasttext-wheel sentence-transformers hdbscan\n\nimport json\nimport os\nimport re\nimport gc\nimport sys\nimport time\nimport random\nimport hashlib\nfrom collections import Counter\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom huggingface_hub import login, HfApi\n\n# === CONFIG ===\nVERSION = \"4.1.2\"\nRAW_DATASET = \"CryptoYogi/vazhi-raw-tamil-qa-v1\"\nCURATED_DATASET = \"CryptoYogi/vazhi-curated-tamil-qa-v1\"\nOUTPUT_DATASET = \"CryptoYogi/vazhi-tamil-sft-v4_1\"\nDAPT_MODEL = \"CryptoYogi/qwen3-0.6b-tamil-v1_1\"\nTOKENIZER_MODEL = \"Qwen/Qwen3-0.6B\"\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\nSFT_MAX_SEQ_LENGTH = 2048\n\nSYSTEM_PROMPT = (\n    \"\\u0ba8\\u0bc0\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd VAZHI (\\u0bb5\\u0bb4\\u0bbf), \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd \\u0bae\\u0b95\\u0bcd\\u0b95\\u0bb3\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bbe\\u0ba9 AI \\u0b89\\u0ba4\\u0bb5\\u0bbf\\u0baf\\u0bbe\\u0bb3\\u0bb0\\u0bcd. \"\n    \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bbf\\u0bb2\\u0bcd \\u0ba4\\u0bc6\\u0bb3\\u0bbf\\u0bb5\\u0bbe\\u0b95\\u0bb5\\u0bc1\\u0bae\\u0bcd \\u0b89\\u0ba4\\u0bb5\\u0bbf\\u0baf\\u0bbe\\u0b95\\u0bb5\\u0bc1\\u0bae\\u0bcd \\u0baa\\u0ba4\\u0bbf\\u0bb2\\u0bb3\\u0bbf\\u0baf\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd. \"\n    '\\u0ba4\\u0bc6\\u0bb0\\u0bbf\\u0baf\\u0bbe\\u0bb5\\u0bbf\\u0b9f\\u0bcd\\u0b9f\\u0bbe\\u0bb2\\u0bcd \"\\u0ba4\\u0bc6\\u0bb0\\u0bbf\\u0baf\\u0bb5\\u0bbf\\u0bb2\\u0bcd\\u0bb2\\u0bc8\" \\u0b8e\\u0ba9\\u0bcd\\u0bb1\\u0bc1 \\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd.'\n)\n\nBUCKET_TARGETS = {\n    \"vazhi_packs\":    {\"min\": 2500, \"target\": 3000, \"max\": 3000},\n    \"handcrafted\":    {\"min\": 100,  \"target\": 147,  \"max\": 200},\n    \"general\":        {\"min\": 300,  \"target\": 500,  \"max\": 700},\n    \"indicalign\":     {\"min\": 10000, \"target\": 12000, \"max\": 14000},\n    \"safety\":         {\"min\": 1500, \"target\": 2000,  \"max\": 2500},\n}\n\nSOURCE_PRIORITY = {\n    \"vazhi_packs\": 10, \"handcrafted\": 10,\n    \"general\": 5,\n    \"indicalign\": 2,\n}\n\n# GPU check + tier detection\n# Qwen3 has 151K vocab — logits tensor = batch × seq × 151K × 2 bytes\n# L4 (22GB): batch=16 → ~2.5GB logits, safe\n# A100 (40GB): batch=32 → ~5GB logits, safe\nassert torch.cuda.is_available(), \"GPU required! Change runtime: Runtime > Change runtime type > GPU\"\ngpu_name = torch.cuda.get_device_name(0).lower()\nIS_HIGH_END_GPU = any(x in gpu_name for x in [\"a100\", \"l4\", \"h100\", \"a10\"])\nMODEL_DTYPE = torch.bfloat16 if IS_HIGH_END_GPU else torch.float16\nPPL_BATCH_SIZE = 16  # Conservative for 151K vocab logits (even A100 needs care)\nSBERT_BATCH_SIZE = 512 if IS_HIGH_END_GPU else 256\nVRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n\n# Scale PPL batch to available VRAM\nif VRAM_GB >= 40:\n    PPL_BATCH_SIZE = 32\nelif VRAM_GB >= 22:\n    PPL_BATCH_SIZE = 16\nelse:\n    PPL_BATCH_SIZE = 8\n\nprint(f\"\\u2705 GPU: {torch.cuda.get_device_name(0)} ({VRAM_GB:.0f}GB)\")\nprint(f\"   Tier: {'high-end' if IS_HIGH_END_GPU else 'standard'} \\u2192 dtype={MODEL_DTYPE}, PPL batch={PPL_BATCH_SIZE}, SBERT batch={SBERT_BATCH_SIZE}\")\nprint(f\"\\u2705 Config loaded: Dataset Factory v{VERSION}\")\nprint(f\"   Resuming from HF: {RAW_DATASET}\")\nprint(f\"   Output: {OUTPUT_DATASET}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2. HuggingFace login\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "    login(token=hf_token)\n",
    "    print(\"\\u2705 Logged in via Kaggle secrets\")\n",
    "except Exception:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        hf_token = userdata.get('HF_TOKEN')\n",
    "        login(token=hf_token)\n",
    "        print(\"\\u2705 Logged in via Colab secrets\")\n",
    "    except Exception:\n",
    "        login()\n",
    "        print(\"\\u2705 Logged in interactively\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3. Resume from HF checkpoint (Stage 1 already complete)\n",
    "print(f\"Loading raw dataset from HF: {RAW_DATASET}\")\n",
    "raw_ds = load_dataset(RAW_DATASET, split=\"train\")\n",
    "total_raw = len(raw_ds)\n",
    "print(f\"\\u2705 Loaded {total_raw:,} samples\")\n",
    "print(f\"   Columns: {raw_ds.column_names}\")\n",
    "print(f\"   Sources: {Counter(raw_ds['source']).most_common()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 4. Helper functions\n",
    "\n",
    "def to_chatml(instruction, output, system_prompt=None):\n",
    "    sp = system_prompt or SYSTEM_PROMPT\n",
    "    return (\n",
    "        f\"<|im_start|>system\\n{sp}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{instruction}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n{output}<|im_end|>\"\n",
    "    )\n",
    "\n",
    "CHATML_PATTERN = re.compile(\n",
    "    r'<\\|im_start\\|>system\\n.+?<\\|im_end\\|>\\n'\n",
    "    r'<\\|im_start\\|>user\\n(.+?)<\\|im_end\\|>\\n'\n",
    "    r'<\\|im_start\\|>assistant\\n(.+?)<\\|im_end\\|>',\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "def validate_chatml_strict(text):\n",
    "    match = CHATML_PATTERN.search(text)\n",
    "    if not match:\n",
    "        return False, \"no ChatML structure found\"\n",
    "    if len(match.group(1).strip()) < 2:\n",
    "        return False, \"empty user content\"\n",
    "    if len(match.group(2).strip()) < 2:\n",
    "        return False, \"empty assistant content\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "def count_tamil_chars(text):\n",
    "    return sum(1 for c in text if '\\u0b80' <= c <= '\\u0bff')\n",
    "\n",
    "def tamil_char_pct(text):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    return count_tamil_chars(text) / len(text)\n",
    "\n",
    "def is_verbatim_kural_qa(question, answer):\n",
    "    verbatim_patterns = [\n",
    "        r'\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*\\d+\\s*(\\u0b8e\\u0ba9\\u0bcd\\u0ba9|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd|\\u0b8e\\u0bb4\\u0bc1\\u0ba4\\u0bbf\\s*\\u0b95\\u0bbe\\u0b9f\\u0bcd\\u0b9f\\u0bc1|\\u0b95\\u0bc2\\u0bb1\\u0bc1\\u0b95)',\n",
    "        r'(first|\\u0bae\\u0bc1\\u0ba4\\u0bb2\\u0bcd)\\s*\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*(\\u0b8e\\u0ba9\\u0bcd\\u0ba9|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd)',\n",
    "        r'\\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bbf\\u0ba9\\u0bcd\\s+\\u0bae\\u0bc1\\u0ba4\\u0bb2\\u0bcd\\s+\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd',\n",
    "        r'\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*[\\d]+(?:\\s*\\u0b90)?\\s*\\u0b8e\\u0bb4\\u0bc1\\u0ba4\\u0bbf',\n",
    "    ]\n",
    "    for pat in verbatim_patterns:\n",
    "        if re.search(pat, question, re.IGNORECASE):\n",
    "            return True\n",
    "    if len(answer) < 200 and \"\\n\" in answer and not any(\n",
    "        w in answer for w in [\"\\u0bb5\\u0bbf\\u0bb3\\u0b95\\u0bcd\\u0b95\\u0bae\\u0bcd\", \"\\u0baa\\u0bca\\u0bb0\\u0bc1\\u0bb3\\u0bcd\", \"\\u0b85\\u0bb0\\u0bcd\\u0ba4\\u0bcd\\u0ba4\\u0bae\\u0bcd\"]\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Self-test\n",
    "good = to_chatml(\"test\", \"answer\")\n",
    "assert validate_chatml_strict(good)[0]\n",
    "print(\"\\u2705 Helper functions ready\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Checkpoint Resume (run ONLY if resuming from a crash)\nIf the runtime restarted and you lost in-memory data, run the next cell to reload from the local checkpoint saved after PPL scoring. **Skip this cell on a fresh run.**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# CHECKPOINT RESUME — Run ONLY if resuming from crash. Skip on fresh run.\n# Loads deduped list with PPL scores from local checkpoint file.\n\nCHECKPOINT_PATH = \"/content/vazhi_ppl_checkpoint.json\"\n\nimport json, os\nif os.path.exists(CHECKPOINT_PATH):\n    print(f\"Loading checkpoint from {CHECKPOINT_PATH}...\")\n    with open(CHECKPOINT_PATH) as f:\n        checkpoint = json.load(f)\n    deduped = checkpoint[\"deduped\"]\n    total_raw = checkpoint[\"total_raw\"]\n    candidates = [None] * checkpoint[\"n_candidates\"]  # placeholder for summary\n    clean_candidates = [None] * checkpoint[\"n_clean\"]\n    heuristic_dropped = checkpoint[\"heuristic_dropped\"]\n    total_dedup_removed = checkpoint[\"total_dedup_removed\"]\n    safety_count = checkpoint[\"safety_count\"]\n    toxic_dropped = checkpoint[\"toxic_dropped\"]\n    print(f\"✅ Resumed {len(deduped):,} samples with PPL scores\")\n    print(f\"   PPL scored: {sum(1 for s in deduped if s.get('perplexity') is not None):,}\")\n    print(f\"   → Skip cells 6-11, continue from cell 12 (SBERT)\")\nelse:\n    print(f\"No checkpoint at {CHECKPOINT_PATH} — run fresh from cell 6\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 2: CURATE\n",
    "\n",
    "### Pass 1 (CPU): Language detection → heuristics → dedup → toxicity\n",
    "### Pass 2 (GPU): Perplexity scoring → semantic categorization → composite quality score\n",
    "\n",
    "**Fix in v4.1.2:** vazhi_packs and handcrafted bypass lang-id and tamil_pct filters (hand-curated product data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Pass 1A: Language Detection with fasttext\n# Patch fasttext for NumPy 2.x compatibility before importing\n\nimport fasttext\nft_file = os.path.join(os.path.dirname(fasttext.__file__), 'FastText.py')\nwith open(ft_file) as f:\n    code = f.read()\nif 'np.array(probs, copy=False)' in code:\n    code = code.replace('np.array(probs, copy=False)', 'np.asarray(probs)')\n    with open(ft_file, 'w') as f:\n        f.write(code)\n    print(\"  Patched fasttext for NumPy 2.x\")\nfor mod in [k for k in sys.modules if k.startswith('fasttext')]:\n    del sys.modules[mod]\nimport fasttext\nimport urllib.request\n\nLID_MODEL_PATH = \"/tmp/lid.176.bin\"\nif not os.path.exists(LID_MODEL_PATH):\n    print(\"Downloading fasttext language ID model (126MB)...\")\n    urllib.request.urlretrieve(\n        \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\",\n        LID_MODEL_PATH,\n    )\nlid_model = fasttext.load_model(LID_MODEL_PATH)\nprint(\"\\u2705 fasttext lid.176.bin loaded\")\n\n\ndef detect_lang(text):\n    clean = text.replace(\"\\n\", \" \").strip()\n    if not clean:\n        return \"unknown\", 0.0\n    labels, scores = lid_model.predict(clean, k=1)\n    if not labels:\n        return \"unknown\", 0.0\n    return labels[0].replace(\"__label__\", \"\"), float(scores[0])\n\n\ndef add_lang_fields(example):\n    combined = example[\"instruction\"] + \" \" + example[\"output\"]\n    lang_id, lang_conf = detect_lang(combined)\n    example[\"lang_id\"] = lang_id\n    example[\"lang_confidence\"] = round(lang_conf, 4)\n    return example\n\n\nprint(f\"Running language detection on {len(raw_ds):,} samples...\")\nt0 = time.time()\nraw_ds = raw_ds.map(add_lang_fields, desc=\"Language detection\")\nlang_elapsed = time.time() - t0\n\nlang_counts = Counter(raw_ds[\"lang_id\"])\nprint(f\"\\nLanguage distribution (top 10): {lang_counts.most_common(10)}\")\nprint(f\"   Lang-id took {lang_elapsed:.0f}s ({len(raw_ds)/lang_elapsed:.0f} samples/sec)\")\n\n# SOURCE-AWARE FILTER: Keep Tamil OR hand-curated product data\nbefore = len(raw_ds)\ntamil_ds = raw_ds.filter(\n    lambda x: (x[\"lang_id\"] == \"ta\" and x[\"lang_confidence\"] >= 0.6)\n    or x[\"source\"] in (\"vazhi_packs\", \"handcrafted\")\n)\nlang_dropped = before - len(tamil_ds)\n\nprint(f\"\\n\\u2705 Language detection complete\")\nprint(f\"   Before: {before:,}\")\nprint(f\"   Dropped (non-Tamil, excl. vazhi_packs/handcrafted): {lang_dropped:,}\")\nprint(f\"   Remaining: {len(tamil_ds):,}\")\n\n# Per-source check\nsrc_counts = Counter(tamil_ds[\"source\"])\nprint(f\"   Per-source: {src_counts.most_common()}\")\n\ncandidates = tamil_ds.to_list()\nprint(f\"\\u2705 {len(candidates):,} candidates in memory\")\n\ndel raw_ds, tamil_ds\ngc.collect()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pass 1B: Quality Heuristics + Format Sanity\n",
    "# FIX: vazhi_packs/handcrafted bypass tamil_pct filter\n",
    "\n",
    "def repetition_score(text):\n",
    "    toks = text.split()\n",
    "    if len(toks) < 5:\n",
    "        return 0.0\n",
    "    return Counter(toks).most_common(1)[0][1] / len(toks)\n",
    "\n",
    "def is_sane(instruction, output):\n",
    "    if \"<think>\" in output: return False\n",
    "    if output.strip() == instruction.strip(): return False\n",
    "    if \"data:image\" in output: return False\n",
    "    if \"<|im_start|>system\" in output: return False\n",
    "    if \"systemsystem\" in output.lower(): return False\n",
    "    return True\n",
    "\n",
    "def is_echo(instruction, output):\n",
    "    if not instruction or not output:\n",
    "        return False\n",
    "    instr_set = set(instruction.split())\n",
    "    out_set = set(output.split())\n",
    "    if not out_set:\n",
    "        return False\n",
    "    return len(instr_set & out_set) / len(out_set) > 0.80\n",
    "\n",
    "print(f\"Running quality heuristics on {len(candidates):,} candidates...\")\n",
    "heuristic_stats = Counter()\n",
    "clean_candidates = []\n",
    "\n",
    "for s in candidates:\n",
    "    flags = []\n",
    "\n",
    "    # Tamil % — SKIP for hand-curated product data\n",
    "    if s[\"tamil_pct\"] < 0.30 and s[\"source\"] not in (\"vazhi_packs\", \"handcrafted\"):\n",
    "        flags.append(\"low_tamil_pct\")\n",
    "\n",
    "    rep = repetition_score(s[\"output\"])\n",
    "    s[\"repetition\"] = round(rep, 4)\n",
    "    if rep > 0.25 and len(s[\"output\"].split()) > 20:\n",
    "        flags.append(\"high_repetition\")\n",
    "\n",
    "    if is_echo(s[\"instruction\"], s[\"output\"]):\n",
    "        flags.append(\"echo\")\n",
    "\n",
    "    if not is_sane(s[\"instruction\"], s[\"output\"]):\n",
    "        flags.append(\"format_insane\")\n",
    "\n",
    "    if len(s[\"output\"].strip()) < 10:\n",
    "        flags.append(\"trivial_output\")\n",
    "    if len(s[\"instruction\"].strip()) < 5:\n",
    "        flags.append(\"trivial_instruction\")\n",
    "\n",
    "    s[\"heuristic_flags\"] = flags\n",
    "    for f in flags:\n",
    "        heuristic_stats[f] += 1\n",
    "    if len(flags) == 0:\n",
    "        clean_candidates.append(s)\n",
    "\n",
    "heuristic_dropped = len(candidates) - len(clean_candidates)\n",
    "print(f\"\\n\\u2705 Heuristic filtering complete\")\n",
    "print(f\"   Before: {len(candidates):,}\")\n",
    "print(f\"   Dropped: {heuristic_dropped:,}\")\n",
    "print(f\"   Remaining: {len(clean_candidates):,}\")\n",
    "print(f\"   Flags: {heuristic_stats.most_common()}\")\n",
    "# Verify vazhi_packs survived\n",
    "vp_count = sum(1 for s in clean_candidates if s[\"source\"] == \"vazhi_packs\")\n",
    "hc_count = sum(1 for s in clean_candidates if s[\"source\"] == \"handcrafted\")\n",
    "print(f\"   vazhi_packs: {vp_count:,} | handcrafted: {hc_count:,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pass 1C: Deduplication (exact only — MinHash skipped for <50K pool)\n",
    "\n",
    "print(f\"Deduplicating {len(clean_candidates):,} candidates...\")\n",
    "\n",
    "seen = {}\n",
    "for s in clean_candidates:\n",
    "    key = s[\"instruction\"][:200].strip().lower()\n",
    "    if key not in seen:\n",
    "        seen[key] = s\n",
    "    else:\n",
    "        existing_priority = SOURCE_PRIORITY.get(seen[key][\"source\"], 1)\n",
    "        new_priority = SOURCE_PRIORITY.get(s[\"source\"], 1)\n",
    "        if new_priority > existing_priority:\n",
    "            seen[key] = s\n",
    "        elif new_priority == existing_priority and s[\"tamil_pct\"] > seen[key][\"tamil_pct\"]:\n",
    "            seen[key] = s\n",
    "\n",
    "deduped = list(seen.values())\n",
    "exact_dupes = len(clean_candidates) - len(deduped)\n",
    "print(f\"  Exact dedupe: {len(clean_candidates):,} \\u2192 {len(deduped):,} ({exact_dupes:,} removed)\")\n",
    "\n",
    "for s in deduped:\n",
    "    s[\"is_duplicate\"] = False\n",
    "\n",
    "total_dedup_removed = exact_dupes\n",
    "print(f\"\\u2705 Deduplication complete\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pass 1D: Toxicity Screening (source-aware)\n",
    "\n",
    "TOXIC_WORDS_TA = [\n",
    "    \"\\u0ba4\\u0bc7\\u0bb5\\u0b9f\\u0bbf\\u0baf\\u0bbe\", \"\\u0bb5\\u0bc7\\u0b9a\\u0bbf\",\n",
    "    \"\\u0baa\\u0bc1\\u0ba3\\u0bcd\\u0b9f\\u0bc8\", \"\\u0ba4\\u0bb1\\u0bcd\\u0b95\\u0bca\\u0bb2\\u0bc8\",\n",
    "    \"\\u0b95\\u0bca\\u0bb2\\u0bcd\\u0bb2\", \"\\u0bb5\\u0bc6\\u0b9f\\u0bbf\\u0baa\\u0bca\\u0bb0\\u0bc1\\u0bb3\\u0bcd\",\n",
    "]\n",
    "TOXIC_WORDS_EN = [\n",
    "    \"suicide method\", \"how to kill\", \"make a bomb\", \"hack into\",\n",
    "    \"child porn\", \"exploit children\",\n",
    "]\n",
    "\n",
    "def check_wordlist(text):\n",
    "    text_lower = text.lower()\n",
    "    flags = []\n",
    "    for word in TOXIC_WORDS_TA:\n",
    "        if word in text:\n",
    "            flags.append(f\"ta:{word[:10]}\")\n",
    "    for phrase in TOXIC_WORDS_EN:\n",
    "        if phrase in text_lower:\n",
    "            flags.append(f\"en:{phrase[:15]}\")\n",
    "    return flags\n",
    "\n",
    "def classify_toxicity(instruction, output, source, subset):\n",
    "    instr_flags = check_wordlist(instruction)\n",
    "    output_flags = check_wordlist(output)\n",
    "    if subset in (\"Toxic_Matrix\", \"HHRLHF_T\") and instr_flags and not output_flags:\n",
    "        return instr_flags, True\n",
    "    return instr_flags + output_flags, False\n",
    "\n",
    "print(f\"Running toxicity screening on {len(deduped):,} candidates...\")\n",
    "safety_count = 0\n",
    "toxic_dropped = 0\n",
    "for s in deduped:\n",
    "    tox_flags, is_safety = classify_toxicity(\n",
    "        s[\"instruction\"], s[\"output\"], s[\"source\"], s[\"subset\"]\n",
    "    )\n",
    "    s[\"toxicity_flags\"] = tox_flags\n",
    "    s[\"is_safety_sample\"] = is_safety\n",
    "    if is_safety:\n",
    "        safety_count += 1\n",
    "    elif tox_flags:\n",
    "        toxic_dropped += 1\n",
    "\n",
    "print(f\"\\u2705 Toxicity screening complete\")\n",
    "print(f\"   Safety samples: {safety_count:,}\")\n",
    "print(f\"   Toxic flagged: {toxic_dropped:,}\")\n",
    "print(f\"   Clean: {len(deduped) - safety_count - toxic_dropped:,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pass 1 Summary\n",
    "\n",
    "lang_dropped = total_raw - len(candidates)\n",
    "print(\"=\" * 60)\n",
    "print(\"PASS 1 SUMMARY (CPU Filters)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Raw input:          {total_raw:,}\")\n",
    "print(f\"  After lang-id:      {len(candidates):,} (-{lang_dropped:,})\")\n",
    "print(f\"  After heuristics:   {len(clean_candidates):,} (-{heuristic_dropped:,})\")\n",
    "print(f\"  After dedup:        {len(deduped):,} (-{total_dedup_removed:,})\")\n",
    "print(f\"  Safety routed:      {safety_count:,}\")\n",
    "print(f\"  Toxic flagged:      {toxic_dropped:,}\")\n",
    "print(f\"  \\u2192 Pass 1 output:   {len(deduped):,} candidates\")\n",
    "\n",
    "survivor_sources = Counter(s[\"source\"] for s in deduped)\n",
    "print(f\"\\nSurvivors by source:\")\n",
    "for src, count in survivor_sources.most_common():\n",
    "    print(f\"  {src}: {count:,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Pass 2E: Perplexity Scoring (GPU, BATCHED)\n# Qwen3 151K vocab → logits = batch × seq × 151K × 2 bytes\n# batch=16, seq=512 → ~2.5GB per batch (safe for L4 22GB)\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nprint(f\"Loading DAPT v1.1 model: {DAPT_MODEL}\")\nppl_tokenizer = AutoTokenizer.from_pretrained(DAPT_MODEL)\nif ppl_tokenizer.pad_token is None:\n    ppl_tokenizer.pad_token = ppl_tokenizer.eos_token\n\nppl_model = AutoModelForCausalLM.from_pretrained(\n    DAPT_MODEL, torch_dtype=MODEL_DTYPE, device_map=\"auto\"\n)\nppl_model.eval()\nprint(f\"\\u2705 PPL model loaded on {ppl_model.device} (dtype={MODEL_DTYPE})\")\n\nppl_candidates = deduped\nprint(f\"  Scoring {len(ppl_candidates):,} candidates in batches of {PPL_BATCH_SIZE}\")\n\n\ndef compute_perplexity_batch(texts, batch_size=PPL_BATCH_SIZE, max_length=512):\n    \"\"\"Batched perplexity: one forward pass per batch, per-sample loss with padding mask.\"\"\"\n    ppls = []\n    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        inputs = ppl_tokenizer(\n            batch, return_tensors=\"pt\", truncation=True,\n            max_length=max_length, padding=True\n        ).to(ppl_model.device)\n\n        with torch.no_grad():\n            logits = ppl_model(**inputs).logits\n\n        # Per-sample PPL with padding mask\n        for j in range(len(batch)):\n            shift_logits = logits[j, :-1, :]\n            shift_labels = inputs[\"input_ids\"][j, 1:]\n            shift_mask = inputs[\"attention_mask\"][j, 1:]\n\n            losses = loss_fct(shift_logits, shift_labels)\n            masked = losses * shift_mask\n            n_tokens = shift_mask.sum()\n            if n_tokens > 0:\n                ppls.append(torch.exp(masked.sum() / n_tokens).item())\n            else:\n                ppls.append(None)\n\n        # Free GPU memory between batches (151K vocab logits are huge)\n        del logits, inputs\n        torch.cuda.empty_cache()\n\n        if (i // batch_size) % 50 == 0:\n            print(f\"  ...{min(i+batch_size, len(texts)):,} / {len(texts):,}\")\n\n    return ppls\n\n\noutput_texts = [s[\"output\"] for s in ppl_candidates]\nprint(f\"\\nComputing perplexity...\")\nt0 = time.time()\nppl_scores = compute_perplexity_batch(output_texts)\nelapsed = time.time() - t0\n\nfor i, s in enumerate(ppl_candidates):\n    s[\"perplexity\"] = round(ppl_scores[i], 2) if ppl_scores[i] is not None else None\n\nfor s in deduped:\n    if \"perplexity\" not in s:\n        s[\"perplexity\"] = None\n\nscored = [s[\"perplexity\"] for s in deduped if s[\"perplexity\"] is not None]\nif scored:\n    print(f\"\\n\\u2705 Perplexity scoring complete in {elapsed:.0f}s ({elapsed/60:.1f} min)\")\n    print(f\"   Scored: {len(scored):,} / {len(deduped):,}\")\n    print(f\"   Speed: {len(scored)/elapsed:.0f} samples/sec\")\n    print(f\"   PPL: min={min(scored):.1f}, median={sorted(scored)[len(scored)//2]:.1f}, max={max(scored):.1f}\")\n    print(f\"   PPL < 50: {sum(1 for p in scored if p < 50):,}\")\n    print(f\"   PPL 50-200: {sum(1 for p in scored if 50 <= p < 200):,}\")\n    print(f\"   PPL >= 200 (garbage): {sum(1 for p in scored if p >= 200):,}\")\n\ndel ppl_model\ntorch.cuda.empty_cache()\nprint(\"\\u2705 PPL model unloaded\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# CHECKPOINT SAVE — Persist PPL-scored data to local file\n# Protects against runtime restart losing 15+ min of GPU work\n\nCHECKPOINT_PATH = \"/content/vazhi_ppl_checkpoint.json\"\n\ncheckpoint = {\n    \"deduped\": deduped,\n    \"total_raw\": total_raw,\n    \"n_candidates\": len(candidates),\n    \"n_clean\": len(clean_candidates),\n    \"heuristic_dropped\": heuristic_dropped,\n    \"total_dedup_removed\": total_dedup_removed,\n    \"safety_count\": safety_count,\n    \"toxic_dropped\": toxic_dropped,\n}\n\nimport json\nwith open(CHECKPOINT_PATH, \"w\") as f:\n    json.dump(checkpoint, f)\n\nsize_mb = os.path.getsize(CHECKPOINT_PATH) / 1e6\nprint(f\"✅ Checkpoint saved: {CHECKPOINT_PATH} ({size_mb:.1f} MB)\")\nprint(f\"   {len(deduped):,} samples with PPL scores preserved\")\nprint(f\"   If runtime restarts, run the resume cell to skip Pass 1 + PPL\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Pass 2F: SBERT Embeddings (skip HDBSCAN — domain classification uses keyword tagger)\n# HDBSCAN is O(n²) on 35K × 768-dim = too slow. Keyword tagger is faster and directly useful.\n\nfrom sentence_transformers import SentenceTransformer\n\nprint(\"Loading IndicSBERT...\")\nsbert_model = SentenceTransformer(\"l3cube-pune/tamil-sentence-similarity-sbert\")\nprint(f\"✅ IndicSBERT loaded\")\n\ninstructions = [s[\"instruction\"][:512] for s in deduped]\nprint(f\"Computing embeddings for {len(instructions):,} instructions (batch={SBERT_BATCH_SIZE})...\")\nt0 = time.time()\nembeddings = sbert_model.encode(\n    instructions, batch_size=SBERT_BATCH_SIZE, show_progress_bar=True, normalize_embeddings=True\n)\nembed_elapsed = time.time() - t0\nprint(f\"✅ Embeddings: {embeddings.shape} in {embed_elapsed:.0f}s ({len(instructions)/embed_elapsed:.0f} samples/sec)\")\n\n# Skip HDBSCAN — domain classification uses keyword tagger (next cell)\nfor s in deduped:\n    s[\"embedding_cluster\"] = -1\n    s[\"auto_category\"] = \"unclustered\"\n\ndel sbert_model, embeddings\ntorch.cuda.empty_cache()\nprint(f\"✅ Done — domain classification in next cell\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pass 2G: Composite Quality Score + Tokenized Length\n",
    "\n",
    "from transformers import AutoTokenizer as AT\n",
    "\n",
    "print(f\"Loading tokenizer: {TOKENIZER_MODEL}\")\n",
    "sft_tokenizer = AT.from_pretrained(TOKENIZER_MODEL, trust_remote_code=True)\n",
    "print(f\"\\u2705 Tokenizer loaded (vocab: {sft_tokenizer.vocab_size:,})\")\n",
    "\n",
    "\n",
    "def compute_quality_score(s):\n",
    "    lang_conf = s.get(\"lang_confidence\", 0.0)\n",
    "    rep = s.get(\"repetition\", 0.0)\n",
    "    tamil = s.get(\"tamil_pct\", 0.0)\n",
    "    tok_len = s.get(\"tokenized_length\", SFT_MAX_SEQ_LENGTH + 1)\n",
    "    tox = s.get(\"toxicity_flags\", [])\n",
    "    return round(\n",
    "        lang_conf * 0.35 +\n",
    "        (1 / (1 + rep * 10)) * 0.25 +\n",
    "        tamil * 0.20 +\n",
    "        (1.0 if tok_len <= SFT_MAX_SEQ_LENGTH else 0.0) * 0.10 +\n",
    "        (0.0 if tox else 1.0) * 0.10\n",
    "    , 4)\n",
    "\n",
    "\n",
    "print(f\"Scoring {len(deduped):,} samples...\")\n",
    "for i, s in enumerate(deduped):\n",
    "    chatml_text = to_chatml(s[\"instruction\"], s[\"output\"])\n",
    "    s[\"tokenized_length\"] = len(sft_tokenizer.encode(chatml_text, add_special_tokens=False))\n",
    "    s[\"quality_score\"] = compute_quality_score(s)\n",
    "    if (i + 1) % 10000 == 0:\n",
    "        print(f\"  ...{i+1:,} / {len(deduped):,}\")\n",
    "\n",
    "tok_lengths = [s[\"tokenized_length\"] for s in deduped]\n",
    "quality_scores = [s[\"quality_score\"] for s in deduped]\n",
    "print(f\"\\n\\u2705 Scoring complete\")\n",
    "print(f\"   Tokens: min={min(tok_lengths)}, median={sorted(tok_lengths)[len(tok_lengths)//2]}, max={max(tok_lengths)}\")\n",
    "print(f\"   Within 2048: {sum(1 for t in tok_lengths if t <= SFT_MAX_SEQ_LENGTH):,} / {len(tok_lengths):,}\")\n",
    "print(f\"   Quality: min={min(quality_scores):.3f}, median={sorted(quality_scores)[len(quality_scores)//2]:.3f}\")\n",
    "print(f\"   Score >= 0.45: {sum(1 for q in quality_scores if q >= 0.45):,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Pass 2H: Domain Classification (keyword-based + source-aware)\n# Makes curated dataset filterable by domain for selective SFT emphasis\n\nDOMAIN_KEYWORDS = {\n    \"healthcare\": [\n        \"மருத்துவ\", \"சிகிச்சை\", \"நோய்\", \"மருந்து\", \"உடல்\", \"ஆரோக்கிய\",\n        \"மருத்துவர்\", \"காய்ச்சல்\", \"நீரிழிவு\", \"இரத்த\", \"புற்றுநோய்\",\n        \"கர்ப்ப\", \"தடுப்பூசி\", \"ஊட்டச்சத்து\", \"முதலுதவி\", \"மனநல\",\n        \"hospital\", \"doctor\", \"health\", \"diabetes\", \"fever\", \"medicine\",\n    ],\n    \"legal\": [\n        \"சட்ட\", \"வழக்கு\", \"நீதிமன்ற\", \"உரிமை\", \"FIR\", \"காவல்\",\n        \"வழக்கறிஞர்\", \"ஜாமின்\", \"தண்டனை\", \"குற்ற\", \"சொத்து\",\n        \"பத்திரம்\", \"விவாகரத்து\", \"புகார்\", \"நுகர்வோர்\",\n        \"legal\", \"court\", \"law\", \"police\", \"rights\", \"property\",\n    ],\n    \"education\": [\n        \"கல்வி\", \"பள்ளி\", \"பல்கலை\", \"தேர்வு\", \"படிப்பு\", \"மாணவ\",\n        \"புலமைப்பரிசு\", \"NEET\", \"கல்லூரி\", \"பட்டம்\", \"ஆசிரிய\",\n        \"பாடத்திட்ட\", \"சான்றிதழ்\", \"அரசுப்பள்ளி\", \"உதவித்தொகை\",\n        \"school\", \"education\", \"exam\", \"scholarship\", \"student\",\n    ],\n    \"security\": [\n        \"மோசடி\", \"ஏமாற்ற\", \"பாதுகாப்பு\", \"OTP\", \"ஹேக்\",\n        \"சைபர்\", \"போலி\", \"வங்கி மோசடி\", \"ஃபிஷிங்\", \"லிங்க்\",\n        \"scam\", \"fraud\", \"phishing\", \"hack\", \"password\", \"cyber\",\n    ],\n    \"government\": [\n        \"அரசு\", \"திட்ட\", \"ஓய்வூதிய\", \"ரேஷன்\", \"மானிய\",\n        \"ஆதார்\", \"வாக்காளர்\", \"பிறப்பு சான்றிதழ்\", \"பாஸ்போர்ட்\",\n        \"வருமான சான்றிதழ்\", \"சமூக நல\", \"இலவச\", \"அரசாணை\",\n        \"pension\", \"ration\", \"scheme\", \"government\", \"subsidy\", \"aadhaar\",\n    ],\n    \"culture\": [\n        \"திருக்குறள்\", \"பண்பாடு\", \"கலாச்சார\", \"பொங்கல்\", \"திருவிழா\",\n        \"தமிழ் இலக்கிய\", \"சங்க\", \"பாரதியார்\", \"கோவில்\", \"நாட்டுப்புற\",\n        \"பரதநாட்டிய\", \"கர்நாடக இசை\", \"சிலப்பதிகாரம்\", \"தொல்காப்பிய\",\n        \"culture\", \"temple\", \"festival\", \"literature\", \"heritage\",\n    ],\n}\n\n\ndef classify_domain(sample):\n    \"\"\"Assign domain: vazhi_packs use subset, others use keyword matching.\"\"\"\n    source = sample[\"source\"]\n    subset = sample.get(\"subset\", \"\")\n\n    # vazhi_packs: subset IS the domain\n    if source == \"vazhi_packs\" and subset in DOMAIN_KEYWORDS:\n        return subset\n\n    # Safety samples get \"safety\" domain\n    if sample.get(\"is_safety_sample\", False):\n        return \"safety\"\n\n    # Handcrafted: map to safety/guardrails\n    if source == \"handcrafted\":\n        return \"safety\"\n\n    # Keyword matching on instruction + output\n    text = (sample[\"instruction\"] + \" \" + sample[\"output\"]).lower()\n    best_domain = \"general\"\n    best_hits = 0\n    for domain, keywords in DOMAIN_KEYWORDS.items():\n        hits = sum(1 for kw in keywords if kw.lower() in text)\n        if hits > best_hits:\n            best_hits = hits\n            best_domain = domain\n\n    # Require at least 2 keyword hits to avoid false positives\n    return best_domain if best_hits >= 2 else \"general\"\n\n\nprint(f\"Classifying {len(deduped):,} samples into domains...\")\nt0 = time.time()\nfor s in deduped:\n    s[\"domain\"] = classify_domain(s)\n\ndomain_counts = Counter(s[\"domain\"] for s in deduped)\nelapsed = time.time() - t0\nprint(f\"\\n✅ Domain classification complete in {elapsed:.1f}s\")\nprint(f\"\\nDomain distribution:\")\nfor domain, count in domain_counts.most_common():\n    pct = 100 * count / len(deduped)\n    print(f\"  {domain}: {count:,} ({pct:.1f}%)\")\n\n# Cross-check: vazhi_packs domains should match their pack names\nvp_domains = Counter(s[\"domain\"] for s in deduped if s[\"source\"] == \"vazhi_packs\")\nprint(f\"\\nvazhi_packs domains: {vp_domains.most_common()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Upload curated dataset to HF\n\ncurated_records = []\nfor s in deduped:\n    curated_records.append({\n        \"instruction\": s[\"instruction\"], \"output\": s[\"output\"],\n        \"source\": s[\"source\"], \"subset\": s[\"subset\"],\n        \"char_length\": s[\"char_length\"], \"tamil_pct\": s[\"tamil_pct\"],\n        \"lang_id\": s[\"lang_id\"], \"lang_confidence\": s[\"lang_confidence\"],\n        \"heuristic_flags\": s.get(\"heuristic_flags\", []),\n        \"repetition\": s.get(\"repetition\", 0.0),\n        \"toxicity_flags\": s.get(\"toxicity_flags\", []),\n        \"is_safety_sample\": s.get(\"is_safety_sample\", False),\n        \"is_duplicate\": s.get(\"is_duplicate\", False),\n        \"perplexity\": s.get(\"perplexity\"),\n        \"embedding_cluster\": s.get(\"embedding_cluster\"),\n        \"auto_category\": s.get(\"auto_category\"),\n        \"domain\": s.get(\"domain\", \"general\"),\n        \"quality_score\": s.get(\"quality_score\", 0.0),\n        \"tokenized_length\": s.get(\"tokenized_length\", 0),\n    })\n\nprint(f\"Uploading {len(curated_records):,} curated samples to {CURATED_DATASET}...\")\napi = HfApi()\napi.create_repo(CURATED_DATASET, repo_type=\"dataset\", exist_ok=True)\ncurated_ds = Dataset.from_list(curated_records)\ncurated_ds.push_to_hub(CURATED_DATASET)\nprint(f\"✅ Curated dataset uploaded: https://huggingface.co/datasets/{CURATED_DATASET}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 3: COMPOSE\n",
    "\n",
    "Select from curated pools with absolute count targets, build final SFT dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Stage 3A: Filtering\n",
    "\n",
    "print(\"Applying Stage 3 filters...\")\n",
    "df = curated_records.copy()\n",
    "before = len(df)\n",
    "\n",
    "df = [s for s in df if not s[\"is_duplicate\"]]\n",
    "print(f\"  After dedup: {len(df):,} (-{before - len(df):,})\")\n",
    "b = len(df)\n",
    "df = [s for s in df if s[\"tokenized_length\"] <= SFT_MAX_SEQ_LENGTH]\n",
    "print(f\"  After token \\u2264 {SFT_MAX_SEQ_LENGTH}: {len(df):,} (-{b - len(df):,})\")\n",
    "b = len(df)\n",
    "df = [s for s in df if s[\"lang_id\"] == \"ta\" or s[\"source\"] in (\"vazhi_packs\", \"handcrafted\")]\n",
    "print(f\"  After lang_id (source-aware): {len(df):,} (-{b - len(df):,})\")\n",
    "b = len(df)\n",
    "df = [s for s in df if len(s[\"heuristic_flags\"]) == 0]\n",
    "print(f\"  After clean heuristics: {len(df):,} (-{b - len(df):,})\")\n",
    "b = len(df)\n",
    "df = [s for s in df if len(s[\"toxicity_flags\"]) == 0 or s[\"is_safety_sample\"]]\n",
    "print(f\"  After toxicity: {len(df):,} (-{b - len(df):,})\")\n",
    "b = len(df)\n",
    "df = [s for s in df if s[\"quality_score\"] >= 0.45]\n",
    "print(f\"  After quality \\u2265 0.45: {len(df):,} (-{b - len(df):,})\")\n",
    "b = len(df)\n",
    "df = [s for s in df if s[\"perplexity\"] is None or s[\"perplexity\"] < 200]\n",
    "print(f\"  After PPL < 200: {len(df):,} (-{b - len(df):,})\")\n",
    "\n",
    "print(f\"\\n\\u2705 Filtering: {before:,} \\u2192 {len(df):,}\")\n",
    "filtered_sources = Counter(s[\"source\"] for s in df)\n",
    "print(f\"\\nFiltered pool by source:\")\n",
    "for src, count in filtered_sources.most_common():\n",
    "    print(f\"  {src}: {count:,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Stage 3B: Composition with Absolute Count Targets\n",
    "\n",
    "safety_pool = [s for s in df if s[\"is_safety_sample\"]]\n",
    "non_safety = [s for s in df if not s[\"is_safety_sample\"]]\n",
    "\n",
    "source_pools = {}\n",
    "for s in non_safety:\n",
    "    source_pools.setdefault(s[\"source\"], []).append(s)\n",
    "source_pools[\"safety\"] = safety_pool\n",
    "\n",
    "print(\"Composing final dataset...\")\n",
    "composed = {}\n",
    "total_composed = 0\n",
    "\n",
    "for bucket_name, targets in BUCKET_TARGETS.items():\n",
    "    pool = source_pools.get(bucket_name, [])\n",
    "    target = targets[\"target\"]\n",
    "    min_count = targets[\"min\"]\n",
    "    max_count = targets[\"max\"]\n",
    "\n",
    "    if len(pool) < min_count:\n",
    "        print(f\"  \\u26a0\\ufe0f {bucket_name}: only {len(pool):,} available, min is {min_count}\")\n",
    "        selected = pool\n",
    "    elif len(pool) <= target:\n",
    "        selected = pool\n",
    "    else:\n",
    "        use_count = min(target, max_count)\n",
    "        pool_sorted = sorted(pool, key=lambda x: x[\"quality_score\"], reverse=True)\n",
    "        selected = pool_sorted[:use_count]\n",
    "\n",
    "    composed[bucket_name] = selected\n",
    "    total_composed += len(selected)\n",
    "    print(f\"  {bucket_name}: {len(selected):,} / {len(pool):,} (target: {target}, range: {min_count}-{max_count})\")\n",
    "\n",
    "print(f\"\\n\\u2705 Composition: {total_composed:,} total\")\n",
    "\n",
    "all_met = True\n",
    "for bucket_name, targets in BUCKET_TARGETS.items():\n",
    "    actual = len(composed.get(bucket_name, []))\n",
    "    if actual < targets[\"min\"]:\n",
    "        print(f\"  \\u274c {bucket_name}: {actual} < min {targets['min']}\")\n",
    "        all_met = False\n",
    "if all_met:\n",
    "    print(\"\\u2705 All bucket minimums met\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Stage 3C: ChatML Conversion + Validation\n",
    "\n",
    "all_samples = []\n",
    "chatml_failures = 0\n",
    "\n",
    "for bucket_name, samples in composed.items():\n",
    "    for s in samples:\n",
    "        if s[\"source\"] == \"vazhi_packs\" and is_verbatim_kural_qa(s[\"instruction\"], s[\"output\"]):\n",
    "            continue\n",
    "        text = to_chatml(s[\"instruction\"], s[\"output\"])\n",
    "        valid, reason = validate_chatml_strict(text)\n",
    "        if not valid:\n",
    "            chatml_failures += 1\n",
    "            continue\n",
    "        all_samples.append({\n",
    "            \"text\": text, \"bucket\": bucket_name,\n",
    "            \"source\": s[\"source\"], \"subset\": s[\"subset\"],\n",
    "            \"quality_score\": s[\"quality_score\"],\n",
    "            \"tokenized_length\": s[\"tokenized_length\"],\n",
    "        })\n",
    "\n",
    "random.shuffle(all_samples)\n",
    "print(f\"\\u2705 ChatML: {len(all_samples):,} valid, {chatml_failures} failures\")\n",
    "\n",
    "bucket_counts = Counter(s[\"bucket\"] for s in all_samples)\n",
    "print(f\"\\n\\U0001f4ca Bucket distribution:\")\n",
    "for bucket, count in sorted(bucket_counts.items()):\n",
    "    pct = 100 * count / len(all_samples)\n",
    "    print(f\"  {bucket}: {count:,} ({pct:.1f}%)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Stage 3D: Stratified Train/Eval Split (90/10)\n",
    "\n",
    "EVAL_RATIO = 0.10\n",
    "train_samples = []\n",
    "eval_samples = []\n",
    "\n",
    "by_bucket = {}\n",
    "for s in all_samples:\n",
    "    by_bucket.setdefault(s[\"bucket\"], []).append(s)\n",
    "\n",
    "for bucket, samples in by_bucket.items():\n",
    "    random.shuffle(samples)\n",
    "    n_eval = max(1, int(len(samples) * EVAL_RATIO))\n",
    "    eval_samples.extend(samples[:n_eval])\n",
    "    train_samples.extend(samples[n_eval:])\n",
    "\n",
    "random.shuffle(train_samples)\n",
    "random.shuffle(eval_samples)\n",
    "\n",
    "print(f\"\\U0001f4ca Split: Train={len(train_samples):,} Eval={len(eval_samples):,}\")\n",
    "print(f\"  Eval ratio: {len(eval_samples) / (len(train_samples) + len(eval_samples)):.1%}\")\n",
    "\n",
    "max_tok = max(s[\"tokenized_length\"] for s in all_samples)\n",
    "print(f\"  Max tokens: {max_tok} (limit: {SFT_MAX_SEQ_LENGTH})\")\n",
    "assert max_tok <= SFT_MAX_SEQ_LENGTH"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Stage 3E: Upload to HuggingFace + Summary\n",
    "\n",
    "train_ds = Dataset.from_list(train_samples)\n",
    "eval_ds = Dataset.from_list(eval_samples)\n",
    "dataset_dict = DatasetDict({\"train\": train_ds, \"validation\": eval_ds})\n",
    "\n",
    "api.create_repo(OUTPUT_DATASET, repo_type=\"dataset\", exist_ok=True)\n",
    "dataset_dict.push_to_hub(OUTPUT_DATASET)\n",
    "\n",
    "print(f\"\\n\\u2705 Uploaded: https://huggingface.co/datasets/{OUTPUT_DATASET}\")\n",
    "print(f\"   Train: {len(train_ds):,} | Eval: {len(eval_ds):,}\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"VAZHI Dataset Factory v{VERSION} \\u2014 COMPLETE\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"\\n  Stage 1 (Retrieve): {total_raw:,} raw \\u2192 {RAW_DATASET}\")\n",
    "print(f\"  Stage 2 (Curate):   {len(curated_records):,} curated \\u2192 {CURATED_DATASET}\")\n",
    "print(f\"  Stage 3 (Compose):  {len(all_samples):,} final \\u2192 {OUTPUT_DATASET}\")\n",
    "\n",
    "print(f\"\\n  Buckets:\")\n",
    "for bucket, count in sorted(bucket_counts.items()):\n",
    "    target = BUCKET_TARGETS[bucket]\n",
    "    status = \"\\u2705\" if count >= target[\"min\"] else \"\\u26a0\\ufe0f\"\n",
    "    print(f\"    {status} {bucket}: {count:,} (target: {target['target']})\")\n",
    "\n",
    "print(f\"\\n  Sample outputs (2 per bucket):\")\n",
    "shown = Counter()\n",
    "for s in all_samples:\n",
    "    if shown[s['bucket']] < 2:\n",
    "        shown[s['bucket']] += 1\n",
    "        print(f\"\\n  [{s['bucket'].upper()}] source={s['source']} quality={s['quality_score']:.3f}\")\n",
    "        match = CHATML_PATTERN.search(s[\"text\"])\n",
    "        if match:\n",
    "            print(f\"    Q: {match.group(1)[:100]}\")\n",
    "            print(f\"    A: {match.group(2)[:150]}\")\n",
    "    if all(shown[b] >= 2 for b in BUCKET_TARGETS):\n",
    "        break\n",
    "\n",
    "print(f\"\\n\\u2705 Done! Next: SFT training with LoRA (r=8, q_proj+v_proj, 2 epochs)\")\n",
    "print(f\"   Base model: {DAPT_MODEL}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
