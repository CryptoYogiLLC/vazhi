{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI - Pre-trained Tamil Model Evaluation\n",
    "\n",
    "**Goal**: Test existing Tamil models to find one suitable for mobile deployment\n",
    "\n",
    "**Models to Test**:\n",
    "1. Sarvam-1 (2B) - Indian AI company, optimized for 10 Indian languages\n",
    "2. Gemma 2B Tamil - Community fine-tuned Google Gemma\n",
    "\n",
    "**Why**: Our Qwen2.5-0.5B LoRA training failed (output garbage despite good loss).\n",
    "Pre-trained models skip the training risk entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q llama-cpp-python huggingface_hub transformers accelerate\n",
    "!pip install -q bitsandbytes  # For loading large models in 4-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Questions\n",
    "\n",
    "Standard questions to evaluate Tamil capability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_QUESTIONS = [\n",
    "    \"வணக்கம், நீங்கள் யார்?\",\n",
    "    \"திருக்குறளின் முதல் குறள் என்ன?\",\n",
    "    \"தமிழ்நாட்டின் தலைநகரம் எது?\",\n",
    "    \"ஔவையாரின் ஆத்திசூடி பற்றி சொல்லுங்கள்\",\n",
    "]\n",
    "\n",
    "# Expected answers for validation\n",
    "EXPECTED = {\n",
    "    \"திருக்குறளின் முதல் குறள் என்ன?\": \"அகர முதல எழுத்தெல்லாம் ஆதி பகவன் முதற்றே உலகு\",\n",
    "    \"தமிழ்நாட்டின் தலைநகரம் எது?\": \"சென்னை\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Option A: Sarvam-1\n",
    "\n",
    "**Model**: sarvamai/sarvam-1 (2B parameters)\n",
    "**Optimized for**: 10 Indian languages including Tamil\n",
    "**Company**: Sarvam AI (Bangalore-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check if GGUF versions exist\n",
    "from huggingface_hub import HfApi, list_models\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Search for Sarvam GGUF\n",
    "print(\"Searching for Sarvam-1 GGUF models...\")\n",
    "models = list(api.list_models(search=\"sarvam gguf\", limit=10))\n",
    "for m in models:\n",
    "    print(f\"  - {m.id}\")\n",
    "\n",
    "print(\"\\nSearching for Tamil Gemma GGUF models...\")\n",
    "models = list(api.list_models(search=\"tamil gemma gguf\", limit=10))\n",
    "for m in models:\n",
    "    print(f\"  - {m.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Sarvam-1 using transformers (not GGUF)\n",
    "# This tells us if the model works before we try quantization\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "print(\"Loading Sarvam-1 in 4-bit...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "sarvam_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"sarvamai/sarvam-1\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "sarvam_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"sarvamai/sarvam-1\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded! Parameters: {sarvam_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sarvam(question, max_tokens=200):\n",
    "    \"\"\"Test Sarvam-1 with a Tamil question\"\"\"\n",
    "    # Sarvam uses a specific prompt format - check their docs\n",
    "    # For now, try simple prompt\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    \n",
    "    inputs = sarvam_tokenizer(prompt, return_tensors=\"pt\").to(sarvam_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = sarvam_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=sarvam_tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = sarvam_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract answer part\n",
    "    if \"Answer:\" in response:\n",
    "        return response.split(\"Answer:\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "# Test all questions\n",
    "print(\"=\" * 60)\n",
    "print(\"SARVAM-1 TEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in TEST_QUESTIONS:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {test_sarvam(q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Option B: Gemma 2B Tamil\n",
    "\n",
    "**Model**: abhinand/tamil-gemma-2b-instruct-v0.1\n",
    "**Base**: Google Gemma 2B\n",
    "**Fine-tuned by**: Community contributor (abhinand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear Sarvam from memory\n",
    "import gc\n",
    "del sarvam_model\n",
    "del sarvam_tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared Sarvam from memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gemma 2B Tamil\n",
    "print(\"Loading Gemma 2B Tamil in 4-bit...\")\n",
    "\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"abhinand/tamil-gemma-2b-instruct-v0.1\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"abhinand/tamil-gemma-2b-instruct-v0.1\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded! Parameters: {gemma_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gemma(question, max_tokens=200):\n",
    "    \"\"\"Test Gemma Tamil with a question\"\"\"\n",
    "    # Gemma instruct format\n",
    "    prompt = f\"<start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    inputs = gemma_tokenizer(prompt, return_tensors=\"pt\").to(gemma_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = gemma_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=gemma_tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = gemma_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract model response\n",
    "    if \"<start_of_turn>model\" in response:\n",
    "        return response.split(\"<start_of_turn>model\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "# Test all questions\n",
    "print(\"=\" * 60)\n",
    "print(\"GEMMA 2B TAMIL TEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in TEST_QUESTIONS:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {test_gemma(q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GGUF Conversion (for the better model)\n",
    "\n",
    "Once we identify which model works better, convert to GGUF for mobile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup llama.cpp for GGUF conversion\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "!cd llama.cpp && pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the better model and convert\n",
    "# Uncomment the model you want to convert:\n",
    "\n",
    "# MODEL_TO_CONVERT = \"sarvamai/sarvam-1\"\n",
    "# MODEL_TO_CONVERT = \"abhinand/tamil-gemma-2b-instruct-v0.1\"\n",
    "\n",
    "# Download and convert to GGUF\n",
    "# !python llama.cpp/convert_hf_to_gguf.py {MODEL_TO_CONVERT} --outfile tamil-model-f16.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build quantize tool\n",
    "# !cd llama.cpp && mkdir -p build && cd build && cmake .. && make -j4 llama-quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize to different sizes\n",
    "# !./llama.cpp/build/bin/llama-quantize tamil-model-f16.gguf tamil-model-q8_0.gguf q8_0\n",
    "# !./llama.cpp/build/bin/llama-quantize tamil-model-f16.gguf tamil-model-q4_k_m.gguf q4_k_m\n",
    "# !ls -lh tamil-model-*.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test GGUF Output Quality\n",
    "\n",
    "Critical test: Does the quantized model still produce good Tamil?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build llama-cli\n",
    "# !cd llama.cpp && cd build && make -j4 llama-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GGUF model\n",
    "# !./llama.cpp/build/bin/llama-cli -m tamil-model-q4_k_m.gguf \\\n",
    "#     -p \"திருக்குறளின் முதல் குறள் என்ன?\" \\\n",
    "#     -n 150 --temp 0.7 -ngl 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Decision\n",
    "\n",
    "Fill this in after testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "=================================================================\n",
    "MODEL COMPARISON SUMMARY\n",
    "=================================================================\n",
    "\n",
    "| Aspect              | Sarvam-1           | Gemma 2B Tamil     |\n",
    "|---------------------|--------------------|--------------------|  \n",
    "| Tamil Quality       | [Fill after test]  | [Fill after test]  |\n",
    "| Thirukkural Correct | [Yes/No]           | [Yes/No]           |\n",
    "| Response Coherence  | [1-5 rating]       | [1-5 rating]       |\n",
    "| GGUF Q4 Size        | [Size]             | [Size]             |\n",
    "| GGUF Quality        | [Works/Broken]     | [Works/Broken]     |\n",
    "\n",
    "DECISION: [Which model to use for VAZHI]\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
