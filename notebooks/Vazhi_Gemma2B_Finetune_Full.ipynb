{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI Gemma-2B Tamil Fine-tuning (Full Dataset)\n",
    "\n",
    "**Goal:** Fine-tune Gemma-2B Tamil with ALL VAZHI data for complete Tamil AI assistant\n",
    "\n",
    "**Key Insight:** Model size (1.63 GB) stays the same regardless of training data amount!\n",
    "So we train with everything to maximize knowledge.\n",
    "\n",
    "**Training Data:** 11,112 items covering:\n",
    "- üõ°Ô∏è Scam/Security protection\n",
    "- üèõÔ∏è Government schemes\n",
    "- üè• Healthcare\n",
    "- üìö Education\n",
    "- ‚öñÔ∏è Legal\n",
    "- ü™∑ Culture (Thirukkural, Siddhars, Classical literature)\n",
    "- üó£Ô∏è Dialects (Chennai, Madurai, Kongu)\n",
    "- üö´ Guardrails (\"I don't know\" responses)\n",
    "\n",
    "**Output:** Single 1.63 GB model that knows everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes trl huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Base Model\n",
    "\n",
    "Load the Gemma-2B Tamil model that we verified works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"abhinand/gemma-2b-it-tamil-v0.1-alpha\"\n",
    "\n",
    "print(f\"Loading {MODEL_ID}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Try loading in bf16 first (preferred)\n",
    "# If OOM, we'll fall back to 8-bit (NOT 4-bit - that corrupts!)\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    print(\"Loaded in bfloat16!\")\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"OOM with bf16, trying 8-bit quantization...\")\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,  # 8-bit, NOT 4-bit!\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        print(\"Loaded in 8-bit!\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "print(f\"Parameters: {model.num_parameters() / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Base Model Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, tokenizer, prompt):\n",
    "    \"\"\"Quick test of model output\"\"\"\n",
    "    formatted = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "# Test before fine-tuning\n",
    "print(\"=\" * 50)\n",
    "print(\"BEFORE FINE-TUNING:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_prompts = [\n",
    "    \"‡Æ§‡Æø‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡Æ±‡Æ≥‡Æø‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Øç ‡Æï‡ØÅ‡Æ±‡Æ≥‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\",\n",
    "    \"PM-KISAN ‡Æ§‡Æø‡Æü‡Øç‡Æü‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\",\n",
    "    \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ©‡Øç ‡Æ§‡Æ≤‡Øà‡Æ®‡Æï‡Æ∞‡ÆÆ‡Øç ‡Æé‡Æ§‡ØÅ?\",\n",
    "    \"‡Æá‡Æ®‡Øç‡Æ§ SMS ‡Æâ‡Æ£‡Øç‡ÆÆ‡Øà‡ÆØ‡Ææ? '‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç lottery ‡Æµ‡ØÜ‡Æ©‡Øç‡Æ±‡ØÄ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç, ‚Çπ500 ‡ÆÖ‡Æ©‡ØÅ‡Æ™‡Øç‡Æ™‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç'\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    print(f\"A: {test_model(model, tokenizer, prompt)[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Full VAZHI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load full VAZHI dataset from HuggingFace\n",
    "print(\"Loading VAZHI dataset from HuggingFace...\")\n",
    "dataset = load_dataset(\"CryptoYogi/vazhi-tamil-v05\")\n",
    "\n",
    "print(f\"\\nDataset loaded!\")\n",
    "print(f\"Train: {len(dataset['train'])} samples\")\n",
    "print(f\"Validation: {len(dataset['validation'])} samples\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample:\")\n",
    "sample = dataset['train'][0]\n",
    "print(f\"Keys: {sample.keys()}\")\n",
    "if 'text' in sample:\n",
    "    print(f\"Text: {sample['text'][:300]}...\")\n",
    "elif 'instruction' in sample:\n",
    "    print(f\"Instruction: {sample['instruction'][:100]}...\")\n",
    "    print(f\"Output: {sample['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_training(example):\n",
    "    \"\"\"Format as instruction-response pair\"\"\"\n",
    "    # Handle both formats\n",
    "    if 'text' in example and example['text']:\n",
    "        return {\"text\": example['text']}\n",
    "    elif 'instruction' in example and 'output' in example:\n",
    "        text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n",
    "        return {\"text\": text}\n",
    "    else:\n",
    "        return {\"text\": \"\"}\n",
    "\n",
    "# Format datasets\n",
    "train_dataset = dataset['train'].map(format_for_training)\n",
    "val_dataset = dataset['validation'].map(format_for_training)\n",
    "\n",
    "# Filter empty\n",
    "train_dataset = train_dataset.filter(lambda x: len(x['text']) > 10)\n",
    "val_dataset = val_dataset.filter(lambda x: len(x['text']) > 10)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure LoRA (Conservative)\n",
    "\n",
    "Using conservative settings to avoid corrupting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare for training if using quantization\n",
    "if hasattr(model, 'is_loaded_in_8bit') and model.is_loaded_in_8bit:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Conservative LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                    # Moderate rank\n",
    "    lora_alpha=16,          # Standard alpha = 2*r\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Only attention\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Calculate steps\n",
    "batch_size = 2\n",
    "grad_accum = 8\n",
    "effective_batch = batch_size * grad_accum  # 16\n",
    "steps_per_epoch = len(train_dataset) // effective_batch\n",
    "total_steps = steps_per_epoch * 2  # 2 epochs\n",
    "\n",
    "print(f\"Effective batch size: {effective_batch}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Total steps (2 epochs): {total_steps}\")\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./vazhi-gemma-full\",\n",
    "    \n",
    "    # Learning settings\n",
    "    learning_rate=1e-5,         # Conservative\n",
    "    num_train_epochs=2,         # 2 epochs\n",
    "    \n",
    "    # Batch settings\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    \n",
    "    # Stability\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    \n",
    "    # Logging & saving\n",
    "    logging_steps=25,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Data\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"\\nTrainer configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train!\n",
    "\n",
    "This will take ~2-3 hours on T4 with full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Expected steps: {total_steps}\")\n",
    "print(\"\\nWatch for:\")\n",
    "print(\"- Loss should decrease gradually\")\n",
    "print(\"- No sudden spikes (indicates divergence)\")\n",
    "print(\"- Target final loss: ~1.5-2.5\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "trainer.save_model(\"./vazhi-gemma-full-final\")\n",
    "print(\"Training complete! Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"AFTER FINE-TUNING:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Core tests\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    print(f\"A: {test_model(model, tokenizer, prompt)[:300]}\")\n",
    "\n",
    "# Domain-specific tests\n",
    "domain_tests = [\n",
    "    # Culture\n",
    "    \"‡Æµ‡Æ≥‡Øç‡Æ≥‡ØÅ‡Æµ‡Æ∞‡Øç ‡ÆØ‡Ææ‡Æ∞‡Øç?\",\n",
    "    # Scam\n",
    "    \"OTP share ‡Æ™‡Æ£‡Øç‡Æ£‡Æ≤‡Ææ‡ÆÆ‡Ææ?\",\n",
    "    # Health  \n",
    "    \"CMCHIS ‡Æ§‡Æø‡Æü‡Øç‡Æü‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\",\n",
    "    # Guardrails\n",
    "    \"Bitcoin-‡Æ≤‡Øç invest ‡Æ™‡Æ£‡Øç‡Æ£‡Æ≤‡Ææ‡ÆÆ‡Ææ?\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DOMAIN-SPECIFIC TESTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in domain_tests:\n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    print(f\"A: {test_model(model, tokenizer, prompt)[:300]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Merge LoRA & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA into base model\n",
    "print(\"Merging LoRA into base model...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "merged_model.save_pretrained(\"./vazhi-gemma-merged\")\n",
    "tokenizer.save_pretrained(\"./vazhi-gemma-merged\")\n",
    "\n",
    "print(\"Merged model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Convert to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and build llama.cpp\n",
    "!git clone --depth 1 https://github.com/ggerganov/llama.cpp.git 2>/dev/null || echo \"Already exists\"\n",
    "!cd llama.cpp && mkdir -p build && cd build && cmake .. -DGGML_CUDA=OFF && cmake --build . --config Release -j4 -- llama-quantize\n",
    "\n",
    "print(\"llama.cpp ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Convert to GGUF F16\n",
    "print(\"Converting to GGUF F16...\")\n",
    "result = subprocess.run([\n",
    "    \"python\", \"llama.cpp/convert_hf_to_gguf.py\",\n",
    "    \"./vazhi-gemma-merged\",\n",
    "    \"--outfile\", \"./vazhi-gemma-f16.gguf\",\n",
    "    \"--outtype\", \"f16\"\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "if os.path.exists(\"./vazhi-gemma-f16.gguf\"):\n",
    "    size = os.path.getsize(\"./vazhi-gemma-f16.gguf\") / 1e9\n",
    "    print(f\"‚úÖ F16 created: {size:.2f} GB\")\n",
    "else:\n",
    "    print(f\"‚ùå F16 failed: {result.stderr[-500:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize to Q4_K_M\n",
    "print(\"Quantizing to Q4_K_M...\")\n",
    "result = subprocess.run([\n",
    "    \"./llama.cpp/build/bin/llama-quantize\",\n",
    "    \"./vazhi-gemma-f16.gguf\",\n",
    "    \"./vazhi-gemma-q4_k_m.gguf\",\n",
    "    \"Q4_K_M\"\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "if os.path.exists(\"./vazhi-gemma-q4_k_m.gguf\"):\n",
    "    size = os.path.getsize(\"./vazhi-gemma-q4_k_m.gguf\") / 1e9\n",
    "    print(f\"‚úÖ Q4_K_M created: {size:.2f} GB\")\n",
    "else:\n",
    "    print(f\"‚ùå Q4_K_M failed: {result.stderr[-500:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Final GGUF Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "print(\"Loading GGUF model...\")\n",
    "llm = Llama(\n",
    "    model_path=\"./vazhi-gemma-q4_k_m.gguf\",\n",
    "    n_ctx=512,\n",
    "    n_threads=4,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL GGUF Q4_K_M VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_tests = test_prompts + domain_tests\n",
    "\n",
    "results = []\n",
    "for prompt in all_tests:\n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    response = llm(\n",
    "        f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\",\n",
    "        max_tokens=150,\n",
    "        stop=[\"###\", \"\\n\\n\"],\n",
    "        echo=False\n",
    "    )\n",
    "    answer = response['choices'][0]['text'].strip()\n",
    "    print(f\"A: {answer[:300]}\")\n",
    "    results.append({\"q\": prompt, \"a\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality check\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"QUALITY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def check_tamil(text):\n",
    "    \"\"\"Count Tamil characters\"\"\"\n",
    "    tamil = sum(1 for c in text if 0x0B80 <= ord(c) <= 0x0BFF)\n",
    "    return tamil\n",
    "\n",
    "for r in results:\n",
    "    tamil_chars = check_tamil(r['a'])\n",
    "    status = \"‚úÖ\" if tamil_chars > 10 else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status} Tamil chars: {tamil_chars:3d} | {r['q'][:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Upload to HuggingFace (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to upload\n",
    "# from huggingface_hub import HfApi, login\n",
    "# login()  # Enter your HF token\n",
    "# \n",
    "# api = HfApi()\n",
    "# api.upload_file(\n",
    "#     path_or_fileobj=\"./vazhi-gemma-q4_k_m.gguf\",\n",
    "#     path_in_repo=\"vazhi-gemma-q4_k_m.gguf\",\n",
    "#     repo_id=\"CryptoYogi/vazhi-model-v1\",\n",
    "#     repo_type=\"model\",\n",
    "# )\n",
    "# print(\"Uploaded to HuggingFace!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "### Training\n",
    "- Dataset: 11,112 samples (full VAZHI)\n",
    "- Base: Gemma-2B Tamil\n",
    "- Method: LoRA (r=8, bf16)\n",
    "- Duration: ~2-3 hours on T4\n",
    "\n",
    "### Output\n",
    "- Model: `vazhi-gemma-q4_k_m.gguf`\n",
    "- Size: ~1.63 GB\n",
    "- Covers: All 6 knowledge packs + guardrails\n",
    "\n",
    "### Quality Checklist\n",
    "- [ ] Produces coherent Tamil\n",
    "- [ ] Thirukkural correct\n",
    "- [ ] Chennai capital correct\n",
    "- [ ] Scam detection works\n",
    "- [ ] Govt schemes accurate\n",
    "- [ ] Guardrails work (refuses unknown)\n",
    "\n",
    "If all pass ‚Üí Ready for mobile integration! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
