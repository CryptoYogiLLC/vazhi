{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI SFT v3.5 â€” Completion-Only Masking\n",
    "\n",
    "**Critical fix over v3.4:** Train ONLY on assistant response tokens, not the full ChatML transcript.\n",
    "\n",
    "### What failed in v3.1â€“v3.4\n",
    "| Version | Root Cause |\n",
    "|---------|------------|\n",
    "| v3.1 | Mixed raw text + ChatML â†’ \"systemsystem...\" garbage |\n",
    "| v3.2 | ChatML-only fix, but fp16 issues on T4 |\n",
    "| v3.3 | Instruct model's `<think>` tokens conflicted with ChatML; LR 1e-4 too aggressive |\n",
    "| v3.4 | Never run â€” used base model but still trained on FULL transcript (system+user+assistant) |\n",
    "\n",
    "### v3.5 Fixes (this notebook)\n",
    "1. **Completion-only masking** â€” `DataCollatorForCompletionOnlyLM` masks system/user tokens with -100, loss computed ONLY on assistant response\n",
    "2. **Base model** â€” Qwen3-0.6B-Base (no `<think>` conflict)\n",
    "3. **LR 2e-5** â€” safe for fine-tuning\n",
    "4. **Preflight verification** â€” confirms masking works before training starts\n",
    "5. **Comprehensive eval suite** â€” 12 diverse prompts tested post-training\n",
    "\n",
    "**Target:** Kaggle P100 (16GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "**After running this cell, RESTART the session** (Runtime â†’ Restart session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q -U \\\n  \"transformers>=4.51.0\" \\\n  \"accelerate>=0.34.2\" \\\n  \"peft>=0.12.0\" \\\n  \"trl>=0.12.0,<0.20.0\" \\\n  \"bitsandbytes>=0.43.3\" \\\n  \"datasets>=2.21.0\" \\\n  \"huggingface_hub>=0.24.7\"\n\nprint(\"âœ… Dependencies installed\")\nprint(\"âš ï¸  RESTART THE SESSION NOW (Runtime â†’ Restart session)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force single GPU BEFORE importing torch â€” prevents cuda:0/cuda:1 device mismatch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# Seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Repos\n",
    "BALANCED_DATASET = \"CryptoYogi/vazhi-tamil-sft-v3_3\"  # Reuse balanced dataset\n",
    "BASE_MODEL = \"Qwen/Qwen3-0.6B-Base\"                  # BASE model â€” no <think> conflict\n",
    "OUTPUT_MODEL = \"CryptoYogi/vazhi-qwen3-v3_5\"\n",
    "\n",
    "# System prompt\n",
    "SYSTEM_PROMPT = (\n",
    "    \"à®¨à¯€à®™à¯à®•à®³à¯ VAZHI (à®µà®´à®¿), à®¤à®®à®¿à®´à¯ à®®à®•à¯à®•à®³à¯à®•à¯à®•à®¾à®© AI à®‰à®¤à®µà®¿à®¯à®¾à®³à®°à¯. \"\n",
    "    \"à®¤à®®à®¿à®´à®¿à®²à¯ à®¤à¯†à®³à®¿à®µà®¾à®•à®µà¯à®®à¯ à®‰à®¤à®µà®¿à®¯à®¾à®•à®µà¯à®®à¯ à®ªà®¤à®¿à®²à®³à®¿à®¯à¯à®™à¯à®•à®³à¯. \"\n",
    "    'à®¤à¯†à®°à®¿à®¯à®¾à®µà®¿à®Ÿà¯à®Ÿà®¾à®²à¯ \"à®¤à¯†à®°à®¿à®¯à®µà®¿à®²à¯à®²à¯ˆ\" à®à®©à¯à®±à¯ à®šà¯Šà®²à¯à®²à¯à®™à¯à®•à®³à¯.'\n",
    ")\n",
    "\n",
    "print(f\"âœ… Configuration loaded\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"\\nğŸ”‘ KEY CHANGES in v3.5:\")\n",
    "print(f\"   1. Completion-only masking (DataCollatorForCompletionOnlyLM)\")\n",
    "print(f\"   2. Base model: {BASE_MODEL}\")\n",
    "print(f\"   3. LR: 2e-5, LoRA r=32, 3 epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "print(\"âœ… Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset\n",
    "\n",
    "Reusing the balanced, ChatML-only dataset from v3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ğŸ“š Loading balanced dataset from {BALANCED_DATASET}...\")\n",
    "balanced_ds = load_dataset(BALANCED_DATASET, split=\"train\")\n",
    "print(f\"âœ… Loaded {len(balanced_ds)} samples\")\n",
    "\n",
    "# Verify 100% ChatML format\n",
    "chatml_count = sum(1 for s in balanced_ds if \"<|im_start|>\" in s[\"text\"] and \"<|im_end|>\" in s[\"text\"])\n",
    "assistant_count = sum(1 for s in balanced_ds if \"<|im_start|>assistant\" in s[\"text\"])\n",
    "print(f\"   ChatML formatted: {chatml_count}/{len(balanced_ds)} ({chatml_count/len(balanced_ds)*100:.1f}%)\")\n",
    "print(f\"   Has assistant tag: {assistant_count}/{len(balanced_ds)} ({assistant_count/len(balanced_ds)*100:.1f}%)\")\n",
    "\n",
    "if assistant_count < len(balanced_ds):\n",
    "    print(f\"\\nâš ï¸  WARNING: {len(balanced_ds) - assistant_count} samples missing assistant tag!\")\n",
    "    print(\"   These samples will have ALL tokens masked (zero loss). Filtering them out...\")\n",
    "    balanced_ds = balanced_ds.filter(lambda s: \"<|im_start|>assistant\" in s[\"text\"])\n",
    "    print(f\"   After filtering: {len(balanced_ds)} samples\")\n",
    "else:\n",
    "    print(\"âœ… All samples have assistant tag â€” masking will work correctly\")\n",
    "\n",
    "# Show a sample\n",
    "print(f\"\\nğŸ“ Sample (first 300 chars):\")\n",
    "print(balanced_ds[0][\"text\"][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Base Model + Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ğŸ“¥ Loading tokenizer from {BASE_MODEL}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Add ChatML special tokens (base model doesn't have them)\n",
    "special_tokens = [\"<|im_start|>\", \"<|im_end|>\"]\n",
    "tokens_to_add = [t for t in special_tokens if t not in tokenizer.get_vocab()]\n",
    "if tokens_to_add:\n",
    "    print(f\"   Adding special tokens: {tokens_to_add}\")\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": tokens_to_add})\n",
    "else:\n",
    "    print(\"   ChatML tokens already in vocab\")\n",
    "\n",
    "# Set pad token if not set â€” safe for Qwen (unlike Gemma where this caused vocab holes)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"   Set pad_token = eos_token (ID {tokenizer.pad_token_id})\")\n",
    "\n",
    "print(f\"âœ… Tokenizer ready: {len(tokenizer)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“¥ Loading model {BASE_MODEL}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\":0},\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Resize embeddings if we added special tokens\n",
    "if len(tokenizer) > model.config.vocab_size:\n",
    "    print(f\"   Resizing embeddings: {model.config.vocab_size} â†’ {len(tokenizer)}\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.use_cache = False  # Required for gradient checkpointing\n",
    "\n",
    "print(f\"âœ… Model loaded: {model.num_parameters():,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Convert any bf16 params to fp16 (safety check for P100)\n",
    "bf16_count = sum(1 for _, p in model.named_parameters() if p.dtype == torch.bfloat16)\n",
    "if bf16_count > 0:\n",
    "    print(f\"âš ï¸  Converting {bf16_count} bf16 parameters to fp16\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dtype == torch.bfloat16:\n",
    "            param.data = param.data.to(torch.float16)\n",
    "else:\n",
    "    print(\"âœ… No bf16 parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Completion-Only Masking\n",
    "\n",
    "**This is the critical fix.** Without masking, the model trains on the entire ChatML transcript (system + user + assistant). With `DataCollatorForCompletionOnlyLM`, only assistant response tokens contribute to the loss.\n",
    "\n",
    "```\n",
    "<|im_start|>system\\n...       â†’ masked (-100)\n",
    "<|im_start|>user\\n...         â†’ masked (-100)\n",
    "<|im_start|>assistant\\n       â†’ masked (-100)\n",
    "actual response here<|im_end|> â†’ âœ… TRAINED\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the response template to get consistent token IDs\n",
    "response_template_str = \"<|im_start|>assistant\\n\"\n",
    "response_template_ids = tokenizer.encode(response_template_str, add_special_tokens=False)\n",
    "\n",
    "print(f\"Response template: {response_template_str!r}\")\n",
    "print(f\"Token IDs: {response_template_ids}\")\n",
    "print(f\"Decoded back: {tokenizer.decode(response_template_ids)!r}\")\n",
    "\n",
    "# Verify the template can be found in actual data\n",
    "sample_text = balanced_ds[0][\"text\"]\n",
    "sample_ids = tokenizer.encode(sample_text, add_special_tokens=False)\n",
    "template_found = False\n",
    "for i in range(len(sample_ids) - len(response_template_ids) + 1):\n",
    "    if sample_ids[i:i+len(response_template_ids)] == response_template_ids:\n",
    "        template_found = True\n",
    "        print(f\"âœ… Response template found at token position {i} in sample\")\n",
    "        break\n",
    "\n",
    "if not template_found:\n",
    "    print(\"âŒ STOP: Response template NOT found in tokenized sample!\")\n",
    "    print(\"   The collator will mask ALL tokens â†’ zero training signal.\")\n",
    "    print(\"   Debug: check tokenization of the template vs the full text.\")\n",
    "    # Show token-by-token for debugging\n",
    "    print(f\"\\n   Template tokens: {response_template_ids}\")\n",
    "    print(f\"   Sample tokens around 'assistant': \", end=\"\")\n",
    "    text_lower = sample_text.lower()\n",
    "    if \"assistant\" in text_lower:\n",
    "        idx = text_lower.index(\"assistant\")\n",
    "        context = sample_text[max(0,idx-20):idx+30]\n",
    "        context_ids = tokenizer.encode(context, add_special_tokens=False)\n",
    "        print(context_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the completion-only collator\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template_ids,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# === PREFLIGHT: Verify masking on a real sample ===\n",
    "sample_text = balanced_ds[0][\"text\"]\n",
    "tokenized = tokenizer(sample_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "mock_batch = [{\"input_ids\": tokenized[\"input_ids\"][0], \"attention_mask\": tokenized[\"attention_mask\"][0]}]\n",
    "processed = collator(mock_batch)\n",
    "\n",
    "labels = processed[\"labels\"][0]\n",
    "masked_count = (labels == -100).sum().item()\n",
    "total_count = len(labels)\n",
    "trainable_count = total_count - masked_count\n",
    "\n",
    "print(f\"\\nğŸ“Š Masking verification on sample 0:\")\n",
    "print(f\"   Total tokens:              {total_count}\")\n",
    "print(f\"   Masked (system+user+tags):  {masked_count} ({masked_count/total_count*100:.1f}%)\")\n",
    "print(f\"   Trainable (assistant only): {trainable_count} ({trainable_count/total_count*100:.1f}%)\")\n",
    "\n",
    "if trainable_count == 0:\n",
    "    print(\"\\nâŒ STOP: No trainable tokens! Response template not found in tokenized data.\")\n",
    "    print(\"   Do NOT proceed with training â€” it will produce a broken model.\")\n",
    "elif trainable_count == total_count:\n",
    "    print(\"\\nâŒ STOP: ALL tokens trainable â€” masking is NOT working!\")\n",
    "    print(\"   The model would learn to predict system/user scaffolding.\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Masking working correctly\")\n",
    "    # Show what the trainable tokens decode to\n",
    "    trainable_ids = labels[labels != -100].tolist()\n",
    "    trainable_text = tokenizer.decode(trainable_ids)\n",
    "    print(f\"   Trainable text: {trainable_text[:200]}\")\n",
    "\n",
    "# Spot-check 5 more samples\n",
    "print(f\"\\nğŸ“Š Spot-checking 5 more samples...\")\n",
    "fail_count = 0\n",
    "for idx in [10, 50, 100, 500, len(balanced_ds)-1]:\n",
    "    if idx >= len(balanced_ds):\n",
    "        continue\n",
    "    t = tokenizer(balanced_ds[idx][\"text\"], return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    b = collator([{\"input_ids\": t[\"input_ids\"][0], \"attention_mask\": t[\"attention_mask\"][0]}])\n",
    "    n_train = (b[\"labels\"][0] != -100).sum().item()\n",
    "    n_total = len(b[\"labels\"][0])\n",
    "    status = \"âœ…\" if 0 < n_train < n_total else \"âŒ\"\n",
    "    if n_train == 0 or n_train == n_total:\n",
    "        fail_count += 1\n",
    "    print(f\"   Sample {idx}: {n_train}/{n_total} trainable {status}\")\n",
    "\n",
    "if fail_count > 0:\n",
    "    print(f\"\\nâš ï¸  {fail_count} samples have masking issues â€” investigate before training\")\n",
    "else:\n",
    "    print(f\"\\nâœ… All spot-checks passed â€” safe to proceed with training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training\n",
    "\n",
    "Checkpoints saved every 100 steps. If you want to check output quality early:\n",
    "1. Wait for step 100 checkpoint\n",
    "2. Interrupt the kernel\n",
    "3. Skip to the **Test** section below\n",
    "4. If outputs look reasonable, re-run this cell to continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=\"/kaggle/working/vazhi-v3_5\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=25,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    fp16=False,       # Disabled â€” Qwen3 has internal bf16 ops\n",
    "    bf16=False,       # Disabled â€” P100 doesn't support bf16\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=balanced_ds,\n",
    "    args=sft_config,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=collator,  # â† CRITICAL: completion-only masking\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer initialized\")\n",
    "print(f\"   Epochs: 3\")\n",
    "print(f\"   Learning rate: 2e-5\")\n",
    "print(f\"   LoRA rank: 32\")\n",
    "print(f\"   Batch size: 1 Ã— 16 = 16 effective\")\n",
    "print(f\"   Mode: FP32 (P100 safe)\")\n",
    "print(f\"   Save checkpoints: every 100 steps\")\n",
    "print(f\"   ğŸ”‘ Data collator: DataCollatorForCompletionOnlyLM (assistant tokens only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸš€ Starting training...\")\n",
    "trainer.train()\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save & Push to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ’¾ Saving model...\")\n",
    "trainer.save_model(\"/kaggle/working/vazhi-v3_5-final\")\n",
    "\n",
    "print(\"ğŸ”€ Merging LoRA weights...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Push to HuggingFace\n",
    "api = HfApi()\n",
    "api.create_repo(OUTPUT_MODEL, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“¤ Pushing to {OUTPUT_MODEL}...\")\n",
    "merged_model.push_to_hub(OUTPUT_MODEL, private=False)\n",
    "tokenizer.push_to_hub(OUTPUT_MODEL, private=False)\n",
    "\n",
    "print(f\"\\nâœ… Model uploaded: https://huggingface.co/{OUTPUT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comprehensive Evaluation\n",
    "\n",
    "12 prompts across categories: greetings, factual, culture, safety, practical, \"don't know\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.config.use_cache = True\n",
    "\n",
    "test_prompts = [\n",
    "    # Greetings (2)\n",
    "    (\"greeting\", \"à®µà®£à®•à¯à®•à®®à¯\"),\n",
    "    (\"greeting\", \"à®¨à¯€à®™à¯à®•à®³à¯ à®¯à®¾à®°à¯?\"),\n",
    "    # Factual â€” non-kural (3)\n",
    "    (\"factual\", \"à®¤à®®à®¿à®´à¯à®¨à®¾à®Ÿà¯à®Ÿà®¿à®©à¯ à®¤à®²à¯ˆà®¨à®•à®°à®®à¯ à®à®©à¯à®©?\"),\n",
    "    (\"factual\", \"2+2 à®à®©à¯à®©?\"),\n",
    "    (\"factual\", \"à®ªà¯Šà®™à¯à®•à®²à¯ à®à®ªà¯à®ªà¯‹à®¤à¯ à®•à¯Šà®£à¯à®Ÿà®¾à®Ÿà®ªà¯à®ªà®Ÿà¯à®•à®¿à®±à®¤à¯?\"),\n",
    "    # Culture â€” Thirukkural (2)\n",
    "    (\"culture\", \"à®¤à®¿à®°à¯à®•à¯à®•à¯à®±à®³à®¿à®©à¯ à®®à¯à®¤à®²à¯ à®•à¯à®±à®³à¯ à®à®©à¯à®©?\"),\n",
    "    (\"culture\", \"à®¤à®¿à®°à¯à®µà®³à¯à®³à¯à®µà®°à¯ à®¯à®¾à®°à¯?\"),\n",
    "    # Safety/practical (2)\n",
    "    (\"safety\", \"à®’à®°à¯ scam message à®µà®¨à¯à®¤à®¾à®²à¯ à®à®©à¯à®© à®šà¯†à®¯à¯à®µà®¤à¯?\"),\n",
    "    (\"safety\", \"à®µà¯€à®Ÿà¯à®Ÿà®¿à®²à¯ à®¤à¯€ à®µà®¿à®ªà®¤à¯à®¤à¯ à®à®±à¯à®ªà®Ÿà¯à®Ÿà®¾à®²à¯ à®à®©à¯à®© à®šà¯†à®¯à¯à®¯ à®µà¯‡à®£à¯à®Ÿà¯à®®à¯?\"),\n",
    "    # Don't know (2)\n",
    "    (\"unknown\", \"à®¨à®¾à®³à¯ˆ à®ªà®™à¯à®•à¯ à®šà®¨à¯à®¤à¯ˆ à®à®±à¯à®®à®¾?\"),\n",
    "    (\"unknown\", \"à®à®©à¯ à®•à®£à®¿à®©à®¿à®¯à®¿à®²à¯ à®µà¯ˆà®°à®¸à¯ à®‡à®°à¯à®•à¯à®•à®¿à®±à®¤à®¾?\"),\n",
    "    # General knowledge (1)\n",
    "    (\"general\", \"à®¤à®®à®¿à®´à¯ à®®à¯Šà®´à®¿à®¯à®¿à®©à¯ à®šà®¿à®±à®ªà¯à®ªà¯ à®à®©à¯à®©?\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ§ª EVALUATION: {len(test_prompts)} prompts\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for category, prompt_text in test_prompts:\n",
    "    full_prompt = (\n",
    "        f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{prompt_text}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(merged_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = merged_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=4,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    if \"<|im_start|>assistant\" in response:\n",
    "        response = response.split(\"<|im_start|>assistant\")[-1]\n",
    "        response = response.split(\"<|im_end|>\")[0].strip()\n",
    "        if response.startswith(\"\\n\"):\n",
    "            response = response[1:]\n",
    "    \n",
    "    # Check for failure patterns\n",
    "    has_loop = len(set(response.split())) < max(3, len(response.split()) * 0.3) if response.split() else True\n",
    "    has_system = \"system\" in response.lower()[:50]\n",
    "    has_think = \"<think>\" in response\n",
    "    is_empty = len(response.strip()) < 5\n",
    "    \n",
    "    status = \"âœ…\"\n",
    "    if has_loop: status = \"âš ï¸ LOOP\"\n",
    "    if has_system: status = \"âŒ SYSTEM LEAK\"\n",
    "    if has_think: status = \"âŒ THINK LEAK\"\n",
    "    if is_empty: status = \"âŒ EMPTY\"\n",
    "    \n",
    "    results.append((category, prompt_text, response[:200], status))\n",
    "    \n",
    "    print(f\"\\n[{category.upper()}] {status}\")\n",
    "    print(f\"Q: {prompt_text}\")\n",
    "    print(f\"A: {response[:300]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ“Š EVALUATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "pass_count = sum(1 for r in results if r[3] == \"âœ…\")\n",
    "print(f\"   Passed: {pass_count}/{len(results)}\")\n",
    "for cat, prompt, resp, status in results:\n",
    "    print(f\"   {status} [{cat}] {prompt[:40]}\")\n",
    "\n",
    "if pass_count == len(results):\n",
    "    print(f\"\\nğŸ‰ All tests passed! Model looks good.\")\n",
    "elif pass_count >= len(results) * 0.7:\n",
    "    print(f\"\\nâš ï¸  Mostly passing but some issues. Review failures above.\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Too many failures. Model needs investigation.\")\n",
    "    print(f\"   Consider: dataset quality, more epochs, or two-stage DAPT+SFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### v3.5 vs v3.4 Changes\n",
    "\n",
    "| Setting | v3.4 (never run) | v3.5 (this notebook) |\n",
    "|---------|------------------|----------------------|\n",
    "| **Data collator** | **None (full transcript)** | **DataCollatorForCompletionOnlyLM** |\n",
    "| Loss computed on | System + User + Assistant | **Assistant only** |\n",
    "| Base Model | Qwen3-0.6B-Base | Qwen3-0.6B-Base (same) |\n",
    "| Learning Rate | 2e-5 | 2e-5 (same) |\n",
    "| Epochs | 3 | 3 (same) |\n",
    "| LoRA Rank | 32 | 32 (same) |\n",
    "| Save steps | 200 | **100** (earlier checkpoints) |\n",
    "| Masking verification | None | **Preflight check** |\n",
    "| Eval suite | 5 prompts | **12 prompts, 6 categories** |\n",
    "| Failure detection | Manual | **Automated (loop/leak/empty checks)** |\n",
    "\n",
    "### If this fails, next steps:\n",
    "1. **Two-stage DAPT+SFT** â€” Micro-DAPT on raw Tamil text first, then SFT with masking\n",
    "2. **Reduce LoRA r to 16** â€” if r=32 causes instability with 0.6B model\n",
    "3. **Sarvam-1 IQ3_M** â€” proven Tamil, 1.17GB (exceeds <1GB target but works)\n",
    "4. **Gemma-2B Tamil Q4_K_M** â€” 1.63GB, works but far exceeds size target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
