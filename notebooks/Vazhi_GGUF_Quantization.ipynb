{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI GGUF Quantization\n",
    "\n",
    "**Goal**: Create a ~1.7GB quantized model for offline mobile inference\n",
    "\n",
    "**Steps**:\n",
    "1. Merge LoRA adapter with Qwen 2.5 3B base model\n",
    "2. Convert merged model to GGUF format\n",
    "3. Quantize to Q4_K_M\n",
    "4. Test the quantized model\n",
    "\n",
    "**Requirements**: Colab with ~12GB RAM (free tier should work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers peft accelerate huggingface_hub sentencepiece\n",
    "\n",
    "# Check available RAM\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace (needed to download models)\n",
    "from huggingface_hub import login\n",
    "login()  # Enter your HF token when prompted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Merge LoRA with Base Model\n",
    "\n",
    "We'll load the base model and LoRA adapter, merge them, and save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import gc\n",
    "\n",
    "# Model configuration\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "LORA_ADAPTER = \"CryptoYogi/vazhi-lora\"\n",
    "MERGED_OUTPUT = \"./vazhi-merged\"\n",
    "\n",
    "print(f\"Base model: {BASE_MODEL}\")\n",
    "print(f\"LoRA adapter: {LORA_ADAPTER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer first (small memory footprint)\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model in float16 to save memory\n",
    "print(\"Loading base model in float16...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",  # Keep on CPU to save GPU memory\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "print(f\"Base model loaded. Parameters: {base_model.num_parameters():,}\")\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and merge LoRA adapter\n",
    "print(\"Loading LoRA adapter...\")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    LORA_ADAPTER,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(\"LoRA adapter loaded. Merging...\")\n",
    "\n",
    "# Merge LoRA weights into base model\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "print(\"Merge complete!\")\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged model\n",
    "print(f\"Saving merged model to {MERGED_OUTPUT}...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "model.save_pretrained(MERGED_OUTPUT, safe_serialization=True)\n",
    "tokenizer.save_pretrained(MERGED_OUTPUT)\n",
    "\n",
    "print(\"Merged model saved!\")\n",
    "!ls -lh {MERGED_OUTPUT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before next step\n",
    "print(\"Clearing memory...\")\n",
    "del model\n",
    "del base_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(\"Memory cleared.\")\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Quick Test of Merged Model\n",
    "\n",
    "Before converting to GGUF, let's verify the merged model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of merged model\n",
    "print(\"Loading merged model for quick test...\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "test_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MERGED_OUTPUT,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(MERGED_OUTPUT)\n",
    "\n",
    "print(\"Model loaded for testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Thirukkural question\n",
    "test_prompt = \"\"\"<|im_start|>system\n",
    "நீங்கள் VAZHI (வழி), தமிழ் மக்களுக்கான AI உதவியாளர்.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "திருக்குறளின் முதல் குறள் என்ன?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "inputs = test_tokenizer(test_prompt, return_tensors=\"pt\").to(test_model.device)\n",
    "\n",
    "print(\"Generating response...\")\n",
    "with torch.no_grad():\n",
    "    outputs = test_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=test_tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "response = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MERGED MODEL TEST\")\n",
    "print(\"=\"*50)\n",
    "print(response.split(\"assistant\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear test model\n",
    "del test_model\n",
    "del test_tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "print(\"Test model cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install llama.cpp and Convert to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and build llama.cpp\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "%cd llama.cpp\n",
    "!make -j4\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llama.cpp Python dependencies\n",
    "!pip install -q -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert merged model to GGUF format (F16)\n",
    "print(\"Converting to GGUF format...\")\n",
    "print(\"This may take 5-10 minutes...\")\n",
    "\n",
    "!python llama.cpp/convert_hf_to_gguf.py \\\n",
    "    {MERGED_OUTPUT} \\\n",
    "    --outfile vazhi-f16.gguf \\\n",
    "    --outtype f16\n",
    "\n",
    "print(\"\\nConversion complete!\")\n",
    "!ls -lh vazhi-f16.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Quantize to Q4_K_M\n",
    "\n",
    "Q4_K_M provides good balance of quality and size (~1.7GB for 3B model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize to Q4_K_M\n",
    "print(\"Quantizing to Q4_K_M...\")\n",
    "print(\"This provides best quality/size balance for mobile...\")\n",
    "\n",
    "!./llama.cpp/llama-quantize \\\n",
    "    vazhi-f16.gguf \\\n",
    "    vazhi-q4_k_m.gguf \\\n",
    "    q4_k_m\n",
    "\n",
    "print(\"\\nQuantization complete!\")\n",
    "!ls -lh vazhi-*.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create smaller Q4_0 version for very low-end devices\n",
    "print(\"Creating Q4_0 version (smaller but lower quality)...\")\n",
    "\n",
    "!./llama.cpp/llama-quantize \\\n",
    "    vazhi-f16.gguf \\\n",
    "    vazhi-q4_0.gguf \\\n",
    "    q4_0\n",
    "\n",
    "print(\"\\nAll GGUF files:\")\n",
    "!ls -lh vazhi-*.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Q4_K_M model with llama.cpp CLI\n",
    "print(\"Testing quantized model...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "!./llama.cpp/llama-cli \\\n",
    "    -m vazhi-q4_k_m.gguf \\\n",
    "    -p \"<|im_start|>system\\nநீங்கள் VAZHI, தமிழ் உதவியாளர்.<|im_end|>\\n<|im_start|>user\\nதிருக்குறளின் முதல் குறள் என்ன?<|im_end|>\\n<|im_start|>assistant\\n\" \\\n",
    "    -n 150 \\\n",
    "    --temp 0.7 \\\n",
    "    -ngl 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a scam detection question\n",
    "print(\"\\nTesting scam detection...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "!./llama.cpp/llama-cli \\\n",
    "    -m vazhi-q4_k_m.gguf \\\n",
    "    -p \"<|im_start|>system\\nநீங்கள் VAZHI, தமிழ் உதவியாளர்.<|im_end|>\\n<|im_start|>user\\nஇது மோசடியா: 'நீங்கள் 50 லட்சம் lottery வென்றீர்கள், உங்கள் bank details அனுப்புங்கள்'<|im_end|>\\n<|im_start|>assistant\\n\" \\\n",
    "    -n 150 \\\n",
    "    --temp 0.7 \\\n",
    "    -ngl 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Upload to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload GGUF files to HuggingFace\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "# Create or use existing repo\n",
    "GGUF_REPO = \"CryptoYogi/vazhi-gguf\"\n",
    "\n",
    "try:\n",
    "    create_repo(GGUF_REPO, repo_type=\"model\", exist_ok=True)\n",
    "    print(f\"Repository ready: {GGUF_REPO}\")\n",
    "except Exception as e:\n",
    "    print(f\"Repo exists or error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the quantized models\n",
    "api = HfApi()\n",
    "\n",
    "print(\"Uploading Q4_K_M model (recommended)...\")\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"vazhi-q4_k_m.gguf\",\n",
    "    path_in_repo=\"vazhi-q4_k_m.gguf\",\n",
    "    repo_id=GGUF_REPO,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "print(\"Q4_K_M uploaded!\")\n",
    "\n",
    "print(\"\\nUploading Q4_0 model (smaller)...\")\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"vazhi-q4_0.gguf\",\n",
    "    path_in_repo=\"vazhi-q4_0.gguf\",\n",
    "    repo_id=GGUF_REPO,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "print(\"Q4_0 uploaded!\")\n",
    "\n",
    "print(f\"\\nModels available at: https://huggingface.co/{GGUF_REPO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create README for the GGUF repo\n",
    "readme_content = \"\"\"---\n",
    "license: apache-2.0\n",
    "language:\n",
    "- ta\n",
    "- en\n",
    "base_model: Qwen/Qwen2.5-3B-Instruct\n",
    "tags:\n",
    "- tamil\n",
    "- gguf\n",
    "- llama.cpp\n",
    "- mobile\n",
    "- offline\n",
    "---\n",
    "\n",
    "# VAZHI GGUF - Tamil AI Assistant (Quantized)\n",
    "\n",
    "Quantized versions of VAZHI for offline mobile inference.\n",
    "\n",
    "## Models\n",
    "\n",
    "| File | Size | Quality | Use Case |\n",
    "|------|------|---------|----------|\n",
    "| vazhi-q4_k_m.gguf | ~1.7GB | Best | Recommended for most devices |\n",
    "| vazhi-q4_0.gguf | ~1.5GB | Good | Low-memory devices |\n",
    "\n",
    "## Usage with llama.cpp\n",
    "\n",
    "```bash\n",
    "./llama-cli -m vazhi-q4_k_m.gguf \\\n",
    "    -p \"<|im_start|>user\\nதிருக்குறளின் முதல் குறள்?<|im_end|>\\n<|im_start|>assistant\\n\" \\\n",
    "    -n 150\n",
    "```\n",
    "\n",
    "## Base Model\n",
    "\n",
    "- Base: Qwen/Qwen2.5-3B-Instruct\n",
    "- Fine-tuned: [CryptoYogi/vazhi-lora](https://huggingface.co/CryptoYogi/vazhi-lora)\n",
    "- Training: 3,007 Tamil Q&A pairs across 6 domains\n",
    "\n",
    "## Domains\n",
    "\n",
    "- Culture (Thirukkural, temples)\n",
    "- Education (scholarships, exams)\n",
    "- Security (scam detection)\n",
    "- Legal (RTI, consumer rights)\n",
    "- Government (schemes)\n",
    "- Healthcare (Siddha medicine)\n",
    "\n",
    "## License\n",
    "\n",
    "Apache 2.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"GGUF_README.md\", \"w\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"GGUF_README.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=GGUF_REPO,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "print(\"README uploaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Created Files:\n",
    "- `vazhi-q4_k_m.gguf` - Recommended (~1.7GB)\n",
    "- `vazhi-q4_0.gguf` - Smaller (~1.5GB)\n",
    "\n",
    "### Next Steps:\n",
    "1. Download GGUF from HuggingFace\n",
    "2. Integrate into Flutter app using llama.cpp bindings\n",
    "3. Test on actual mobile devices\n",
    "\n",
    "### Download Command:\n",
    "```bash\n",
    "# Using huggingface-cli\n",
    "huggingface-cli download CryptoYogi/vazhi-gguf vazhi-q4_k_m.gguf\n",
    "\n",
    "# Or direct URL\n",
    "wget https://huggingface.co/CryptoYogi/vazhi-gguf/resolve/main/vazhi-q4_k_m.gguf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"VAZHI GGUF QUANTIZATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nFiles created:\")\n",
    "!ls -lh vazhi-*.gguf\n",
    "print(f\"\\nUploaded to: https://huggingface.co/{GGUF_REPO}\")\n",
    "print(\"\\nReady for mobile integration!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
