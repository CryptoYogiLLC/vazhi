{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# VAZHI SFT v4.0 — Instruction Fine-Tuning on DAPT v1.1\n",
    "\n",
    "**Pipeline Step 3 of 3:** SFT on the DAPT-adapted Tamil instruct model.\n",
    "\n",
    "```\n",
    "Step 1: Data Prep (DONE — Vazhi_DAPT_Data_v1_1.ipynb)\n",
    "Step 2: DAPT Training (DONE — Vazhi_DAPT_v1_1_Tamil.ipynb)\n",
    "  → Produced: CryptoYogi/qwen3-0.6b-tamil-v1_1\n",
    "    PPL 2.6, +55% Tamil vs vanilla, 7/8 eval passed\n",
    "\n",
    "Step 3 (THIS NOTEBOOK): SFT — teach instruction-following in Tamil\n",
    "  → Input:  DAPT'd model + v4.0 ChatML dataset (1,514 samples)\n",
    "  → Output: CryptoYogi/vazhi-v4_0 (final VAZHI model)\n",
    "           CryptoYogi/vazhi-v4_0-lora (adapter backup)\n",
    "```\n",
    "\n",
    "**Pre-SFT validation (GPT5.2):** Perplexity baseline + chat template test before training.\n",
    "\n",
    "**Key decisions:**\n",
    "1. **DAPT'd instruct base** — already fluent in Tamil (PPL 2.6), SFT adds instruction-following\n",
    "2. **fp16** — no 4-bit (0.6B fits in 1.1GB), avoids merge corruption\n",
    "3. **Dual T4 DataParallel** — no device_map for training (lesson from DAPT v1.1)\n",
    "4. **Completion-only masking** — only train on assistant responses\n",
    "5. **`<think>` suppression** — during eval generation only\n",
    "6. **Chat template eval** — `apply_chat_template(enable_thinking=False)` per GPT5.2\n",
    "\n",
    "**Target:** Kaggle T4 x2 | ~255 steps (3 epochs) | Est. < 1 hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "**After running this cell, RESTART the session** (Runtime → Restart session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U \\\n",
    "  \"transformers>=4.45.0,<5.0.0\" \\\n",
    "  \"accelerate>=0.34.2\" \\\n",
    "  \"peft>=0.12.0\" \\\n",
    "  \"trl>=0.12.0,<0.20.0\" \\\n",
    "  \"datasets>=2.21.0\" \\\n",
    "  \"huggingface_hub>=0.24.7\"\n",
    "\n",
    "print(\"\\u2705 Dependencies installed\")\n",
    "print(\"\\u26a0\\ufe0f  RESTART THE SESSION NOW (Runtime \\u2192 Restart session)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    TrainerCallback, TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# === KEY CONFIG ===\n",
    "DAPT_MODEL = \"CryptoYogi/qwen3-0.6b-tamil-v1_1\"   # DAPT'd instruct (Step 2 output)\n",
    "VANILLA_MODEL = \"Qwen/Qwen3-0.6B\"                  # For pre-SFT baseline comparison\n",
    "SFT_DATASET = \"CryptoYogi/vazhi-tamil-sft-v4_0\"    # v4.0 ChatML dataset (ADR-010)\n",
    "DAPT_DATASET = \"CryptoYogi/vazhi-dapt-tamil-v1_1\"  # For perplexity baseline\n",
    "OUTPUT_MODEL = \"CryptoYogi/vazhi-v4_0\"              # Final VAZHI model\n",
    "ADAPTER_REPO = \"CryptoYogi/vazhi-v4_0-lora\"         # Adapter backup\n",
    "\n",
    "# Training config\n",
    "LEARNING_RATE = 2e-5       # Conservative for SFT on DAPT'd model\n",
    "NUM_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "BATCH_SIZE = 2             # Per-device\n",
    "GRADIENT_ACCUMULATION = 4  # 2 x 2 GPUs x 4 = 16 effective batch\n",
    "\n",
    "# Qwen3 instruct <think> tokens to suppress during generation\n",
    "THINK_TOKEN_IDS = [151667, 151668]\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"\\u0ba8\\u0bc0\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd VAZHI (\\u0bb5\\u0bb4\\u0bbf), \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd \\u0bae\\u0b95\\u0bcd\\u0b95\\u0bb3\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bbe\\u0ba9 AI \\u0b89\\u0ba4\\u0bb5\\u0bbf\\u0baf\\u0bbe\\u0bb3\\u0bb0\\u0bcd. \"\n",
    "    \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bbf\\u0bb2\\u0bcd \\u0ba4\\u0bc6\\u0bb3\\u0bbf\\u0bb5\\u0bbe\\u0b95\\u0bb5\\u0bc1\\u0bae\\u0bcd \\u0b89\\u0ba4\\u0bb5\\u0bbf\\u0baf\\u0bbe\\u0b95\\u0bb5\\u0bc1\\u0bae\\u0bcd \\u0baa\\u0ba4\\u0bbf\\u0bb2\\u0bb3\\u0bbf\\u0baf\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd. \"\n",
    "    '\\u0ba4\\u0bc6\\u0bb0\\u0bbf\\u0baf\\u0bbe\\u0bb5\\u0bbf\\u0b9f\\u0bcd\\u0b9f\\u0bbe\\u0bb2\\u0bcd \"\\u0ba4\\u0bc6\\u0bb0\\u0bbf\\u0baf\\u0bb5\\u0bbf\\u0bb2\\u0bcd\\u0bb2\\u0bc8\" \\u0b8e\\u0ba9\\u0bcd\\u0bb1\\u0bc1 \\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd.'\n",
    ")\n",
    "\n",
    "# GPU setup\n",
    "n_gpus = torch.cuda.device_count()\n",
    "\n",
    "print(f\"\\u2705 Configuration loaded\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"   GPUs: {n_gpus}\")\n",
    "for i in range(n_gpus):\n",
    "    name = torch.cuda.get_device_name(i)\n",
    "    mem = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "    print(f\"   GPU {i}: {name} ({mem:.0f} GB)\")\n",
    "\n",
    "effective_batch = BATCH_SIZE * n_gpus * GRADIENT_ACCUMULATION\n",
    "print()\n",
    "print(f\"\\U0001f4cb SFT v4.0 on DAPT v1.1:\")\n",
    "print(f\"   Base:     {DAPT_MODEL} (DAPT'd instruct)\")\n",
    "print(f\"   Dataset:  {SFT_DATASET}\")\n",
    "print(f\"   Output:   {OUTPUT_MODEL}\")\n",
    "print(f\"   LR:       {LEARNING_RATE}\")\n",
    "print(f\"   LoRA:     r={LORA_R}, alpha={LORA_ALPHA}\")\n",
    "print(f\"   Batch:    {BATCH_SIZE} x {n_gpus} GPUs x {GRADIENT_ACCUMULATION} accum = {effective_batch} effective\")\n",
    "print(f\"   Epochs:   {NUM_EPOCHS}\")\n",
    "print(f\"   fp16:     True (no 4-bit)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "print(\"\\u2705 Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Pre-SFT Validation (GPT5.2)\n",
    "\n",
    "Before spending compute on SFT, verify that DAPT actually helped:\n",
    "1. **Perplexity baseline** — DAPT'd model should have lower PPL on Tamil text than vanilla\n",
    "2. **Chat template test** — DAPT'd model should still respond to chat-formatted prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PERPLEXITY BASELINE ===\n",
    "# Compare vanilla instruct vs DAPT'd model on Tamil validation blocks\n",
    "\n",
    "print(\"\\U0001f4ca Pre-SFT Validation: Perplexity Baseline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load Tamil val set from DAPT dataset (pre-tokenized 1024-token blocks)\n",
    "print(f\"\\U0001f4e5 Loading Tamil val set from {DAPT_DATASET}...\")\n",
    "dapt_ds = load_dataset(DAPT_DATASET, split=\"validation\")\n",
    "n_eval = min(100, len(dapt_ds))  # Cap for speed\n",
    "print(f\"   {len(dapt_ds)} val blocks available, using {n_eval}\")\n",
    "\n",
    "def compute_ppl(model, dataset, n_samples, device):\n",
    "    \"\"\"Compute perplexity on pre-tokenized blocks.\"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i in range(n_samples):\n",
    "        input_ids = torch.tensor([dataset[i][\"input_ids\"]], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, labels=input_ids.clone())\n",
    "            losses.append(outputs.loss.item())\n",
    "    avg_loss = np.mean(losses)\n",
    "    return np.exp(min(avg_loss, 20)), avg_loss\n",
    "\n",
    "# 1. Vanilla instruct PPL\n",
    "print(f\"\\n\\U0001f4e5 Loading vanilla {VANILLA_MODEL}...\")\n",
    "vanilla = AutoModelForCausalLM.from_pretrained(\n",
    "    VANILLA_MODEL, torch_dtype=torch.float16, device_map={\"\":0}, trust_remote_code=True,\n",
    ")\n",
    "vanilla_ppl, vanilla_loss = compute_ppl(vanilla, dapt_ds, n_eval, vanilla.device)\n",
    "print(f\"   Vanilla PPL: {vanilla_ppl:.2f} (loss: {vanilla_loss:.4f})\")\n",
    "del vanilla; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# 2. DAPT'd model PPL\n",
    "print(f\"\\n\\U0001f4e5 Loading DAPT'd {DAPT_MODEL}...\")\n",
    "dapt = AutoModelForCausalLM.from_pretrained(\n",
    "    DAPT_MODEL, torch_dtype=torch.float16, device_map={\"\":0}, trust_remote_code=True,\n",
    ")\n",
    "dapt_ppl, dapt_loss = compute_ppl(dapt, dapt_ds, n_eval, dapt.device)\n",
    "print(f\"   DAPT PPL:    {dapt_ppl:.2f} (loss: {dapt_loss:.4f})\")\n",
    "\n",
    "# Compare\n",
    "ppl_improvement = vanilla_ppl - dapt_ppl\n",
    "ppl_pct = 100 * (vanilla_ppl - dapt_ppl) / vanilla_ppl\n",
    "print(f\"\\n\\U0001f4ca PPL Comparison:\")\n",
    "print(f\"   Vanilla: {vanilla_ppl:.2f}\")\n",
    "print(f\"   DAPT:    {dapt_ppl:.2f}\")\n",
    "print(f\"   Change:  {ppl_improvement:+.2f} ({ppl_pct:+.1f}%)\")\n",
    "\n",
    "if dapt_ppl < vanilla_ppl:\n",
    "    print(f\"\\n\\u2705 DAPT improved Tamil perplexity! Safe to proceed with SFT.\")\n",
    "elif dapt_ppl < vanilla_ppl * 1.05:  # Within 5%\n",
    "    print(f\"\\n\\u26a0\\ufe0f  DAPT is neutral (within 5%). Proceeding with caution.\")\n",
    "else:\n",
    "    print(f\"\\n\\u274c DAPT made perplexity WORSE! Investigate before SFT.\")\n",
    "    print(f\"   This would mean DAPT hurt Tamil fluency \\u2014 check training logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CHAT TEMPLATE TEST ===\n",
    "# Verify DAPT'd model still responds to chat-formatted prompts\n",
    "\n",
    "print(\"\\U0001f9ea Pre-SFT Validation: Chat Template Test\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DAPT_MODEL, trust_remote_code=True)\n",
    "\n",
    "# Check if tokenizer supports enable_thinking\n",
    "try:\n",
    "    test = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": \"test\"}],\n",
    "        tokenize=False, add_generation_prompt=True, enable_thinking=False,\n",
    "    )\n",
    "    USE_THINKING_FLAG = True\n",
    "    print(\"\\u2705 Tokenizer supports enable_thinking=False\")\n",
    "except TypeError:\n",
    "    USE_THINKING_FLAG = False\n",
    "    print(\"\\u26a0\\ufe0f  enable_thinking not supported, using manual template\")\n",
    "\n",
    "def build_chat_prompt(user_text):\n",
    "    msgs = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "    if USE_THINKING_FLAG:\n",
    "        return tokenizer.apply_chat_template(\n",
    "            msgs, tokenize=False, add_generation_prompt=True, enable_thinking=False,\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\n{user_text}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "\n",
    "def extract_response(full_text):\n",
    "    \"\"\"Extract assistant response from generated text.\"\"\"\n",
    "    if \"<|im_start|>assistant\" in full_text:\n",
    "        resp = full_text.split(\"<|im_start|>assistant\")[-1]\n",
    "        resp = resp.split(\"<|im_end|>\")[0].strip()\n",
    "        if resp.startswith(\"\\n\"):\n",
    "            resp = resp[1:]\n",
    "        return resp\n",
    "    return full_text\n",
    "\n",
    "def count_tamil_chars(text):\n",
    "    return sum(1 for c in text if '\\u0B80' <= c <= '\\u0BFF')\n",
    "\n",
    "def tamil_char_pct(text):\n",
    "    if not text: return 0.0\n",
    "    return 100.0 * count_tamil_chars(text) / len(text)\n",
    "\n",
    "chat_prompts = [\n",
    "    \"\\u0bb5\\u0ba3\\u0b95\\u0bcd\\u0b95\\u0bae\\u0bcd\",\n",
    "    \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\\u0ba8\\u0bbe\\u0b9f\\u0bcd\\u0b9f\\u0bbf\\u0ba9\\u0bcd \\u0ba4\\u0bb2\\u0bc8\\u0ba8\\u0b95\\u0bb0\\u0bae\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0ba9?\",\n",
    "    \"\\u0ba8\\u0bc0\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd \\u0baf\\u0bbe\\u0bb0\\u0bcd?\",\n",
    "]\n",
    "\n",
    "dapt.eval()\n",
    "dapt.config.use_cache = True\n",
    "\n",
    "for prompt_text in chat_prompts:\n",
    "    full_prompt = build_chat_prompt(prompt_text)\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(dapt.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = dapt.generate(\n",
    "            **inputs, max_new_tokens=100, do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            suppress_tokens=THINK_TOKEN_IDS,\n",
    "        )\n",
    "    full = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    response = extract_response(full)\n",
    "    t_pct = tamil_char_pct(response)\n",
    "    print(f\"\\n  Q: {prompt_text}\")\n",
    "    print(f\"  A: {response[:200]}\")\n",
    "    print(f\"  Tamil: {t_pct:.0f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"If responses are Tamil (even if incoherent), DAPT preserved\")\n",
    "print(\"language capability. SFT will teach instruction-following.\")\n",
    "\n",
    "# Free DAPT model\n",
    "del dapt, dapt_ds\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "print(\"\\n\\U0001f5d1\\ufe0f Pre-validation models freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Load SFT Dataset + Validate\n",
    "\n",
    "Dataset Factory v4.0 output (ADR-010):\n",
    "- ~50% domain packs (security, govt, education, legal, healthcare, culture)\n",
    "- ~33% IndicAlign diversity\n",
    "- ~6% Kural interpretive (hard-capped, anti-memorization)\n",
    "- ~3% handcrafted (guardrails, refusal, brevity, greeting)\n",
    "- ~8% general knowledge\n",
    "- All samples in strict ChatML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "print(f\"\\U0001f4da Loading SFT dataset from {SFT_DATASET}...\")\nsft_ds = load_dataset(SFT_DATASET)\ntrain_ds = sft_ds[\"train\"]\neval_ds = sft_ds[\"validation\"]\n\nprint(f\"\\u2705 Dataset loaded:\")\nprint(f\"   Train:      {len(train_ds)} samples\")\nprint(f\"   Validation: {len(eval_ds)} samples\")\nprint(f\"   Columns:    {train_ds.column_names}\")\n\n# Composition stats\nbucket_dist = Counter(item.get('bucket', 'unknown') for item in train_ds)\nprint(f\"\\n\\U0001f4ca Composition:\")\nfor bucket, count in sorted(bucket_dist.items(), key=lambda x: -x[1]):\n    print(f\"   {bucket}: {count} ({100*count/len(train_ds):.1f}%)\")\n\n# ChatML validation (all samples)\nimport re\nCHATML_RE = re.compile(\n    r'<\\|im_start\\|>system\\n.+?<\\|im_end\\|>\\n'\n    r'<\\|im_start\\|>user\\n(.+?)<\\|im_end\\|>\\n'\n    r'<\\|im_start\\|>assistant\\n(.+?)<\\|im_end\\|>',\n    re.DOTALL\n)\n\nfail_count = 0\nthink_count = 0\nfor i in range(len(train_ds)):\n    text = train_ds[i][\"text\"]\n    if not CHATML_RE.search(text):\n        fail_count += 1\n        if fail_count <= 3:\n            print(f\"   \\u274c Sample {i}: invalid ChatML\")\n    # Check for <think> tags in training data (GPT5.2)\n    if \"<think>\" in text or \"</think>\" in text:\n        think_count += 1\n        if think_count <= 3:\n            print(f\"   \\u26a0\\ufe0f Sample {i}: contains <think> tag!\")\n\nif fail_count == 0:\n    print(f\"\\n\\u2705 All {len(train_ds)} train samples pass ChatML validation\")\nelse:\n    print(f\"\\n\\u274c {fail_count} samples failed ChatML validation!\")\n    if fail_count > len(train_ds) * 0.1:\n        raise RuntimeError(\"Too many ChatML failures \\u2014 DO NOT TRAIN\")\n\nif think_count == 0:\n    print(f\"\\u2705 No <think> tags in training data\")\nelse:\n    print(f\"\\u26a0\\ufe0f {think_count} samples contain <think> tags \\u2014 strip before training\")\n\n# Show a sample\nm = CHATML_RE.search(train_ds[0][\"text\"])\nif m:\n    print(f\"\\n\\U0001f50d Sample:\")\n    print(f\"   Q: {m.group(1)[:100]}\")\n    print(f\"   A: {m.group(2)[:150]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Load DAPT'd Model + Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\U0001f4e5 Loading tokenizer from {DAPT_MODEL}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DAPT_MODEL, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"\\u2705 Tokenizer: {len(tokenizer)} tokens\")\n",
    "print(f\"   eos: {tokenizer.eos_token!r} (ID {tokenizer.eos_token_id})\")\n",
    "print(f\"   pad: {tokenizer.pad_token!r} (ID {tokenizer.pad_token_id})\")\n",
    "\n",
    "# Verify ChatML tokens\n",
    "for token in [\"<|im_start|>\", \"<|im_end|>\"]:\n",
    "    assert token in tokenizer.get_vocab(), f\"Missing {token}!\"\n",
    "print(\"\\u2705 ChatML tokens present\")\n",
    "\n",
    "# Verify <think> tokens\n",
    "for tid in THINK_TOKEN_IDS:\n",
    "    print(f\"   Token {tid}: {tokenizer.decode([tid])!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DAPT'd model in fp16 — NO device_map (for DataParallel)\n",
    "print(f\"\\U0001f4e5 Loading {DAPT_MODEL} in fp16...\")\n",
    "print(f\"   NO device_map \\u2014 Trainer handles DataParallel across {n_gpus} GPUs\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DAPT_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = model.to(\"cuda:0\")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Verify no hf_device_map (would prevent DataParallel)\n",
    "has_dm = hasattr(model, \"hf_device_map\")\n",
    "print(f\"   hf_device_map: {has_dm} (must be False)\")\n",
    "if has_dm:\n",
    "    print(f\"   \\u26a0\\ufe0f WARNING: hf_device_map detected!\")\n",
    "\n",
    "mem_gb = torch.cuda.memory_allocated(0) / 1024**3\n",
    "print(f\"\\u2705 Model loaded: {model.num_parameters():,} params ({mem_gb:.1f} GB)\")\n",
    "print(f\"   This is the DAPT'd instruct model (Tamil-enhanced)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "mem_gb = torch.cuda.memory_allocated() / 1024**3\n",
    "print(f\"\\u2705 LoRA applied | GPU: {mem_gb:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Completion-Only Masking\n",
    "\n",
    "Only train on assistant responses. System prompt and user messages are masked (-100).\n",
    "This prevents the model from memorizing prompts and focuses learning on response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the response template token IDs\n",
    "response_template_str = \"<|im_start|>assistant\\n\"\n",
    "response_template_ids = tokenizer.encode(response_template_str, add_special_tokens=False)\n",
    "print(f\"Response template: {response_template_str!r}\")\n",
    "print(f\"Token IDs: {response_template_ids}\")\n",
    "print(f\"Decoded: {tokenizer.decode(response_template_ids)!r}\")\n",
    "\n",
    "# Fallback: without trailing newline (Lesson #26)\n",
    "response_template_short = \"<|im_start|>assistant\"\n",
    "response_template_short_ids = tokenizer.encode(response_template_short, add_special_tokens=False)\n",
    "print(f\"\\nShort template: {response_template_short!r}\")\n",
    "print(f\"Short IDs: {response_template_short_ids}\")\n",
    "\n",
    "# Verify which template is found in actual data\n",
    "sample_ids = tokenizer.encode(train_ds[0][\"text\"], add_special_tokens=False)\n",
    "\n",
    "def find_subseq(seq, subseq):\n",
    "    for i in range(len(seq) - len(subseq) + 1):\n",
    "        if seq[i:i+len(subseq)] == subseq:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "pos = find_subseq(sample_ids, response_template_ids)\n",
    "if pos >= 0:\n",
    "    print(f\"\\n\\u2705 Full template found at position {pos}\")\n",
    "    use_template_ids = response_template_ids\n",
    "else:\n",
    "    pos = find_subseq(sample_ids, response_template_short_ids)\n",
    "    if pos >= 0:\n",
    "        print(f\"\\n\\u26a0\\ufe0f Using short template (found at position {pos})\")\n",
    "        use_template_ids = response_template_short_ids\n",
    "    else:\n",
    "        raise RuntimeError(\"FATAL: Neither template found in tokenized sample!\")\n",
    "\n",
    "# Create collator\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=use_template_ids,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Preflight: verify masking on 20 samples\n",
    "print(f\"\\n\\U0001f4ca Preflight masking verification (20 samples)...\")\n",
    "fail_count = 0\n",
    "total_trainable = 0\n",
    "total_tokens = 0\n",
    "\n",
    "for idx in range(min(20, len(train_ds))):\n",
    "    t = tokenizer(\n",
    "        train_ds[idx][\"text\"], return_tensors=\"pt\",\n",
    "        truncation=True, max_length=MAX_SEQ_LENGTH,\n",
    "    )\n",
    "    b = collator([{\"input_ids\": t[\"input_ids\"][0], \"attention_mask\": t[\"attention_mask\"][0]}])\n",
    "    n_train = (b[\"labels\"][0] != -100).sum().item()\n",
    "    n_total = len(b[\"labels\"][0])\n",
    "    total_trainable += n_train\n",
    "    total_tokens += n_total\n",
    "    if n_train == 0 or n_train == n_total:\n",
    "        fail_count += 1\n",
    "        status = \"ALL MASKED\" if n_train == 0 else \"NO MASKING\"\n",
    "        print(f\"   \\u274c Sample {idx}: {n_train}/{n_total} {status}\")\n",
    "\n",
    "if fail_count == 0:\n",
    "    pct = 100 * total_trainable / total_tokens\n",
    "    print(f\"   All 20 passed \\u2705 (avg {pct:.1f}% trainable tokens)\")\n",
    "else:\n",
    "    print(f\"\\n\\u274c {fail_count}/20 samples have masking issues!\")\n",
    "    if fail_count > 5:\n",
    "        raise RuntimeError(\"TOO MANY FAILURES \\u2014 DO NOT TRAIN\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "24ai4u92d3s",
   "source": "# === PREFLIGHT MINI-TRAINING (GPT5.2) ===\n# Run 2 steps on 200 samples to catch Trainer/device/config issues\n# before burning the full training budget\n\nprint(\"\\U0001f6e1\\ufe0f Preflight: mini-training (2 steps, 200 samples)...\")\n\npreflight_ds = train_ds.select(range(min(200, len(train_ds))))\n\npreflight_config = SFTConfig(\n    output_dir=\"/kaggle/working/preflight_sft\",\n    num_train_epochs=1,\n    max_steps=2,\n    per_device_train_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=1,\n    learning_rate=LEARNING_RATE,\n    logging_steps=1,\n    save_strategy=\"no\",\n    fp16=True,\n    report_to=\"none\",\n    seed=RANDOM_SEED,\n    dataset_text_field=\"text\",\n    max_seq_length=MAX_SEQ_LENGTH,\n    packing=False,\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n)\n\npreflight_trainer = SFTTrainer(\n    model=model,\n    train_dataset=preflight_ds,\n    args=preflight_config,\n    processing_class=tokenizer,\n    data_collator=collator,\n)\n\npreflight_result = preflight_trainer.train()\npreflight_loss = preflight_result.metrics.get(\"train_loss\", 0)\nprint(f\"\\u2705 Preflight complete! Loss: {preflight_loss:.4f}\")\nprint(f\"   No AcceleratorState errors, no device mismatches.\")\nprint(f\"   Trainer + LoRA + DataCollator + DataParallel all working.\")\n\n# Clean up preflight artifacts\ndel preflight_trainer\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\nimport shutil\nif os.path.exists(\"/kaggle/working/preflight_sft\"):\n    shutil.rmtree(\"/kaggle/working/preflight_sft\")\nprint(\"   Preflight artifacts cleaned up.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Training\n",
    "\n",
    "SFT on DAPT'd model: ~255 steps (3 epochs × ~85 steps/epoch).\n",
    "Should complete in well under 1 hour on dual T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute steps\n",
    "steps_per_epoch = len(train_ds) // (BATCH_SIZE * n_gpus * GRADIENT_ACCUMULATION)\n",
    "total_steps = steps_per_epoch * NUM_EPOCHS\n",
    "log_steps = max(total_steps // 30, 5)\n",
    "eval_steps = max(steps_per_epoch // 2, 10)\n",
    "save_steps = max(steps_per_epoch, 20)\n",
    "\n",
    "print(f\"\\U0001f4ca Training Plan:\")\n",
    "print(f\"   Train samples:    {len(train_ds)}\")\n",
    "print(f\"   Steps/epoch:      ~{steps_per_epoch}\")\n",
    "print(f\"   Total steps:      ~{total_steps}\")\n",
    "print(f\"   Log every:        {log_steps} steps\")\n",
    "print(f\"   Eval every:       {eval_steps} steps\")\n",
    "print(f\"   Save every:       {save_steps} steps\")\n",
    "\n",
    "# Loss logging callback\n",
    "class LossLoggingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.eval_losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            if \"loss\" in logs:\n",
    "                step = state.global_step\n",
    "                loss = logs[\"loss\"]\n",
    "                lr = logs.get(\"learning_rate\", 0)\n",
    "                self.losses.append((step, loss))\n",
    "                print(f\"  Step {step:4d}/{total_steps} | Loss: {loss:.4f} | LR: {lr:.2e}\")\n",
    "            if \"eval_loss\" in logs:\n",
    "                self.eval_losses.append((state.global_step, logs[\"eval_loss\"]))\n",
    "                print(f\"  \\U0001f4ca Eval Loss: {logs['eval_loss']:.4f}\")\n",
    "\n",
    "loss_callback = LossLoggingCallback()\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/vazhi-sft-v4_0\"\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=log_steps,\n",
    "    save_steps=save_steps,\n",
    "    eval_steps=eval_steps,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    max_grad_norm=1.0,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    "    seed=RANDOM_SEED,\n",
    "    load_best_model_at_end=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    args=sft_config,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=collator,\n",
    "    callbacks=[loss_callback],\n",
    ")\n",
    "\n",
    "print(f\"\\u2705 SFTTrainer ready\")\n",
    "print(f\"   Model: {DAPT_MODEL} (DAPT'd instruct)\")\n",
    "print(f\"   Completion-only masking: \\u2705\")\n",
    "print(f\"   Dual T4 DataParallel: {n_gpus} GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\U0001f680 Starting SFT v4.0 training...\")\n",
    "print(f\"   ~{total_steps} steps, {NUM_EPOCHS} epochs\")\n",
    "print(f\"   Base: DAPT'd instruct (Tamil PPL 2.6)\")\n",
    "print()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\\u2705 Training complete!\")\n",
    "metrics = train_result.metrics\n",
    "for k, v in metrics.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "\n",
    "# Final eval\n",
    "print(\"\\n\\U0001f4ca Final eval...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "for k, v in eval_metrics.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "\n",
    "# Loss summary\n",
    "if loss_callback.losses:\n",
    "    start = loss_callback.losses[0][1]\n",
    "    end = loss_callback.losses[-1][1]\n",
    "    print(f\"\\n\\U0001f4c8 Loss: {start:.4f} \\u2192 {end:.4f} ({100*(start-end)/start:.1f}% drop)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 8a. Resume from Checkpoint (if Kaggle disconnected)\n",
    "\n",
    "**Only run if training was interrupted.** Skip if training completed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === UNCOMMENT ONLY IF TRAINING WAS INTERRUPTED ===\n",
    "\n",
    "# checkpoints = sorted(glob.glob(f\"{OUTPUT_DIR}/checkpoint-*\"), key=os.path.getmtime)\n",
    "# if checkpoints:\n",
    "#     latest = checkpoints[-1]\n",
    "#     print(f\"\\U0001f504 Resuming from {latest}\")\n",
    "#     train_result = trainer.train(resume_from_checkpoint=latest)\n",
    "#     print(\"\\u2705 Resumed and completed!\")\n",
    "#     for k, v in train_result.metrics.items():\n",
    "#         print(f\"   {k}: {v}\")\n",
    "# else:\n",
    "#     print(\"\\u274c No checkpoints found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 9. Save & Upload LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "ADAPTER_PATH = \"/kaggle/working/vazhi-sft-v4_0-lora\"\n\nprint(\"\\U0001f4be Saving LoRA adapter...\")\ntrainer.save_model(ADAPTER_PATH)\ntokenizer.save_pretrained(ADAPTER_PATH)\n\n# Save training metadata for reproducibility (GPT5.2)\nimport hashlib\nds_hash = hashlib.md5(str(train_ds[:10][\"text\"]).encode()).hexdigest()[:12]\nmetadata = {\n    \"base_model\": DAPT_MODEL,\n    \"dataset\": SFT_DATASET,\n    \"dataset_hash\": ds_hash,\n    \"train_samples\": len(train_ds),\n    \"eval_samples\": len(eval_ds),\n    \"learning_rate\": LEARNING_RATE,\n    \"epochs\": NUM_EPOCHS,\n    \"lora_r\": LORA_R,\n    \"lora_alpha\": LORA_ALPHA,\n    \"max_seq_length\": MAX_SEQ_LENGTH,\n    \"effective_batch\": BATCH_SIZE * n_gpus * GRADIENT_ACCUMULATION,\n    \"train_loss\": metrics.get(\"train_loss\"),\n    \"eval_loss\": eval_metrics.get(\"eval_loss\"),\n}\nwith open(f\"{ADAPTER_PATH}/training_metadata.json\", \"w\") as f:\n    json.dump(metadata, f, indent=2)\nprint(f\"   training_metadata.json saved\")\n\nadapter_files = glob.glob(f\"{ADAPTER_PATH}/*\")\nprint(f\"   Files: {[os.path.basename(f) for f in adapter_files]}\")\nassert any('adapter' in f for f in adapter_files), \"No adapter files!\"\nprint(\"\\u2705 Adapter saved\")\n\n# Upload\napi = HfApi()\napi.create_repo(ADAPTER_REPO, exist_ok=True)\nprint(f\"\\U0001f4e4 Uploading to {ADAPTER_REPO}...\")\napi.upload_folder(\n    folder_path=ADAPTER_PATH,\n    repo_id=ADAPTER_REPO,\n    commit_message=f\"SFT v4.0 adapter on DAPT v1.1, r={LORA_R}, lr={LEARNING_RATE}, {NUM_EPOCHS} epochs\",\n)\nprint(f\"\\u2705 Adapter: https://huggingface.co/{ADAPTER_REPO}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 10. Merge LoRA in FP16\n",
    "\n",
    "**Hard rule (Lesson #27/39):** NEVER merge into 4-bit. Reload base in fp16, merge there.\n",
    "\n",
    "Note: We merge onto the DAPT'd model, not vanilla — this preserves Tamil fluency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free training model\n",
    "del model, trainer\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "print(\"\\U0001f5d1\\ufe0f Training model freed\")\n",
    "\n",
    "# Reload DAPT'd base in fp16 for clean merge\n",
    "print(f\"\\U0001f517 Loading {DAPT_MODEL} in fp16 for merge...\")\n",
    "base_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    DAPT_MODEL, torch_dtype=torch.float16, device_map={\"\":0}, trust_remote_code=True,\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_fp16, ADAPTER_PATH)\n",
    "peft_model.gradient_checkpointing_disable()\n",
    "peft_model.config.use_cache = True\n",
    "peft_model.eval()\n",
    "\n",
    "print(\"\\U0001f500 Merging LoRA in fp16...\")\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "print(f\"\\u2705 Merged: {merged_model.num_parameters():,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 11. Evaluation — Chat-Templated Prompts\n",
    "\n",
    "**This is SFT eval, not DAPT eval.** We test instruction-following with chat template,\n",
    "not raw text continuation. Uses `apply_chat_template(enable_thinking=False)` per GPT5.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": "merged_model.eval()\nmerged_model.config.use_cache = True\n\ndef compute_repeat_ratio(text, n=3):\n    \"\"\"Fraction of tokens in repeated n-gram chains. >0.2 is bad (GPT5.2).\"\"\"\n    words = text.split()\n    if len(words) < n:\n        return 0.0\n    ngrams = [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n    seen = set()\n    repeated_positions = set()\n    for i, ng in enumerate(ngrams):\n        if ng in seen:\n            for j in range(i, i + n):\n                repeated_positions.add(j)\n        seen.add(ng)\n    return len(repeated_positions) / max(len(words), 1)\n\ntest_prompts = [\n    # Greetings (2)\n    (\"greeting\", \"\\u0bb5\\u0ba3\\u0b95\\u0bcd\\u0b95\\u0bae\\u0bcd\"),\n    (\"greeting\", \"\\u0ba8\\u0bc0\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd \\u0baf\\u0bbe\\u0bb0\\u0bcd?\"),\n    # Factual (3)\n    (\"factual\", \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\\u0ba8\\u0bbe\\u0b9f\\u0bcd\\u0b9f\\u0bbf\\u0ba9\\u0bcd \\u0ba4\\u0bb2\\u0bc8\\u0ba8\\u0b95\\u0bb0\\u0bae\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0ba9?\"),\n    (\"factual\", \"\\u0baa\\u0bca\\u0b99\\u0bcd\\u0b95\\u0bb2\\u0bcd \\u0b8e\\u0baa\\u0bcd\\u0baa\\u0bcb\\u0ba4\\u0bc1 \\u0b95\\u0bca\\u0ba3\\u0bcd\\u0b9f\\u0bbe\\u0b9f\\u0baa\\u0bcd\\u0baa\\u0b9f\\u0bc1\\u0b95\\u0bbf\\u0bb1\\u0ba4\\u0bc1?\"),\n    (\"factual\", \"2+2 \\u0b8e\\u0ba9\\u0bcd\\u0ba9?\"),\n    # Culture (2)\n    (\"culture\", \"\\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0bb5\\u0bb3\\u0bcd\\u0bb3\\u0bc1\\u0bb5\\u0bb0\\u0bcd \\u0baf\\u0bbe\\u0bb0\\u0bcd?\"),\n    (\"culture\", \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd \\u0bae\\u0bca\\u0bb4\\u0bbf\\u0baf\\u0bbf\\u0ba9\\u0bcd \\u0b9a\\u0bbf\\u0bb1\\u0baa\\u0bcd\\u0baa\\u0bc1 \\u0b8e\\u0ba9\\u0bcd\\u0ba9?\"),\n    # Safety (2)\n    (\"safety\", \"\\u0b92\\u0bb0\\u0bc1 scam message \\u0bb5\\u0ba8\\u0bcd\\u0ba4\\u0bbe\\u0bb2\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0ba9 \\u0b9a\\u0bc6\\u0baf\\u0bcd\\u0bb5\\u0ba4\\u0bc1?\"),\n    (\"safety\", \"\\u0bb5\\u0bc0\\u0b9f\\u0bcd\\u0b9f\\u0bbf\\u0bb2\\u0bcd \\u0ba4\\u0bc0 \\u0bb5\\u0bbf\\u0baa\\u0ba4\\u0bcd\\u0ba4\\u0bc1 \\u0b8e\\u0ba9\\u0bcd\\u0ba9 \\u0b9a\\u0bc6\\u0baf\\u0bcd\\u0baf \\u0bb5\\u0bc7\\u0ba3\\u0bcd\\u0b9f\\u0bc1\\u0bae\\u0bcd?\"),\n    # Refusal (2)\n    (\"refusal\", \"\\u0ba8\\u0bbe\\u0bb3\\u0bc8 \\u0baa\\u0b99\\u0bcd\\u0b95\\u0bc1 \\u0b9a\\u0ba8\\u0bcd\\u0ba4\\u0bc8 \\u0b8f\\u0bb1\\u0bc1\\u0bae\\u0bbe?\"),\n    (\"refusal\", \"\\u0b8e\\u0ba9\\u0bcd \\u0b95\\u0ba3\\u0bbf\\u0ba9\\u0bbf\\u0baf\\u0bbf\\u0bb2\\u0bcd \\u0bb5\\u0bc8\\u0bb0\\u0bb8\\u0bcd \\u0b87\\u0bb0\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bbf\\u0bb1\\u0ba4\\u0bbe?\"),\n    # General (1)\n    (\"general\", \"\\u0b95\\u0bbe\\u0bb2\\u0bc8\\u0baf\\u0bbf\\u0bb2\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0ba9 \\u0b9a\\u0bbe\\u0baa\\u0bcd\\u0baa\\u0bbf\\u0b9f\\u0bb2\\u0bbe\\u0bae\\u0bcd?\"),\n]\n\nprint(f\"\\n{'='*60}\")\nprint(f\"\\U0001f9ea SFT v4.0 EVAL: {len(test_prompts)} chat-templated prompts\")\nprint(f\"   Using: {'apply_chat_template(enable_thinking=False)' if USE_THINKING_FLAG else 'manual ChatML'}\")\nprint(f\"{'='*60}\")\n\nresults = []\n\nfor category, prompt_text in test_prompts:\n    full_prompt = build_chat_prompt(prompt_text)\n    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(merged_model.device)\n\n    gen_kwargs = dict(\n        max_new_tokens=150,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.eos_token_id,\n        suppress_tokens=THINK_TOKEN_IDS,\n        no_repeat_ngram_size=4,\n    )\n    if category == \"factual\":\n        gen_kwargs[\"do_sample\"] = False\n    else:\n        gen_kwargs[\"do_sample\"] = True\n        gen_kwargs[\"temperature\"] = 0.3\n        gen_kwargs[\"top_p\"] = 0.9\n        gen_kwargs[\"repetition_penalty\"] = 1.2\n\n    with torch.no_grad():\n        outputs = merged_model.generate(**inputs, **gen_kwargs)\n\n    full = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    response = extract_response(full)\n\n    t_pct = tamil_char_pct(response)\n    words = response.split()\n    repeat_r = compute_repeat_ratio(response)\n    has_loop = repeat_r > 0.2\n    has_system = \"system\" in response.lower()[:50]\n    has_think = \"<think>\" in response\n    is_empty = len(response.strip()) < 5\n    is_code = any(c in response[:100] for c in ['=True', '={\"', 'var ', 'function', '<br'])\n\n    status = \"\\u2705\"\n    if is_code: status = \"\\u274c CODE\"\n    elif has_loop: status = \"\\u26a0\\ufe0f LOOP\"\n    elif has_system: status = \"\\u274c SYSTEM\"\n    elif has_think: status = \"\\u274c THINK\"\n    elif is_empty: status = \"\\u274c EMPTY\"\n    elif t_pct < 20 and category not in [\"factual\"]: status = \"\\u26a0\\ufe0f LOW TAMIL\"\n\n    results.append((category, prompt_text, response[:300], status, t_pct, repeat_r))\n\n    print(f\"\\n[{category.upper()}] {status} (Tamil: {t_pct:.0f}%, Repeat: {repeat_r:.2f})\")\n    print(f\"  Q: {prompt_text}\")\n    print(f\"  A: {response[:300]}\")\n    print(\"-\" * 50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": "# === EVAL SUMMARY ===\nprint(f\"\\n{'='*60}\")\nprint(f\"\\U0001f4ca SFT v4.0 EVAL SUMMARY\")\nprint(f\"{'='*60}\")\n\npass_count = sum(1 for r in results if r[3] == \"\\u2705\")\navg_tamil = np.mean([r[4] for r in results])\navg_repeat = np.mean([r[5] for r in results])\nmax_repeat = max(r[5] for r in results)\n\nprint(f\"   Passed:      {pass_count}/{len(results)}\")\nprint(f\"   Avg Tamil:   {avg_tamil:.0f}%\")\nprint(f\"   Avg Repeat:  {avg_repeat:.2f} (>0.2 is bad)\")\nprint(f\"   Max Repeat:  {max_repeat:.2f}\")\nprint()\n\nfor cat, prompt, resp, status, tamil, repeat in results:\n    print(f\"   {status} [{cat}] {prompt[:40]}... (Tamil: {tamil:.0f}%, Rep: {repeat:.2f})\")\n\nprint(f\"\\n\\U0001f4cb Previous SFT attempts for comparison:\")\nprint(f\"   v3.8 (SFT-only, no DAPT): 0/12 passed, avg Tamil 52% \\u274c\")\nprint(f\"   v3.6 (merge corruption):  0/12 passed, 0% Tamil \\u274c\")\n\nif pass_count >= len(results) * 0.8 and avg_tamil > 30 and avg_repeat < 0.2:\n    print(f\"\\n\\U0001f389 SFT v4.0 successful! Proceed to GGUF quantization.\")\nelif pass_count >= len(results) * 0.5:\n    print(f\"\\n\\u26a0\\ufe0f  Partial success. Upload and test manually.\")\nelse:\n    print(f\"\\n\\u274c SFT failed. Check loss curve and pre-merge sanity.\")\n    print(f\"   If loss converged but output is bad: try more epochs or higher LR\")\n    print(f\"   If loss didn't converge: dataset may need more samples\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## 12. Upload Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "api.create_repo(OUTPUT_MODEL, exist_ok=True)\n",
    "\n",
    "print(f\"\\U0001f4e4 Pushing merged fp16 model to {OUTPUT_MODEL}...\")\n",
    "merged_model.push_to_hub(\n",
    "    OUTPUT_MODEL,\n",
    "    private=False,\n",
    "    commit_message=(\n",
    "        f\"SFT v4.0: VAZHI Tamil assistant, DAPT v1.1 base, \"\n",
    "        f\"LoRA r={LORA_R}, lr={LEARNING_RATE}, {NUM_EPOCHS} epochs, \"\n",
    "        f\"{len(train_ds)} samples\"\n",
    "    ),\n",
    ")\n",
    "tokenizer.push_to_hub(OUTPUT_MODEL)\n",
    "\n",
    "print(f\"\\n\\u2705 Model:   https://huggingface.co/{OUTPUT_MODEL}\")\n",
    "print(f\"\\u2705 Adapter: https://huggingface.co/{ADAPTER_REPO}\")\n",
    "print(f\"\\n\\U0001f449 Next: Convert to GGUF (Q4_K_M) for mobile deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Pipeline Complete\n",
    "\n",
    "| Step | Notebook | Artifact |\n",
    "|------|----------|----------|\n",
    "| 1. Data Prep | `Vazhi_DAPT_Data_v1_1.ipynb` | `CryptoYogi/vazhi-dapt-tamil-v1_1` |\n",
    "| 2. DAPT | `Vazhi_DAPT_v1_1_Tamil.ipynb` | `CryptoYogi/qwen3-0.6b-tamil-v1_1` |\n",
    "| 3. SFT (this) | `Vazhi_SFT_v4_0_OnDAPT.ipynb` | `CryptoYogi/vazhi-v4_0` |\n",
    "\n",
    "### Key Config\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Base model | `CryptoYogi/qwen3-0.6b-tamil-v1_1` (DAPT'd instruct) |\n",
    "| Dataset | `CryptoYogi/vazhi-tamil-sft-v4_0` (ADR-010, ~1,514 samples) |\n",
    "| LR | 2e-5 |\n",
    "| Epochs | 3 |\n",
    "| LoRA | r=16, alpha=32 |\n",
    "| Masking | Completion-only (assistant responses only) |\n",
    "| Merge | fp16 (never 4-bit) |\n",
    "\n",
    "### Next Steps\n",
    "1. **GGUF Quantization** — Convert to Q4_K_M (~462MB) for mobile\n",
    "2. **Mobile test** — Load in Flutter app via llama.cpp\n",
    "3. **Iterate** — If quality is insufficient, increase epochs or add data\n",
    "\n",
    "### If SFT failed\n",
    "1. Check loss curve — did it converge?\n",
    "2. Check pre-merge sanity — did PeftModel responses look ok?\n",
    "3. Try higher LR (5e-5) or more epochs (5)\n",
    "4. Dataset may need more samples — combine with more IndicAlign\n",
    "5. Fallback: Sarvam-1 IQ3_M (1.17GB, proven Tamil)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
