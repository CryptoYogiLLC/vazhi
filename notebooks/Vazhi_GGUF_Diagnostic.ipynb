{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI GGUF Diagnostic Notebook\n",
    "\n",
    "**Goal**: Identify where quality loss occurs in the GGUF pipeline\n",
    "\n",
    "**Test Question**: திருக்குறளின் முதல் குறள் என்ன?\n",
    "\n",
    "**Expected Answer**: அகர முதல எழுத்தெல்லாம்...\n",
    "\n",
    "**Checkpoints**:\n",
    "1. LoRA Model (before merge) - Should work ✅\n",
    "2. Merged Model (after merge) - Test this\n",
    "3. GGUF F16 (after conversion) - Test this\n",
    "4. GGUF Q8_0 (light quantization) - Test this\n",
    "5. GGUF Q4_K_M (aggressive quantization) - Currently broken\n",
    "\n",
    "**Platform**: Kaggle (30GB RAM) recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers peft accelerate huggingface_hub sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()  # Enter your HF token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import gc\n",
    "\n",
    "# Configuration\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "LORA_ADAPTER = \"CryptoYogi/vazhi-lora\"\n",
    "\n",
    "# Test prompt - Thirukkural first verse\n",
    "TEST_PROMPT = \"\"\"<|im_start|>system\n",
    "நீங்கள் VAZHI (வழி), தமிழ் மக்களுக்கான AI உதவியாளர்.\n",
    "தமிழ் கலாச்சாரம், திருக்குறள், சித்தர்கள், கோவில்கள் பற்றி உதவுங்கள்.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "திருக்குறளின் முதல் குறள் என்ன?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "EXPECTED_KEYWORDS = [\"அகர\", \"முதல\", \"எழுத்தெல்லாம்\", \"ஆதி\", \"பகவன்\"]\n",
    "\n",
    "def test_model(model, tokenizer, name):\n",
    "    \"\"\"Test model and check for expected keywords\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    inputs = tokenizer(TEST_PROMPT, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = response.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    print(f\"\\nResponse:\\n{answer}\\n\")\n",
    "    \n",
    "    # Check for expected keywords\n",
    "    found = [kw for kw in EXPECTED_KEYWORDS if kw in answer]\n",
    "    print(f\"Keywords found: {len(found)}/{len(EXPECTED_KEYWORDS)} - {found}\")\n",
    "    \n",
    "    if len(found) >= 3:\n",
    "        print(\"✅ PASS - Response contains Thirukkural content\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ FAIL - Response missing Thirukkural content\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1: Test LoRA Model (Before Merge)\n",
    "\n",
    "This should work - it's what we tested after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "print(\"Loading LoRA adapter...\")\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    LORA_ADAPTER,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Test LoRA model\n",
    "checkpoint1_pass = test_model(lora_model, tokenizer, \"Checkpoint 1: LoRA Model (before merge)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 2: Test Merged Model (After Merge)\n",
    "\n",
    "This is where the LoRA weights are merged into the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging LoRA into base model...\")\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Test merged model\n",
    "checkpoint2_pass = test_model(merged_model, tokenizer, \"Checkpoint 2: Merged Model (after merge)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged model for GGUF conversion\n",
    "MERGED_OUTPUT = \"./vazhi-merged\"\n",
    "print(f\"Saving merged model to {MERGED_OUTPUT}...\")\n",
    "merged_model.save_pretrained(MERGED_OUTPUT, safe_serialization=True)\n",
    "tokenizer.save_pretrained(MERGED_OUTPUT)\n",
    "print(\"Saved!\")\n",
    "\n",
    "# Clear memory\n",
    "del lora_model\n",
    "del merged_model\n",
    "del base_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup llama.cpp for GGUF Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and build llama.cpp\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "!cd llama.cpp && mkdir -p build && cd build && cmake .. && make -j4\n",
    "!pip install -q -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 3: Test GGUF F16 (After Conversion, Before Quantization)\n",
    "\n",
    "This tests if the HuggingFace → GGUF conversion preserves quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to GGUF F16\n",
    "print(\"Converting to GGUF F16...\")\n",
    "!python llama.cpp/convert_hf_to_gguf.py \\\n",
    "    {MERGED_OUTPUT} \\\n",
    "    --outfile vazhi-f16.gguf \\\n",
    "    --outtype f16\n",
    "\n",
    "!ls -lh vazhi-f16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test F16 GGUF\nprint(\"\\n\" + \"=\"*60)\nprint(\"Testing: Checkpoint 3: GGUF F16 (no quantization)\")\nprint(\"=\"*60)\n\n!./llama.cpp/build/bin/llama-cli \\\n    -m vazhi-f16.gguf \\\n    -p \"<|im_start|>system\\nநீங்கள் VAZHI, தமிழ் கலாச்சாரம் பற்றி உதவுங்கள்.<|im_end|>\\n<|im_start|>user\\nதிருக்குறளின் முதல் குறள் என்ன?<|im_end|>\\n<|im_start|>assistant\\n\" \\\n    -n 150 \\\n    --temp 0.7 \\\n    -ngl 0 \\\n    --stop \"<|im_end|>\" \\\n    2>&1 | tail -30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 4: Test GGUF Q8_0 (Light Quantization)\n",
    "\n",
    "Q8_0 is 8-bit quantization - less aggressive than Q4_K_M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize to Q8_0\n",
    "print(\"Quantizing to Q8_0...\")\n",
    "!./llama.cpp/build/bin/llama-quantize \\\n",
    "    vazhi-f16.gguf \\\n",
    "    vazhi-q8_0.gguf \\\n",
    "    q8_0\n",
    "\n",
    "!ls -lh vazhi-q8_0.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test Q8_0 GGUF\nprint(\"\\n\" + \"=\"*60)\nprint(\"Testing: Checkpoint 4: GGUF Q8_0 (8-bit quantization)\")\nprint(\"=\"*60)\n\n!./llama.cpp/build/bin/llama-cli \\\n    -m vazhi-q8_0.gguf \\\n    -p \"<|im_start|>system\\nநீங்கள் VAZHI, தமிழ் கலாச்சாரம் பற்றி உதவுங்கள்.<|im_end|>\\n<|im_start|>user\\nதிருக்குறளின் முதல் குறள் என்ன?<|im_end|>\\n<|im_start|>assistant\\n\" \\\n    -n 150 \\\n    --temp 0.7 \\\n    -ngl 0 \\\n    --stop \"<|im_end|>\" \\\n    2>&1 | tail -30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 5: Test GGUF Q4_K_M (Aggressive Quantization)\n",
    "\n",
    "This is what we currently have - 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize to Q4_K_M\n",
    "print(\"Quantizing to Q4_K_M...\")\n",
    "!./llama.cpp/build/bin/llama-quantize \\\n",
    "    vazhi-f16.gguf \\\n",
    "    vazhi-q4_k_m.gguf \\\n",
    "    q4_k_m\n",
    "\n",
    "!ls -lh vazhi-q4_k_m.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test Q4_K_M GGUF\nprint(\"\\n\" + \"=\"*60)\nprint(\"Testing: Checkpoint 5: GGUF Q4_K_M (4-bit quantization)\")\nprint(\"=\"*60)\n\n!./llama.cpp/build/bin/llama-cli \\\n    -m vazhi-q4_k_m.gguf \\\n    -p \"<|im_start|>system\\nநீங்கள் VAZHI, தமிழ் கலாச்சாரம் பற்றி உதவுங்கள்.<|im_end|>\\n<|im_start|>user\\nதிருக்குறளின் முதல் குறள் என்ன?<|im_end|>\\n<|im_start|>assistant\\n\" \\\n    -n 150 \\\n    --temp 0.7 \\\n    -ngl 0 \\\n    --stop \"<|im_end|>\" \\\n    2>&1 | tail -30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Run this cell after testing all checkpoints to see the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Review the outputs above and mark each checkpoint:\n",
    "\n",
    "| Checkpoint | Stage | Expected | Actual | File Size |\n",
    "|------------|-------|----------|--------|----------|\n",
    "| 1 | LoRA Model | ✅ Pass | ? | N/A |\n",
    "| 2 | Merged Model | ? | ? | ~6GB |\n",
    "| 3 | GGUF F16 | ? | ? | ~6GB |\n",
    "| 4 | GGUF Q8_0 | ? | ? | ~3.2GB |\n",
    "| 5 | GGUF Q4_K_M | ❌ Fail | ? | ~1.8GB |\n",
    "\n",
    "If quality degrades at:\n",
    "- Checkpoint 2: Problem with LoRA merge\n",
    "- Checkpoint 3: Problem with GGUF conversion\n",
    "- Checkpoint 4: Problem with quantization (use Q8_0 instead)\n",
    "- Checkpoint 5 only: Q4_K_M too aggressive, use Q8_0\n",
    "\"\"\")\n",
    "print(\"\\nFile sizes:\")\n",
    "!ls -lh vazhi-*.gguf 2>/dev/null || echo \"No GGUF files yet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Best Working Model\n",
    "\n",
    "Once you identify the best working quantization, upload it to HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "GGUF_REPO = \"CryptoYogi/vazhi-gguf\"\n",
    "\n",
    "# Upload Q8_0 if it works better\n",
    "# Uncomment the model you want to upload:\n",
    "\n",
    "# print(\"Uploading Q8_0 model...\")\n",
    "# api.upload_file(\n",
    "#     path_or_fileobj=\"vazhi-q8_0.gguf\",\n",
    "#     path_in_repo=\"vazhi-q8_0.gguf\",\n",
    "#     repo_id=GGUF_REPO,\n",
    "#     repo_type=\"model\",\n",
    "# )\n",
    "# print(f\"Uploaded to https://huggingface.co/{GGUF_REPO}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
