{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI Dataset Factory v4.0\n",
    "\n",
    "Constructs the curated SFT dataset from all source data per [ADR-010](../docs/adr/010-data-pipeline-architecture.md).\n",
    "\n",
    "**Pipeline:** Source Data (local) \u2192 Dataset Factory (this notebook) \u2192 Training \u2192 Artifacts (HuggingFace)\n",
    "\n",
    "**Key features:**\n",
    "1. Loads vazhi-packs (6 domain packs, 3,007 Q&A pairs)\n",
    "2. Streams IndicAlign diversity samples (~1,500 Tamil samples)\n",
    "3. Includes handcrafted samples (refusal, brevity, greeting, guardrails)\n",
    "4. Loads general knowledge Q&A from LEGACY files\n",
    "5. Anti-memorization filter for Thirukkural verbatim Q&As\n",
    "6. **Hard-enforces** composition targets (fails if violated)\n",
    "7. Stratified train/eval split (90/10) by source bucket\n",
    "8. Uploads to HuggingFace with version tracking\n",
    "\n",
    "**Run on:** Kaggle (P100) or Colab (T4)\n",
    "\n",
    "**Output:** `CryptoYogi/vazhi-tamil-sft-v4_0` with `train` and `validation` splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Config & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets huggingface_hub\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# === CONFIG ===\n",
    "VERSION = \"4.0\"\n",
    "OUTPUT_DATASET = \"CryptoYogi/vazhi-tamil-sft-v4_0\"\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"\\u0ba8\\u0bc0\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd VAZHI (\\u0bb5\\u0bb4\\u0bbf), \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd \\u0bae\\u0b95\\u0bcd\\u0b95\\u0bb3\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bbe\\u0ba9 AI \\u0b89\\u0ba4\\u0bb5\\u0bbf\\u0baf\\u0bbe\\u0bb3\\u0bb0\\u0bcd. \"\n",
    "    \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bbf\\u0bb2\\u0bcd \\u0ba4\\u0bc6\\u0bb3\\u0bbf\\u0bb5\\u0bbe\\u0b95\\u0bb5\\u0bc1\\u0bae\\u0bcd \\u0b89\\u0ba4\\u0bb5\\u0bbf\\u0baf\\u0bbe\\u0b95\\u0bb5\\u0bc1\\u0bae\\u0bcd \\u0baa\\u0ba4\\u0bbf\\u0bb2\\u0bb3\\u0bbf\\u0baf\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd. \"\n",
    "    '\\u0ba4\\u0bc6\\u0bb0\\u0bbf\\u0baf\\u0bbe\\u0bb5\\u0bbf\\u0b9f\\u0bcd\\u0b9f\\u0bbe\\u0bb2\\u0bcd \"\\u0ba4\\u0bc6\\u0bb0\\u0bbf\\u0baf\\u0bb5\\u0bbf\\u0bb2\\u0bcd\\u0bb2\\u0bc8\" \\u0b8e\\u0ba9\\u0bcd\\u0bb1\\u0bc1 \\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd.'\n",
    ")\n",
    "\n",
    "# Hard composition constraints (Factory fails if violated)\n",
    "COMPOSITION_TARGETS = {\n",
    "    \"domain_packs\": {\"target\": 0.45, \"min\": 0.40, \"max\": 0.50},\n",
    "    \"indicalign\":   {\"target\": 0.30, \"min\": 0.25, \"max\": 0.35},\n",
    "    \"kural_interp\": {\"target\": 0.15, \"min\": 0.00, \"max\": 0.15},\n",
    "    \"handcrafted\":  {\"target\": 0.03, \"min\": 0.02, \"max\": 0.05},\n",
    "    \"general\":      {\"target\": 0.07, \"min\": 0.05, \"max\": 0.10},\n",
    "}\n",
    "\n",
    "MAX_CHAR_LENGTH = 1500\n",
    "MIN_TAMIL_PCT = 30  # Minimum Tamil character percentage\n",
    "\n",
    "print(f\"\\u2705 Config loaded: Dataset Factory v{VERSION}\")\n",
    "print(f\"   Output: {OUTPUT_DATASET}\")\n",
    "print(f\"   Composition targets:\")\n",
    "for name, cfg in COMPOSITION_TARGETS.items():\n",
    "    print(f\"     {name}: {cfg['target']:.0%} (min {cfg['min']:.0%}, max {cfg['max']:.0%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "# On Kaggle:\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "    login(token=hf_token)\n",
    "    print(\"\\u2705 Logged in via Kaggle secrets\")\n",
    "except Exception:\n",
    "    # On Colab or local: use HF_TOKEN env var or interactive login\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        hf_token = userdata.get('HF_TOKEN')\n",
    "        login(token=hf_token)\n",
    "        print(\"\\u2705 Logged in via Colab secrets\")\n",
    "    except Exception:\n",
    "        login()\n",
    "        print(\"\\u2705 Logged in interactively\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_chatml(instruction, output, system_prompt=None):\n",
    "    \"\"\"Convert instruction/output pair to strict ChatML format.\"\"\"\n",
    "    sp = system_prompt or SYSTEM_PROMPT\n",
    "    return (\n",
    "        f\"<|im_start|>system\\n{sp}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{instruction}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n{output}<|im_end|>\"\n",
    "    )\n",
    "\n",
    "\n",
    "CHATML_PATTERN = re.compile(\n",
    "    r'<\\|im_start\\|>system\\n.+?<\\|im_end\\|>\\n'\n",
    "    r'<\\|im_start\\|>user\\n(.+?)<\\|im_end\\|>\\n'\n",
    "    r'<\\|im_start\\|>assistant\\n(.+?)<\\|im_end\\|>',\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "\n",
    "def validate_chatml_strict(text):\n",
    "    \"\"\"Validate a sample has proper ChatML with non-empty user AND assistant.\"\"\"\n",
    "    match = CHATML_PATTERN.search(text)\n",
    "    if not match:\n",
    "        return False, \"no ChatML structure found\"\n",
    "    user_content = match.group(1).strip()\n",
    "    assistant_content = match.group(2).strip()\n",
    "    if len(user_content) < 2:\n",
    "        return False, \"empty user content\"\n",
    "    if len(assistant_content) < 2:\n",
    "        return False, \"empty assistant content\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "\n",
    "def count_tamil_chars(text):\n",
    "    \"\"\"Count Tamil Unicode characters.\"\"\"\n",
    "    return sum(1 for c in text if '\\u0b80' <= c <= '\\u0bff')\n",
    "\n",
    "\n",
    "def tamil_char_pct(text):\n",
    "    \"\"\"Get Tamil character percentage.\"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    return 100.0 * count_tamil_chars(text) / len(text)\n",
    "\n",
    "\n",
    "def is_verbatim_kural_qa(question, answer):\n",
    "    \"\"\"Reject Q&As that ask for exact verse text. Err on the side of rejecting.\"\"\"\n",
    "    verbatim_patterns = [\n",
    "        r'\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*\\d+\\s*(\\u0b8e\\u0ba9\\u0bcd\\u0ba9|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd|\\u0b8e\\u0bb4\\u0bc1\\u0ba4\\u0bbf\\s*\\u0b95\\u0bbe\\u0b9f\\u0bcd\\u0b9f\\u0bc1|\\u0b95\\u0bc2\\u0bb1\\u0bc1\\u0b95)',\n",
    "        r'(first|\\u0bae\\u0bc1\\u0ba4\\u0bb2\\u0bcd)\\s*\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*(\\u0b8e\\u0ba9\\u0bcd\\u0ba9|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd)',\n",
    "        r'\\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bbf\\u0ba9\\u0bcd\\s+\\u0bae\\u0bc1\\u0ba4\\u0bb2\\u0bcd\\s+\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd',\n",
    "        r'\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*[\\d]+(?:\\s*\\u0b90)?\\s*\\u0b8e\\u0bb4\\u0bc1\\u0ba4\\u0bbf',\n",
    "    ]\n",
    "    for pat in verbatim_patterns:\n",
    "        if re.search(pat, question, re.IGNORECASE):\n",
    "            return True\n",
    "    # Also reject if answer looks like a raw couplet (short + has newline = verse format)\n",
    "    if len(answer) < 200 and \"\\n\" in answer and not any(\n",
    "        w in answer for w in [\n",
    "            \"\\u0bb5\\u0bbf\\u0bb3\\u0b95\\u0bcd\\u0b95\\u0bae\\u0bcd\",\n",
    "            \"\\u0baa\\u0bca\\u0bb0\\u0bc1\\u0bb3\\u0bcd\",\n",
    "            \"\\u0b85\\u0bb0\\u0bcd\\u0ba4\\u0bcd\\u0ba4\\u0bae\\u0bcd\",\n",
    "        ]\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Self-test\n",
    "good = to_chatml(\"test question\", \"test answer\")\n",
    "valid, reason = validate_chatml_strict(good)\n",
    "assert valid, f\"Self-test failed: {reason}\"\n",
    "print(\"\\u2705 Helper functions defined and self-tested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Domain Packs (vazhi-packs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load flattened vazhi-packs from data/sources/sft/vazhi-packs/\n",
    "# On Kaggle: upload the data/sources/ directory as a dataset or clone the repo\n",
    "\n",
    "PACKS_DIR = Path(\"../data/sources/sft/vazhi-packs\")\n",
    "if not PACKS_DIR.exists():\n",
    "    # Try Kaggle input path\n",
    "    PACKS_DIR = Path(\"/kaggle/input/vazhi-sources/sft/vazhi-packs\")\n",
    "if not PACKS_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Cannot find vazhi-packs. Upload data/sources/ as a Kaggle dataset \"\n",
    "        \"or clone the repo.\"\n",
    "    )\n",
    "\n",
    "domain_pack_samples = []\n",
    "kural_interp_samples = []\n",
    "\n",
    "for pack_file in sorted(PACKS_DIR.glob(\"*.json\")):\n",
    "    pack_name = pack_file.stem\n",
    "    with open(pack_file, encoding=\"utf-8\") as f:\n",
    "        pairs = json.load(f)\n",
    "    \n",
    "    pack_domain = []\n",
    "    pack_kural = []\n",
    "    pack_verbatim_rejected = 0\n",
    "    \n",
    "    for pair in pairs:\n",
    "        instruction = pair[\"instruction\"]\n",
    "        output = pair[\"output\"]\n",
    "        \n",
    "        # Length filter\n",
    "        if len(instruction) + len(output) > MAX_CHAR_LENGTH:\n",
    "            continue\n",
    "        \n",
    "        # Check if this is a Kural-related Q&A\n",
    "        is_kural = any(\n",
    "            k in instruction + output\n",
    "            for k in [\"\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\", \"\\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\", \"\\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0bb5\\u0bb3\\u0bcd\\u0bb3\\u0bc1\\u0bb5\\u0bb0\\u0bcd\"]\n",
    "        )\n",
    "        \n",
    "        if is_kural:\n",
    "            # Apply anti-memorization filter\n",
    "            if is_verbatim_kural_qa(instruction, output):\n",
    "                pack_verbatim_rejected += 1\n",
    "                continue\n",
    "            pack_kural.append((instruction, output, pair.get(\"category\", \"\")))\n",
    "        else:\n",
    "            pack_domain.append((instruction, output, pair.get(\"category\", \"\")))\n",
    "    \n",
    "    domain_pack_samples.extend(pack_domain)\n",
    "    kural_interp_samples.extend(pack_kural)\n",
    "    \n",
    "    print(f\"  {pack_name}: {len(pack_domain)} domain, {len(pack_kural)} kural interpretive, {pack_verbatim_rejected} verbatim rejected\")\n",
    "\n",
    "print(f\"\\n\\u2705 Domain packs: {len(domain_pack_samples)} samples\")\n",
    "print(f\"\\u2705 Kural interpretive: {len(kural_interp_samples)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load IndicAlign Diversity Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream IndicAlign Tamil samples from HuggingFace\n",
    "# Subsets: Dolly_T, WikiHow, Wiki_Conv, OpenAssistant_T\n",
    "\n",
    "INDICALIGN_SUBSETS = [\"Dolly_T\", \"WikiHow\", \"Wiki_Conv\", \"OpenAssistant_T\"]\n",
    "TARGET_INDICALIGN = 1500  # Total samples to collect\n",
    "PER_SUBSET_TARGET = TARGET_INDICALIGN // len(INDICALIGN_SUBSETS)\n",
    "\n",
    "indicalign_samples = []\n",
    "\n",
    "for subset in INDICALIGN_SUBSETS:\n",
    "    try:\n",
    "        ds = load_dataset(\n",
    "            \"ai4bharat/indic-align\",\n",
    "            subset,\n",
    "            split=\"train\",\n",
    "            streaming=True,\n",
    "        )\n",
    "        \n",
    "        subset_samples = []\n",
    "        for item in ds:\n",
    "            if len(subset_samples) >= PER_SUBSET_TARGET:\n",
    "                break\n",
    "            \n",
    "            # IndicAlign has 'messages' field with role/content pairs\n",
    "            messages = item.get(\"messages\", [])\n",
    "            if not messages:\n",
    "                # Try alternate format\n",
    "                instruction = item.get(\"instruction\", item.get(\"input\", \"\"))\n",
    "                output = item.get(\"output\", item.get(\"response\", \"\"))\n",
    "            else:\n",
    "                # Extract user and assistant messages\n",
    "                instruction = \"\"\n",
    "                output = \"\"\n",
    "                for msg in messages:\n",
    "                    role = msg.get(\"role\", \"\")\n",
    "                    content = msg.get(\"content\", \"\")\n",
    "                    if role == \"user\" and not instruction:\n",
    "                        instruction = content\n",
    "                    elif role == \"assistant\" and not output:\n",
    "                        output = content\n",
    "            \n",
    "            if not instruction or not output:\n",
    "                continue\n",
    "            \n",
    "            # Length and Tamil quality filters\n",
    "            combined = instruction + output\n",
    "            if len(combined) > MAX_CHAR_LENGTH:\n",
    "                continue\n",
    "            if tamil_char_pct(combined) < MIN_TAMIL_PCT:\n",
    "                continue\n",
    "            \n",
    "            subset_samples.append((instruction, output, f\"indicalign_{subset.lower()}\"))\n",
    "        \n",
    "        indicalign_samples.extend(subset_samples)\n",
    "        print(f\"  {subset}: {len(subset_samples)} samples\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  {subset}: FAILED - {e}\")\n",
    "\n",
    "print(f\"\\n\\u2705 IndicAlign total: {len(indicalign_samples)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Handcrafted Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HANDCRAFTED_DIR = Path(\"../data/sources/sft/handcrafted\")\n",
    "if not HANDCRAFTED_DIR.exists():\n",
    "    HANDCRAFTED_DIR = Path(\"/kaggle/input/vazhi-sources/sft/handcrafted\")\n",
    "if not HANDCRAFTED_DIR.exists():\n",
    "    raise FileNotFoundError(\"Cannot find handcrafted samples directory.\")\n",
    "\n",
    "handcrafted_samples = []\n",
    "\n",
    "for hc_file in sorted(HANDCRAFTED_DIR.glob(\"*.json\")):\n",
    "    with open(hc_file, encoding=\"utf-8\") as f:\n",
    "        items = json.load(f)\n",
    "    \n",
    "    for item in items:\n",
    "        handcrafted_samples.append((\n",
    "            item[\"instruction\"],\n",
    "            item[\"output\"],\n",
    "            item.get(\"category\", hc_file.stem),\n",
    "        ))\n",
    "    \n",
    "    print(f\"  {hc_file.stem}: {len(items)} samples\")\n",
    "\n",
    "print(f\"\\n\\u2705 Handcrafted total: {len(handcrafted_samples)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load General Knowledge (LEGACY Q&A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select ~300 general knowledge Q&As from LEGACY files\n",
    "# These cover dialects, emotions, daily routines, weather, etc.\n",
    "\n",
    "LEGACY_DIR = Path(\"../data/LEGACY\")\n",
    "if not LEGACY_DIR.exists():\n",
    "    LEGACY_DIR = Path(\"/kaggle/input/vazhi-legacy\")\n",
    "\n",
    "GENERAL_FILES = [\n",
    "    \"06_health.json\",\n",
    "    \"09_weather.json\",\n",
    "    \"10_shopping.json\",\n",
    "    \"12_daily_routines.json\",\n",
    "    \"13_emotions.json\",\n",
    "    \"14_chennai_dialect.json\",\n",
    "    \"15_madurai_dialect.json\",\n",
    "    \"16_kongu_dialect.json\",\n",
    "    \"31_malaysia_dialect.json\",\n",
    "    \"03_numbers_time.json\",\n",
    "]\n",
    "\n",
    "TARGET_GENERAL = 350\n",
    "general_pool = []\n",
    "\n",
    "for fname in GENERAL_FILES:\n",
    "    fpath = LEGACY_DIR / fname\n",
    "    if not fpath.exists():\n",
    "        print(f\"  {fname}: NOT FOUND, skipping\")\n",
    "        continue\n",
    "    \n",
    "    with open(fpath, encoding=\"utf-8\") as f:\n",
    "        items = json.load(f)\n",
    "    \n",
    "    file_samples = []\n",
    "    for item in items:\n",
    "        instruction = item.get(\"instruction\", item.get(\"question\", \"\"))\n",
    "        output = item.get(\"output\", item.get(\"answer\", \"\"))\n",
    "        if not instruction or not output:\n",
    "            continue\n",
    "        if len(instruction) + len(output) > MAX_CHAR_LENGTH:\n",
    "            continue\n",
    "        category = item.get(\"category\", fname.replace(\".json\", \"\"))\n",
    "        file_samples.append((instruction, output, category))\n",
    "    \n",
    "    general_pool.extend(file_samples)\n",
    "    print(f\"  {fname}: {len(file_samples)} usable samples\")\n",
    "\n",
    "# Sample down to target\n",
    "if len(general_pool) > TARGET_GENERAL:\n",
    "    general_samples = random.sample(general_pool, TARGET_GENERAL)\n",
    "else:\n",
    "    general_samples = general_pool\n",
    "\n",
    "print(f\"\\n\\u2705 General knowledge: {len(general_samples)} samples (from {len(general_pool)} pool)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compose, Validate & Enforce Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_composition(buckets, targets):\n",
    "    \"\"\"Sample each bucket to target size. FAIL if any bucket is short of minimum.\"\"\"\n",
    "    total_target = sum(len(v) for v in buckets.values())\n",
    "    # Use domain_packs size to anchor the total\n",
    "    anchor_size = len(buckets[\"domain_packs\"])\n",
    "    estimated_total = int(anchor_size / targets[\"domain_packs\"][\"target\"])\n",
    "    \n",
    "    result = {}\n",
    "    for name, cfg in targets.items():\n",
    "        available = buckets.get(name, [])\n",
    "        target_count = int(estimated_total * cfg[\"target\"])\n",
    "        min_count = int(estimated_total * cfg[\"min\"])\n",
    "        max_count = int(estimated_total * cfg[\"max\"])\n",
    "        \n",
    "        if len(available) < min_count:\n",
    "            raise ValueError(\n",
    "                f\"FACTORY FAILED: '{name}' has {len(available)} samples, \"\n",
    "                f\"minimum required is {min_count} ({cfg['min']:.0%} of ~{estimated_total}). \"\n",
    "                f\"Add more data to this bucket before retrying.\"\n",
    "            )\n",
    "        \n",
    "        # Sample to target, respecting max\n",
    "        use_count = min(len(available), target_count, max_count)\n",
    "        use_count = max(use_count, min_count)  # At least minimum\n",
    "        \n",
    "        if len(available) > use_count:\n",
    "            result[name] = random.sample(available, use_count)\n",
    "        else:\n",
    "            result[name] = available\n",
    "        \n",
    "        pct = len(result[name]) / estimated_total * 100\n",
    "        print(f\"  {name}: {len(result[name])}/{len(available)} available ({pct:.1f}%)\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Build buckets\n",
    "buckets = {\n",
    "    \"domain_packs\": domain_pack_samples,\n",
    "    \"indicalign\": indicalign_samples,\n",
    "    \"kural_interp\": kural_interp_samples,\n",
    "    \"handcrafted\": handcrafted_samples,\n",
    "    \"general\": general_samples,\n",
    "}\n",
    "\n",
    "print(\"\\U0001f3af Enforcing composition targets...\")\n",
    "print(f\"   Raw bucket sizes:\")\n",
    "for name, samples in buckets.items():\n",
    "    print(f\"     {name}: {len(samples)}\")\n",
    "\n",
    "composed = enforce_composition(buckets, COMPOSITION_TARGETS)\n",
    "\n",
    "print(f\"\\n\\u2705 Composition enforcement passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all to ChatML and validate\n",
    "all_samples = []\n",
    "seen_instructions = set()  # Dedup by instruction prefix\n",
    "\n",
    "for bucket_name, samples in composed.items():\n",
    "    for instruction, output, category in samples:\n",
    "        # Dedup by first 100 chars of instruction\n",
    "        dedup_key = instruction[:100]\n",
    "        if dedup_key in seen_instructions:\n",
    "            continue\n",
    "        seen_instructions.add(dedup_key)\n",
    "        \n",
    "        # Convert to ChatML\n",
    "        text = to_chatml(instruction, output)\n",
    "        \n",
    "        # Validate ChatML\n",
    "        valid, reason = validate_chatml_strict(text)\n",
    "        if not valid:\n",
    "            print(f\"  REJECTED ({bucket_name}): {reason} - {instruction[:50]}\")\n",
    "            continue\n",
    "        \n",
    "        # Tamil percentage check (skip for handcrafted brevity/greeting)\n",
    "        if bucket_name not in [\"handcrafted\"] and tamil_char_pct(text) < MIN_TAMIL_PCT:\n",
    "            continue\n",
    "        \n",
    "        all_samples.append({\n",
    "            \"text\": text,\n",
    "            \"bucket\": bucket_name,\n",
    "            \"category\": category,\n",
    "        })\n",
    "\n",
    "# Shuffle\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "print(f\"\\n\\U0001f4ca Final dataset: {len(all_samples)} samples\")\n",
    "bucket_counts = Counter(s[\"bucket\"] for s in all_samples)\n",
    "for bucket, count in sorted(bucket_counts.items()):\n",
    "    pct = 100 * count / len(all_samples)\n",
    "    print(f\"  {bucket}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stratified Train/Eval Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90/10 stratified split by bucket\n",
    "EVAL_RATIO = 0.10\n",
    "\n",
    "train_samples = []\n",
    "eval_samples = []\n",
    "\n",
    "# Group by bucket\n",
    "by_bucket = {}\n",
    "for s in all_samples:\n",
    "    bucket = s[\"bucket\"]\n",
    "    if bucket not in by_bucket:\n",
    "        by_bucket[bucket] = []\n",
    "    by_bucket[bucket].append(s)\n",
    "\n",
    "for bucket, samples in by_bucket.items():\n",
    "    random.shuffle(samples)\n",
    "    n_eval = max(1, int(len(samples) * EVAL_RATIO))\n",
    "    eval_samples.extend(samples[:n_eval])\n",
    "    train_samples.extend(samples[n_eval:])\n",
    "\n",
    "random.shuffle(train_samples)\n",
    "random.shuffle(eval_samples)\n",
    "\n",
    "print(f\"\\U0001f4ca Stratified split:\")\n",
    "print(f\"  Train: {len(train_samples)}\")\n",
    "print(f\"  Eval:  {len(eval_samples)}\")\n",
    "print(f\"  Eval ratio: {len(eval_samples) / (len(train_samples) + len(eval_samples)):.1%}\")\n",
    "\n",
    "# Verify eval has all buckets\n",
    "eval_buckets = Counter(s[\"bucket\"] for s in eval_samples)\n",
    "print(f\"\\n  Eval bucket distribution:\")\n",
    "for bucket, count in sorted(eval_buckets.items()):\n",
    "    print(f\"    {bucket}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Upload to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_list(train_samples)\n",
    "eval_ds = Dataset.from_list(eval_samples)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": eval_ds,\n",
    "})\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(OUTPUT_DATASET, repo_type=\"dataset\", exist_ok=True)\n",
    "\n",
    "dataset_dict.push_to_hub(OUTPUT_DATASET)\n",
    "\n",
    "print(f\"\\n\\u2705 Dataset uploaded: https://huggingface.co/datasets/{OUTPUT_DATASET}\")\n",
    "print(f\"   Train: {len(train_ds)} samples\")\n",
    "print(f\"   Validation: {len(eval_ds)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Sample Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(f\"VAZHI Dataset Factory v{VERSION} \u2014 Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal samples: {len(all_samples)}\")\n",
    "print(f\"Train: {len(train_samples)} | Eval: {len(eval_samples)}\")\n",
    "print(f\"\\nComposition:\")\n",
    "\n",
    "total = len(all_samples)\n",
    "for bucket, count in sorted(bucket_counts.items()):\n",
    "    target = COMPOSITION_TARGETS[bucket]\n",
    "    pct = count / total\n",
    "    status = \"\\u2705\" if target[\"min\"] <= pct <= target[\"max\"] else \"\\u274c\"\n",
    "    print(f\"  {status} {bucket}: {count} ({pct:.1%}) [target: {target['target']:.0%}, range: {target['min']:.0%}-{target['max']:.0%}]\")\n",
    "\n",
    "# Show sample outputs from each bucket\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"Sample outputs (1 per bucket):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "shown_buckets = set()\n",
    "for s in all_samples:\n",
    "    if s[\"bucket\"] not in shown_buckets:\n",
    "        shown_buckets.add(s[\"bucket\"])\n",
    "        print(f\"\\n[{s['bucket'].upper()}] category={s['category']}\")\n",
    "        # Extract user and assistant from ChatML\n",
    "        match = CHATML_PATTERN.search(s[\"text\"])\n",
    "        if match:\n",
    "            print(f\"  Q: {match.group(1)[:100]}\")\n",
    "            print(f\"  A: {match.group(2)[:150]}\")\n",
    "    if len(shown_buckets) == len(COMPOSITION_TARGETS):\n",
    "        break\n",
    "\n",
    "print(f\"\\n\\u2705 Dataset Factory v{VERSION} complete!\")\n",
    "print(f\"   Dataset: {OUTPUT_DATASET}\")\n",
    "print(f\"   Next step: Use this dataset in training notebook\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
