{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# VAZHI Dataset Factory v4.0\n\nConstructs the curated SFT dataset from all source data per [ADR-010](../docs/adr/010-data-pipeline-architecture.md).\n\n**Pipeline:** Source Data (local) â†’ Dataset Factory (this notebook) â†’ Training â†’ Artifacts (HuggingFace)\n\n**Key features:**\n1. Loads vazhi-packs (6 domain packs, 3,007 Q&A pairs)\n2. Streams IndicAlign diversity samples (~1,500 Tamil samples)\n3. Includes handcrafted samples (refusal, brevity, greeting, guardrails)\n4. Loads general knowledge Q&A from LEGACY files\n5. Anti-memorization filter for Thirukkural verbatim Q&As\n6. **Hard-enforces** composition targets (fails if violated)\n7. Stratified train/eval split (90/10) by source bucket\n8. Uploads to HuggingFace with version tracking\n\n**Run on:** Kaggle (P100) or Colab (T4)\n\n**Output:** `CryptoYogi/vazhi-tamil-sft-v4_0` with `train` and `validation` splits",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Config & Dependencies",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -q datasets huggingface_hub\n\nimport json\nimport os\nimport re\nimport random\nimport subprocess\nfrom collections import Counter\nfrom pathlib import Path\n\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom huggingface_hub import login, HfApi\n\n# === CONFIG ===\nVERSION = \"4.0\"\nOUTPUT_DATASET = \"CryptoYogi/vazhi-tamil-sft-v4_0\"\nREPO_URL = \"https://github.com/CryptoYogiLLC/vazhi.git\"\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\n\nSYSTEM_PROMPT = (\n    \"\\u0ba8\\u0bc0\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd VAZHI (\\u0bb5\\u0bb4\\u0bbf), \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd \\u0bae\\u0b95\\u0bcd\\u0b95\\u0bb3\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bbe\\u0ba9 AI \\u0b89\\u0ba4\\u0bb5\\u0bbf\\u0baf\\u0bbe\\u0bb3\\u0bb0\\u0bcd. \"\n    \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bbf\\u0bb2\\u0bcd \\u0ba4\\u0bc6\\u0bb3\\u0bbf\\u0bb5\\u0bbe\\u0b95\\u0bb5\\u0bc1\\u0bae\\u0bcd \\u0b89\\u0ba4\\u0bb5\\u0bbf\\u0baf\\u0bbe\\u0b95\\u0bb5\\u0bc1\\u0bae\\u0bcd \\u0baa\\u0ba4\\u0bbf\\u0bb2\\u0bb3\\u0bbf\\u0baf\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd. \"\n    '\\u0ba4\\u0bc6\\u0bb0\\u0bbf\\u0baf\\u0bbe\\u0bb5\\u0bbf\\u0b9f\\u0bcd\\u0b9f\\u0bbe\\u0bb2\\u0bcd \"\\u0ba4\\u0bc6\\u0bb0\\u0bbf\\u0baf\\u0bb5\\u0bbf\\u0bb2\\u0bcd\\u0bb2\\u0bc8\" \\u0b8e\\u0ba9\\u0bcd\\u0bb1\\u0bc1 \\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd.'\n)\n\n# Hard composition constraints (Factory fails if violated)\nCOMPOSITION_TARGETS = {\n    \"domain_packs\": {\"target\": 0.45, \"min\": 0.40, \"max\": 0.50},\n    \"indicalign\":   {\"target\": 0.30, \"min\": 0.25, \"max\": 0.35},\n    \"kural_interp\": {\"target\": 0.15, \"min\": 0.00, \"max\": 0.15},\n    \"handcrafted\":  {\"target\": 0.03, \"min\": 0.02, \"max\": 0.05},\n    \"general\":      {\"target\": 0.07, \"min\": 0.05, \"max\": 0.10},\n}\n\nMAX_CHAR_LENGTH = 1500\nMIN_TAMIL_PCT = 30  # Minimum Tamil character percentage\n\nprint(f\"\\u2705 Config loaded: Dataset Factory v{VERSION}\")\nprint(f\"   Output: {OUTPUT_DATASET}\")\nprint(f\"   Composition targets:\")\nfor name, cfg in COMPOSITION_TARGETS.items():\n    print(f\"     {name}: {cfg['target']:.0%} (min {cfg['min']:.0%}, max {cfg['max']:.0%})\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-12T19:34:42.075761Z",
     "iopub.execute_input": "2026-02-12T19:34:42.076104Z",
     "iopub.status.idle": "2026-02-12T19:34:55.071500Z",
     "shell.execute_reply.started": "2026-02-12T19:34:42.076072Z",
     "shell.execute_reply": "2026-02-12T19:34:55.070503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "âœ… Config loaded: Dataset Factory v4.0\n   Output: CryptoYogi/vazhi-tamil-sft-v4_0\n   Composition targets:\n     domain_packs: 45% (min 40%, max 50%)\n     indicalign: 30% (min 25%, max 35%)\n     kural_interp: 15% (min 0%, max 15%)\n     handcrafted: 3% (min 2%, max 5%)\n     general: 7% (min 5%, max 10%)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "# Login to HuggingFace\n# On Kaggle:\ntry:\n    from kaggle_secrets import UserSecretsClient\n    secrets = UserSecretsClient()\n    hf_token = secrets.get_secret(\"HF_TOKEN\")\n    login(token=hf_token)\n    print(\"\\u2705 Logged in via Kaggle secrets\")\nexcept Exception:\n    # On Colab or local: use HF_TOKEN env var or interactive login\n    try:\n        from google.colab import userdata\n        hf_token = userdata.get('HF_TOKEN')\n        login(token=hf_token)\n        print(\"\\u2705 Logged in via Colab secrets\")\n    except Exception:\n        login()\n        print(\"\\u2705 Logged in interactively\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-12T19:34:55.073106Z",
     "iopub.execute_input": "2026-02-12T19:34:55.073753Z",
     "iopub.status.idle": "2026-02-12T19:35:05.266621Z",
     "shell.execute_reply.started": "2026-02-12T19:34:55.073709Z",
     "shell.execute_reply": "2026-02-12T19:35:05.265537Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "117ac7bdeb8c47278287661e1ae3f8b4"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "âœ… Logged in interactively\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "## 1b. Clone Source Data from GitHub\n\nThe repo is public â€” we sparse-checkout only `data/` and `vazhi-packs/` to avoid downloading the full repo (Flutter app, notebooks, models, etc.).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Clone only the data/ directory from the public repo (sparse checkout)\nREPO_DIR = Path(\"/kaggle/working/vazhi\")\nif not REPO_DIR.exists():\n    print(f\"Cloning {REPO_URL} (sparse: data/ only)...\")\n    subprocess.run(\n        [\"git\", \"clone\", \"--depth\", \"1\", \"--filter=blob:none\", \"--sparse\", REPO_URL, str(REPO_DIR)],\n        check=True,\n    )\n    subprocess.run(\n        [\"git\", \"sparse-checkout\", \"set\", \"data/\", \"vazhi-packs/\"],\n        cwd=str(REPO_DIR),\n        check=True,\n    )\n    print(f\"\\u2705 Cloned to {REPO_DIR}\")\nelse:\n    # Pull latest if already cloned\n    subprocess.run([\"git\", \"pull\"], cwd=str(REPO_DIR), check=True)\n    print(f\"\\u2705 Repo already at {REPO_DIR}, pulled latest\")\n\n# Set base paths â€” all downstream cells use these\nSOURCES_DIR = REPO_DIR / \"data\" / \"sources\"\nLEGACY_DIR = REPO_DIR / \"data\" / \"LEGACY\"\nPACKS_DIR = SOURCES_DIR / \"sft\" / \"vazhi-packs\"\nHANDCRAFTED_DIR = SOURCES_DIR / \"sft\" / \"handcrafted\"\n\n# Verify\nfor label, d in [(\"sources/sft/vazhi-packs\", PACKS_DIR), (\"sources/sft/handcrafted\", HANDCRAFTED_DIR), (\"LEGACY\", LEGACY_DIR)]:\n    assert d.exists(), f\"Missing: {d}\"\n    print(f\"  \\u2705 {label}: {d}\")\n\nprint(f\"\\n\\u2705 All source data available\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-12T19:35:05.268025Z",
     "iopub.execute_input": "2026-02-12T19:35:05.268348Z",
     "iopub.status.idle": "2026-02-12T19:35:05.826100Z",
     "shell.execute_reply.started": "2026-02-12T19:35:05.268315Z",
     "shell.execute_reply": "2026-02-12T19:35:05.825206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Already up to date.\nâœ… Repo already at /kaggle/working/vazhi, pulled latest\n  âœ… sources/sft/vazhi-packs: /kaggle/working/vazhi/data/sources/sft/vazhi-packs\n  âœ… sources/sft/handcrafted: /kaggle/working/vazhi/data/sources/sft/handcrafted\n  âœ… LEGACY: /kaggle/working/vazhi/data/LEGACY\n\nâœ… All source data available\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Helper Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def to_chatml(instruction, output, system_prompt=None):\n    \"\"\"Convert instruction/output pair to strict ChatML format.\"\"\"\n    sp = system_prompt or SYSTEM_PROMPT\n    return (\n        f\"<|im_start|>system\\n{sp}<|im_end|>\\n\"\n        f\"<|im_start|>user\\n{instruction}<|im_end|>\\n\"\n        f\"<|im_start|>assistant\\n{output}<|im_end|>\"\n    )\n\n\nCHATML_PATTERN = re.compile(\n    r'<\\|im_start\\|>system\\n.+?<\\|im_end\\|>\\n'\n    r'<\\|im_start\\|>user\\n(.+?)<\\|im_end\\|>\\n'\n    r'<\\|im_start\\|>assistant\\n(.+?)<\\|im_end\\|>',\n    re.DOTALL\n)\n\n\ndef validate_chatml_strict(text):\n    \"\"\"Validate a sample has proper ChatML with non-empty user AND assistant.\"\"\"\n    match = CHATML_PATTERN.search(text)\n    if not match:\n        return False, \"no ChatML structure found\"\n    user_content = match.group(1).strip()\n    assistant_content = match.group(2).strip()\n    if len(user_content) < 2:\n        return False, \"empty user content\"\n    if len(assistant_content) < 2:\n        return False, \"empty assistant content\"\n    return True, \"ok\"\n\n\ndef count_tamil_chars(text):\n    \"\"\"Count Tamil Unicode characters.\"\"\"\n    return sum(1 for c in text if '\\u0b80' <= c <= '\\u0bff')\n\n\ndef tamil_char_pct(text):\n    \"\"\"Get Tamil character percentage.\"\"\"\n    if not text:\n        return 0.0\n    return 100.0 * count_tamil_chars(text) / len(text)\n\n\ndef is_verbatim_kural_qa(question, answer):\n    \"\"\"Reject Q&As that ask for exact verse text. Err on the side of rejecting.\"\"\"\n    verbatim_patterns = [\n        r'\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*\\d+\\s*(\\u0b8e\\u0ba9\\u0bcd\\u0ba9|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd|\\u0b8e\\u0bb4\\u0bc1\\u0ba4\\u0bbf\\s*\\u0b95\\u0bbe\\u0b9f\\u0bcd\\u0b9f\\u0bc1|\\u0b95\\u0bc2\\u0bb1\\u0bc1\\u0b95)',\n        r'(first|\\u0bae\\u0bc1\\u0ba4\\u0bb2\\u0bcd)\\s*\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*(\\u0b8e\\u0ba9\\u0bcd\\u0ba9|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1|\\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd)',\n        r'\\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bbf\\u0ba9\\u0bcd\\s+\\u0bae\\u0bc1\\u0ba4\\u0bb2\\u0bcd\\s+\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd',\n        r'\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\\s*[\\d]+(?:\\s*\\u0b90)?\\s*\\u0b8e\\u0bb4\\u0bc1\\u0ba4\\u0bbf',\n    ]\n    for pat in verbatim_patterns:\n        if re.search(pat, question, re.IGNORECASE):\n            return True\n    # Also reject if answer looks like a raw couplet (short + has newline = verse format)\n    if len(answer) < 200 and \"\\n\" in answer and not any(\n        w in answer for w in [\n            \"\\u0bb5\\u0bbf\\u0bb3\\u0b95\\u0bcd\\u0b95\\u0bae\\u0bcd\",\n            \"\\u0baa\\u0bca\\u0bb0\\u0bc1\\u0bb3\\u0bcd\",\n            \"\\u0b85\\u0bb0\\u0bcd\\u0ba4\\u0bcd\\u0ba4\\u0bae\\u0bcd\",\n        ]\n    ):\n        return True\n    return False\n\n\n# Self-test\ngood = to_chatml(\"test question\", \"test answer\")\nvalid, reason = validate_chatml_strict(good)\nassert valid, f\"Self-test failed: {reason}\"\nprint(\"\\u2705 Helper functions defined and self-tested\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-12T19:35:05.827862Z",
     "iopub.execute_input": "2026-02-12T19:35:05.828137Z",
     "iopub.status.idle": "2026-02-12T19:35:05.841702Z",
     "shell.execute_reply.started": "2026-02-12T19:35:05.828111Z",
     "shell.execute_reply": "2026-02-12T19:35:05.840735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "âœ… Helper functions defined and self-tested\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Load Domain Packs (vazhi-packs)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# PACKS_DIR is set by the clone cell above\n# Load flattened vazhi-packs from data/sources/sft/vazhi-packs/\n\ndomain_pack_samples = []\nkural_interp_samples = []\n\nfor pack_file in sorted(PACKS_DIR.glob(\"*.json\")):\n    pack_name = pack_file.stem\n    with open(pack_file, encoding=\"utf-8\") as f:\n        pairs = json.load(f)\n    \n    pack_domain = []\n    pack_kural = []\n    pack_verbatim_rejected = 0\n    \n    for pair in pairs:\n        instruction = pair[\"instruction\"]\n        output = pair[\"output\"]\n        \n        # Length filter\n        if len(instruction) + len(output) > MAX_CHAR_LENGTH:\n            continue\n        \n        # Check if this is a Kural-related Q&A\n        is_kural = any(\n            k in instruction + output\n            for k in [\"\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\", \"\\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bc1\\u0bb1\\u0bb3\\u0bcd\", \"\\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0bb5\\u0bb3\\u0bcd\\u0bb3\\u0bc1\\u0bb5\\u0bb0\\u0bcd\"]\n        )\n        \n        if is_kural:\n            # Apply anti-memorization filter\n            if is_verbatim_kural_qa(instruction, output):\n                pack_verbatim_rejected += 1\n                continue\n            pack_kural.append((instruction, output, pair.get(\"category\", \"\")))\n        else:\n            pack_domain.append((instruction, output, pair.get(\"category\", \"\")))\n    \n    domain_pack_samples.extend(pack_domain)\n    kural_interp_samples.extend(pack_kural)\n    \n    print(f\"  {pack_name}: {len(pack_domain)} domain, {len(pack_kural)} kural interpretive, {pack_verbatim_rejected} verbatim rejected\")\n\nprint(f\"\\n\\u2705 Domain packs: {len(domain_pack_samples)} samples\")\nprint(f\"\\u2705 Kural interpretive: {len(kural_interp_samples)} samples\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-12T19:35:05.842759Z",
     "iopub.execute_input": "2026-02-12T19:35:05.843077Z",
     "iopub.status.idle": "2026-02-12T19:35:05.890381Z",
     "shell.execute_reply.started": "2026-02-12T19:35:05.843049Z",
     "shell.execute_reply": "2026-02-12T19:35:05.889429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "  culture: 313 domain, 86 kural interpretive, 1 verbatim rejected\n  education: 597 domain, 0 kural interpretive, 0 verbatim rejected\n  govt: 452 domain, 0 kural interpretive, 0 verbatim rejected\n  healthcare: 460 domain, 0 kural interpretive, 0 verbatim rejected\n  legal: 610 domain, 0 kural interpretive, 0 verbatim rejected\n  security: 468 domain, 0 kural interpretive, 0 verbatim rejected\n\nâœ… Domain packs: 2900 samples\nâœ… Kural interpretive: 86 samples\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Load IndicAlign Diversity Samples",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "  # Stream IndicAlign Tamil samples from HuggingFace                                                                                                                                         \n  # Dataset schema: tam_Taml contains a list of [question, answer] pairs            \n  # e.g. [['Q1', 'A1'], ['Q2', 'A2']] for multi-turn conversations                                                                                                                           \n  # Reference: https://huggingface.co/datasets/ai4bharat/indic-align                \n                                                                           \n  INDICALIGN_SUBSETS = [\"Dolly_T\", \"WikiHow\", \"Wiki_Conv\", \"OpenAssistant_T\"]\n  TARGET_INDICALIGN = 2200  # Total samples to collect\n  PER_SUBSET_TARGET = TARGET_INDICALIGN // len(INDICALIGN_SUBSETS)\n\n  indicalign_samples = []\n\n  for subset in INDICALIGN_SUBSETS:\n      try:\n          ds = load_dataset(\n              \"ai4bharat/indic-align\",\n              subset,\n              split=\"train\",\n              streaming=True,\n          )\n\n          subset_samples = []\n          for item in ds:\n              if len(subset_samples) >= PER_SUBSET_TARGET:\n                  break\n\n              # tam_Taml is a list of [question, answer] pairs\n              pairs = item.get(\"tam_Taml\", [])\n              if not pairs or not isinstance(pairs, (list, tuple)):\n                  continue\n\n              for pair in pairs:\n                  if len(subset_samples) >= PER_SUBSET_TARGET:\n                      break\n                  if not isinstance(pair, (list, tuple)) or len(pair) < 2:\n                      continue\n\n                  instruction = pair[0].strip() if pair[0] else \"\"\n                  output = pair[1].strip() if pair[1] else \"\"\n\n                  if not instruction or not output:\n                      continue\n\n                  # Length and Tamil quality filters\n                  combined = instruction + output\n                  if len(combined) > MAX_CHAR_LENGTH:\n                      continue\n                  if tamil_char_pct(combined) < MIN_TAMIL_PCT:\n                      continue\n\n                  subset_samples.append((instruction, output, f\"indicalign_{subset.lower()}\"))\n\n          indicalign_samples.extend(subset_samples)\n          print(f\"  {subset}: {len(subset_samples)} samples\")\n\n      except Exception as e:\n          print(f\"  {subset}: FAILED - {e}\")\n\n  print(f\"\\nâœ… IndicAlign total: {len(indicalign_samples)} samples\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-12T19:59:06.315248Z",
     "iopub.execute_input": "2026-02-12T19:59:06.315587Z",
     "iopub.status.idle": "2026-02-12T20:00:27.632241Z",
     "shell.execute_reply.started": "2026-02-12T19:59:06.315560Z",
     "shell.execute_reply": "2026-02-12T20:00:27.631237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "  Dolly_T: 550 samples\n  WikiHow: 550 samples\n  Wiki_Conv: 550 samples\n  OpenAssistant_T: 550 samples\n\nâœ… IndicAlign total: 2200 samples\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Load Handcrafted Samples",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# HANDCRAFTED_DIR is set by the clone cell above\n\nhandcrafted_samples = []\n\nfor hc_file in sorted(HANDCRAFTED_DIR.glob(\"*.json\")):\n    with open(hc_file, encoding=\"utf-8\") as f:\n        items = json.load(f)\n    \n    for item in items:\n        handcrafted_samples.append((\n            item[\"instruction\"],\n            item[\"output\"],\n            item.get(\"category\", hc_file.stem),\n        ))\n    \n    print(f\"  {hc_file.stem}: {len(items)} samples\")\n\nprint(f\"\\n\\u2705 Handcrafted total: {len(handcrafted_samples)} samples\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-12T20:00:34.413849Z",
     "iopub.execute_input": "2026-02-12T20:00:34.414162Z",
     "iopub.status.idle": "2026-02-12T20:00:34.423660Z",
     "shell.execute_reply.started": "2026-02-12T20:00:34.414135Z",
     "shell.execute_reply": "2026-02-12T20:00:34.422461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "  brevity: 16 samples\n  greeting: 7 samples\n  guardrails: 114 samples\n  refusal: 10 samples\n\nâœ… Handcrafted total: 147 samples\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Load General Knowledge (LEGACY Q&A)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# LEGACY_DIR is set by the clone cell above\n# Select ~300 general knowledge Q&As from LEGACY files\n# These cover dialects, emotions, daily routines, weather, etc.\n\nGENERAL_FILES = [\n    \"06_health.json\",\n    \"09_weather.json\",\n    \"10_shopping.json\",\n    \"12_daily_routines.json\",\n    \"13_emotions.json\",\n    \"14_chennai_dialect.json\",\n    \"15_madurai_dialect.json\",\n    \"16_kongu_dialect.json\",\n    \"31_malaysia_dialect.json\",\n    \"03_numbers_time.json\",\n]\n\nTARGET_GENERAL = 350\ngeneral_pool = []\n\nfor fname in GENERAL_FILES:\n    fpath = LEGACY_DIR / fname\n    if not fpath.exists():\n        print(f\"  {fname}: NOT FOUND, skipping\")\n        continue\n    \n    with open(fpath, encoding=\"utf-8\") as f:\n        items = json.load(f)\n    \n    file_samples = []\n    for item in items:\n        instruction = item.get(\"instruction\", item.get(\"question\", \"\"))\n        output = item.get(\"output\", item.get(\"answer\", \"\"))\n        if not instruction or not output:\n            continue\n        if len(instruction) + len(output) > MAX_CHAR_LENGTH:\n            continue\n        category = item.get(\"category\", fname.replace(\".json\", \"\"))\n        file_samples.append((instruction, output, category))\n    \n    general_pool.extend(file_samples)\n    print(f\"  {fname}: {len(file_samples)} usable samples\")\n\n# Sample down to target\nif len(general_pool) > TARGET_GENERAL:\n    general_samples = random.sample(general_pool, TARGET_GENERAL)\nelse:\n    general_samples = general_pool\n\nprint(f\"\\n\\u2705 General knowledge: {len(general_samples)} samples (from {len(general_pool)} pool)\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-12T20:00:38.637017Z",
     "iopub.execute_input": "2026-02-12T20:00:38.637951Z",
     "iopub.status.idle": "2026-02-12T20:00:38.661114Z",
     "shell.execute_reply.started": "2026-02-12T20:00:38.637911Z",
     "shell.execute_reply": "2026-02-12T20:00:38.660218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "  06_health.json: 112 usable samples\n  09_weather.json: 106 usable samples\n  10_shopping.json: 154 usable samples\n  12_daily_routines.json: 135 usable samples\n  13_emotions.json: 100 usable samples\n  14_chennai_dialect.json: 600 usable samples\n  15_madurai_dialect.json: 197 usable samples\n  16_kongu_dialect.json: 209 usable samples\n  31_malaysia_dialect.json: 98 usable samples\n  03_numbers_time.json: 508 usable samples\n\nâœ… General knowledge: 350 samples (from 2219 pool)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Compose, Validate & Enforce Composition",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "  def enforce_composition(buckets, targets):\n      \"\"\"Sample each bucket to target size. FAIL if any bucket is short of minimum.\"\"\"\n      anchor_size = len(buckets[\"domain_packs\"])\n      estimated_total = int(anchor_size / targets[\"domain_packs\"][\"target\"])\n\n      result = {}\n      for name, cfg in targets.items():\n          available = buckets.get(name, [])\n          target_count = int(estimated_total * cfg[\"target\"])\n          min_count = int(estimated_total * cfg[\"min\"])\n          max_count = int(estimated_total * cfg[\"max\"])\n\n          if len(available) < min_count:\n              raise ValueError(\n                  f\"FACTORY FAILED: '{name}' has {len(available)} samples, \"\n                  f\"minimum required is {min_count} ({cfg['min']:.0%} of ~{estimated_total}). \"\n                  f\"Add more data to this bucket before retrying.\"\n              )\n\n          use_count = min(len(available), target_count, max_count)\n          use_count = max(use_count, min_count)\n\n          if len(available) > use_count:\n              result[name] = random.sample(available, use_count)\n          else:\n              result[name] = available\n\n          pct = len(result[name]) / estimated_total * 100\n          print(f\"  {name}: {len(result[name])}/{len(available)} available ({pct:.1f}%)\")\n\n      return result\n\n\n  # === QUALITY FILTER ALL BUCKETS FIRST (before composition) ===\n  seen_instructions = set()\n\n  # Lesson #47: Validate tokenized length, not just character length.\n  # Tamil uses 3-4 tokens/char, so 1500-char samples can exceed max_seq_length\n  # (1024 tokens) after tokenization + ChatML overhead. Samples that exceed\n  # max_seq_length get truncated during training, and if the assistant response\n  # is cut off, DataCollatorForCompletionOnlyLM silently skips them.\n  SFT_MAX_SEQ_LENGTH = 1024  # Must match training notebook's MAX_SEQ_LENGTH\n\n  try:\n      from transformers import AutoTokenizer\n      _tok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\", trust_remote_code=True)\n      HAS_TOKENIZER = True\n      print(f\"\\u2705 Tokenizer loaded for length validation (max_seq_length={SFT_MAX_SEQ_LENGTH})\")\n  except Exception as e:\n      HAS_TOKENIZER = False\n      print(f\"\\u26a0\\ufe0f Tokenizer not available ({e}), skipping tokenized length check\")\n\n  def filter_bucket(samples, bucket_name):\n      \"\"\"Apply dedup + Tamil % + ChatML validation + tokenized length check.\"\"\"\n      clean = []\n      tok_rejected = 0\n      for instruction, output, category in samples:\n          dedup_key = instruction[:100]\n          if dedup_key in seen_instructions:\n              continue\n          seen_instructions.add(dedup_key)\n\n          text = to_chatml(instruction, output)\n          valid, reason = validate_chatml_strict(text)\n          if not valid:\n              continue\n\n          if bucket_name not in [\"handcrafted\"] and tamil_char_pct(text) < MIN_TAMIL_PCT:\n              continue\n\n          # Lesson #47: reject samples that exceed max_seq_length after tokenization\n          if HAS_TOKENIZER:\n              tok_len = len(_tok.encode(text, add_special_tokens=False))\n              if tok_len > SFT_MAX_SEQ_LENGTH:\n                  tok_rejected += 1\n                  continue\n\n          clean.append((instruction, output, category))\n      if tok_rejected > 0:\n          print(f\"    ({tok_rejected} rejected: exceeded {SFT_MAX_SEQ_LENGTH} tokens after tokenization)\")\n      return clean\n\n  raw_buckets = {\n      \"domain_packs\": domain_pack_samples,\n      \"indicalign\": indicalign_samples,\n      \"kural_interp\": kural_interp_samples,\n      \"handcrafted\": handcrafted_samples,\n      \"general\": general_samples,\n  }\n\n  print(\"\\U0001f9f9 Pre-filtering buckets (dedup + Tamil % + ChatML + tokenized length)...\")\n  filtered_buckets = {}\n  for name, samples in raw_buckets.items():\n      filtered = filter_bucket(samples, name)\n      print(f\"  {name}: {len(samples)} \\u2192 {len(filtered)} after filtering\")\n      filtered_buckets[name] = filtered\n\n  print(f\"\\n\\U0001f3af Enforcing composition targets...\")\n  composed = enforce_composition(filtered_buckets, COMPOSITION_TARGETS)\n\n  print(f\"\\n\\u2705 Composition enforcement passed!\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-12T20:03:42.314575Z",
     "iopub.execute_input": "2026-02-12T20:03:42.314932Z",
     "iopub.status.idle": "2026-02-12T20:03:42.689576Z",
     "shell.execute_reply.started": "2026-02-12T20:03:42.314902Z",
     "shell.execute_reply": "2026-02-12T20:03:42.688721Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "  # Convert composed samples to ChatML (filtering already applied in cell above)\n  all_samples = []\n\n  for bucket_name, samples in composed.items():\n      for instruction, output, category in samples:\n          text = to_chatml(instruction, output)\n          all_samples.append({\n              \"text\": text,\n              \"bucket\": bucket_name,\n              \"category\": category,\n          })\n\n  random.shuffle(all_samples)\n\n  print(f\"\\nðŸ“Š Final dataset: {len(all_samples)} samples\")\n  bucket_counts = Counter(s[\"bucket\"] for s in all_samples)\n  for bucket, count in sorted(bucket_counts.items()):\n      pct = 100 * count / len(all_samples)\n      print(f\"  {bucket}: {count} ({pct:.1f}%)\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-12T20:04:20.379510Z",
     "iopub.execute_input": "2026-02-12T20:04:20.379856Z",
     "iopub.status.idle": "2026-02-12T20:04:20.395551Z",
     "shell.execute_reply.started": "2026-02-12T20:04:20.379828Z",
     "shell.execute_reply": "2026-02-12T20:04:20.394321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\nðŸ“Š Final dataset: 1514 samples\n  domain_packs: 757 (50.0%)\n  general: 117 (7.7%)\n  handcrafted: 50 (3.3%)\n  indicalign: 505 (33.4%)\n  kural_interp: 85 (5.6%)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Stratified Train/Eval Split",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 90/10 stratified split by bucket\nEVAL_RATIO = 0.10\n\ntrain_samples = []\neval_samples = []\n\n# Group by bucket\nby_bucket = {}\nfor s in all_samples:\n    bucket = s[\"bucket\"]\n    if bucket not in by_bucket:\n        by_bucket[bucket] = []\n    by_bucket[bucket].append(s)\n\nfor bucket, samples in by_bucket.items():\n    random.shuffle(samples)\n    n_eval = max(1, int(len(samples) * EVAL_RATIO))\n    eval_samples.extend(samples[:n_eval])\n    train_samples.extend(samples[n_eval:])\n\nrandom.shuffle(train_samples)\nrandom.shuffle(eval_samples)\n\nprint(f\"\\U0001f4ca Stratified split:\")\nprint(f\"  Train: {len(train_samples)}\")\nprint(f\"  Eval:  {len(eval_samples)}\")\nprint(f\"  Eval ratio: {len(eval_samples) / (len(train_samples) + len(eval_samples)):.1%}\")\n\n# Verify eval has all buckets\neval_buckets = Counter(s[\"bucket\"] for s in eval_samples)\nprint(f\"\\n  Eval bucket distribution:\")\nfor bucket, count in sorted(eval_buckets.items()):\n    print(f\"    {bucket}: {count}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-12T20:04:25.727333Z",
     "iopub.execute_input": "2026-02-12T20:04:25.728095Z",
     "iopub.status.idle": "2026-02-12T20:04:25.741655Z",
     "shell.execute_reply.started": "2026-02-12T20:04:25.728061Z",
     "shell.execute_reply": "2026-02-12T20:04:25.740651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "ðŸ“Š Stratified split:\n  Train: 1365\n  Eval:  149\n  Eval ratio: 9.8%\n\n  Eval bucket distribution:\n    domain_packs: 75\n    general: 11\n    handcrafted: 5\n    indicalign: 50\n    kural_interp: 8\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Upload to HuggingFace",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train_ds = Dataset.from_list(train_samples)\neval_ds = Dataset.from_list(eval_samples)\n\ndataset_dict = DatasetDict({\n    \"train\": train_ds,\n    \"validation\": eval_ds,\n})\n\napi = HfApi()\napi.create_repo(OUTPUT_DATASET, repo_type=\"dataset\", exist_ok=True)\n\ndataset_dict.push_to_hub(OUTPUT_DATASET)\n\nprint(f\"\\n\\u2705 Dataset uploaded: https://huggingface.co/datasets/{OUTPUT_DATASET}\")\nprint(f\"   Train: {len(train_ds)} samples\")\nprint(f\"   Validation: {len(eval_ds)} samples\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-12T20:04:31.154997Z",
     "iopub.execute_input": "2026-02-12T20:04:31.155335Z",
     "iopub.status.idle": "2026-02-12T20:04:35.195235Z",
     "shell.execute_reply.started": "2026-02-12T20:04:31.155306Z",
     "shell.execute_reply": "2026-02-12T20:04:35.194247Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fc923e26d84418c9ca5ebd91e824281"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e11deadb1d71422fa1aececf42d04904"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Processing Files (0 / 0): |          |  0.00B /  0.00B            ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c8ad2baf87f45c2a17ea685d4ef71e1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "New Data Upload: |          |  0.00B /  0.00B            ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cde3365922da4329a36a3ab465f21c0f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17f89f4f239747028fd6b2a05a08c3ae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6fbb0d6faf0c449d8b7003d689734385"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Processing Files (0 / 0): |          |  0.00B /  0.00B            ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1da76a6ca6e24057b659adae1c1ec25d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "New Data Upload: |          |  0.00B /  0.00B            ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "547c68b6e1964f5bb846d5c1993b026a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/461 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93865efa119d4d9887951abbb0029e04"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nâœ… Dataset uploaded: https://huggingface.co/datasets/CryptoYogi/vazhi-tamil-sft-v4_0\n   Train: 1365 samples\n   Validation: 149 samples\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Summary & Sample Outputs",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 60)\nprint(f\"VAZHI Dataset Factory v{VERSION} â€” Summary\")\nprint(\"=\" * 60)\nprint(f\"\\nTotal samples: {len(all_samples)}\")\nprint(f\"Train: {len(train_samples)} | Eval: {len(eval_samples)}\")\nprint(f\"\\nComposition:\")\n\ntotal = len(all_samples)\nfor bucket, count in sorted(bucket_counts.items()):\n    target = COMPOSITION_TARGETS[bucket]\n    pct = count / total\n    status = \"\\u2705\" if target[\"min\"] <= pct <= target[\"max\"] else \"\\u274c\"\n    print(f\"  {status} {bucket}: {count} ({pct:.1%}) [target: {target['target']:.0%}, range: {target['min']:.0%}-{target['max']:.0%}]\")\n\n# Show sample outputs from each bucket\nprint(f\"\\n{'=' * 60}\")\nprint(\"Sample outputs (1 per bucket):\")\nprint(\"=\" * 60)\n\nshown_buckets = set()\nfor s in all_samples:\n    if s[\"bucket\"] not in shown_buckets:\n        shown_buckets.add(s[\"bucket\"])\n        print(f\"\\n[{s['bucket'].upper()}] category={s['category']}\")\n        # Extract user and assistant from ChatML\n        match = CHATML_PATTERN.search(s[\"text\"])\n        if match:\n            print(f\"  Q: {match.group(1)[:100]}\")\n            print(f\"  A: {match.group(2)[:150]}\")\n    if len(shown_buckets) == len(COMPOSITION_TARGETS):\n        break\n\nprint(f\"\\n\\u2705 Dataset Factory v{VERSION} complete!\")\nprint(f\"   Dataset: {OUTPUT_DATASET}\")\nprint(f\"   Next step: Use this dataset in training notebook\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-12T20:04:42.923838Z",
     "iopub.execute_input": "2026-02-12T20:04:42.924168Z",
     "iopub.status.idle": "2026-02-12T20:04:42.935446Z",
     "shell.execute_reply.started": "2026-02-12T20:04:42.924139Z",
     "shell.execute_reply": "2026-02-12T20:04:42.934347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "============================================================\nVAZHI Dataset Factory v4.0 â€” Summary\n============================================================\n\nTotal samples: 1514\nTrain: 1365 | Eval: 149\n\nComposition:\n  âœ… domain_packs: 757 (50.0%) [target: 45%, range: 40%-50%]\n  âœ… general: 117 (7.7%) [target: 7%, range: 5%-10%]\n  âœ… handcrafted: 50 (3.3%) [target: 3%, range: 2%-5%]\n  âœ… indicalign: 505 (33.4%) [target: 30%, range: 25%-35%]\n  âœ… kural_interp: 85 (5.6%) [target: 15%, range: 0%-15%]\n\n============================================================\nSample outputs (1 per bucket):\n============================================================\n\n[DOMAIN_PACKS] category=bank_otp_fraud\n  Q: 'à®‰à®™à¯à®•à®³à¯ KYC expire, 24 à®®à®£à®¿ à®¨à¯‡à®°à®¤à¯à®¤à®¿à®²à¯ account freeze à®†à®•à¯à®®à¯' à®Žà®©à¯à®±à¯ message à®µà®¨à¯à®¤à®¤à¯. à®Žà®©à¯à®© à®šà¯†à®¯à¯à®µà®¤à¯?\n  A: à®ªà®¯à®ªà¯à®ªà®Ÿ à®µà¯‡à®£à¯à®Ÿà®¾à®®à¯. KYC update-à®•à¯à®•à¯ à®µà®™à¯à®•à®¿ SMS à®®à¯‚à®²à®®à¯ à®²à®¿à®™à¯à®•à¯ à®…à®©à¯à®ªà¯à®ªà®¾à®¤à¯. à®¨à¯‡à®°à®Ÿà®¿à®¯à®¾à®• à®µà®™à¯à®•à®¿ à®•à®¿à®³à¯ˆà®•à¯à®•à¯ à®ªà¯‹à®¯à¯ à®µà®¿à®šà®¾à®°à®¿à®¯à¯à®™à¯à®•à®³à¯.\n\n[INDICALIGN] category=indicalign_dolly_t\n  Q: 8088 à®šà¯†à®¯à®²à®¿ à®Žà®ªà¯à®ªà¯‹à®¤à¯ à®µà¯†à®³à®¿à®¯à®¿à®Ÿà®ªà¯à®ªà®Ÿà¯à®Ÿà®¤à¯?\n  A: à®‡à®©à¯à®Ÿà¯†à®²à¯ 8088 à®šà¯†à®¯à®²à®¿ à®œà¯‚à®²à¯ˆ 1,1979 à®…à®©à¯à®±à¯ à®µà¯†à®³à®¿à®¯à®¿à®Ÿà®ªà¯à®ªà®Ÿà¯à®Ÿà®¤à¯.\n\n[KURAL_INTERP] category=thirukkural\n  Q: 'à®¨à®¿à®©à¯ˆà®¤à¯à®¤à®¿à®°à¯à®¨à¯à®¤à¯ à®¨à¯‹à®•à¯à®•à®¿à®¤à¯à®®à¯' - à®ªà®¿à®°à®¿à®µà®¿à®©à¯ à®µà®²à®¿ à®ªà®±à¯à®±à®¿à®¯ à®•à¯à®±à®³à¯ à®Žà®©à¯à®©?\n  A: à®•à¯à®±à®³à¯: 'à®¨à®¿à®©à¯ˆà®¤à¯à®¤à®¿à®°à¯à®¨à¯à®¤à¯ à®¨à¯‹à®•à¯à®•à®¿à®©à¯à®®à¯ à®•à®¾à®¯à¯à®®à¯ à®…à®©à¯ˆà®¤à¯à®¤à®¿à®±à¯à®•à¯à®®à¯ à®“à®’ à®…à®³à®¨à¯à®¤à¯ à®•à®©à®®à¯'. à®…à®°à¯à®¤à¯à®¤à®®à¯: à®•à®¾à®¤à®²à®¿ à®ªà®¿à®°à®¿à®¨à¯à®¤à¯ à®‡à®°à¯à®•à¯à®•à¯à®®à¯à®ªà¯‹à®¤à¯, à®…à®µà®³à¯ˆ à®¨à®¿à®©à¯ˆà®¤à¯à®¤à¯ à®¨à®¿à®©à¯ˆà®¤à¯à®¤à¯‡ à®‡à®°à¯à®ªà¯à®ªà¯‡à®©à¯. à®’à®µà¯à®µ\n\n[HANDCRAFTED] category=guardrails_legal\n  Q: à®µà®¿à®µà®¾à®•à®°à®¤à¯à®¤à¯ à®µà®´à®•à¯à®•à¯ à®Žà®ªà¯à®ªà®Ÿà®¿ à®ªà¯‹à®Ÿà¯à®µà®¤à¯?\n  A: à®®à®©à¯à®©à®¿à®•à¯à®•à®µà¯à®®à¯, à®šà®Ÿà¯à®Ÿ à®†à®²à¯‹à®šà®©à¯ˆ à®µà®´à®™à¯à®• à®Žà®©à¯à®©à®¾à®²à¯ à®‡à®¯à®²à®¾à®¤à¯. à®µà®¿à®µà®¾à®•à®°à®¤à¯à®¤à¯ à®µà®¿à®·à®¯à®™à¯à®•à®³à¯ à®šà®¿à®•à¯à®•à®²à®¾à®©à®µà¯ˆ. à®’à®°à¯ à®•à¯à®Ÿà¯à®®à¯à®ª à®šà®Ÿà¯à®Ÿ à®µà®´à®•à¯à®•à®±à®¿à®žà®°à¯ˆ à®…à®£à¯à®•à¯à®™à¯à®•à®³à¯. à®‡à®²à®µà®š à®šà®Ÿà¯à®Ÿ à®‰à®¤à®µà®¿à®•à¯à®•à¯ à®šà®Ÿà¯à®Ÿ à®šà¯‡à®µà¯ˆ \n\n[GENERAL] category=daily_routines\n  Q: à®•à®¾à®²à¯ˆà®¯à®¿à®²à¯ à®šà¯†à®¯à¯à®¯à®•à¯à®•à¯‚à®Ÿà®¾à®¤à®µà¯ˆ à®Žà®©à¯à®©?\n  A: à®•à®¾à®²à¯ˆà®¯à®¿à®²à¯ à®•à¯‹à®ªà®ªà¯à®ªà®Ÿà®•à¯à®•à¯‚à®Ÿà®¾à®¤à¯, à®Žà®¤à®¿à®°à¯à®®à®±à¯ˆ à®Žà®£à¯à®£à®™à¯à®•à®³à¯ à®µà¯‡à®£à¯à®Ÿà®¾à®®à¯. à®µà¯†à®±à¯à®®à¯ à®µà®¯à®¿à®±à¯à®±à®¿à®²à¯ à®•à®¾à®ªà®¿, à®Ÿà¯€ à®•à¯à®Ÿà®¿à®•à¯à®•à®•à¯à®•à¯‚à®Ÿà®¾à®¤à¯. à®…à®µà®šà®°à®ªà¯à®ªà®Ÿà¯à®Ÿà¯ à®•à®¾à®²à¯ˆ à®‰à®£à®µà¯ˆ à®¤à®µà®¿à®°à¯à®•à¯à®•à®•à¯à®•à¯‚à®Ÿà®¾à®¤à¯. à®®à¯Šà®ªà¯ˆà®²à¯ à®‰à®Ÿà®©à¯‡ \n\nâœ… Dataset Factory v4.0 complete!\n   Dataset: CryptoYogi/vazhi-tamil-sft-v4_0\n   Next step: Use this dataset in training notebook\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 26
  }
 ]
}
