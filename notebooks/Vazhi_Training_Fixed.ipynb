{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# VAZHI Full Training\n\n**Full training on all 11K+ samples with validated settings.**\n\n**Validated Settings (from upper boundary test):**\n- Learning rate: 5e-5 ✓\n- Tokenizer: UNMODIFIED ✓\n- Loss decreased: 3.39 → 3.00 ✓\n- Responses: Coherent Tamil ✓\n\n**This Run:**\n- Training samples: ALL (~11,000)\n- Epochs: 1 (full dataset is large enough)\n- Expected time: ~2-3 hours"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CRITICAL: Force single GPU\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n# Suppress TensorFlow/JAX CUDA factory warnings (Kaggle has all frameworks installed)\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"GRPC_VERBOSITY\"] = \"ERROR\"\nos.environ[\"GLOG_minloglevel\"] = \"3\"\n\n# Suppress harmless warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\".*UnsupportedFieldAttributeWarning.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*'repr' attribute.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*'frozen' attribute.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*OrderedVocab.*holes.*\")  # Gemma tokenizer quirk\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Suppress absl logging (JAX/TF)\nimport logging\nlogging.getLogger(\"absl\").setLevel(logging.ERROR)\n\n# Install dependencies\n!pip install -q bitsandbytes peft trl accelerate datasets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Model WITHOUT Modifying Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# Use our forked model (run Vazhi_Fork_Base_Model.ipynb first to create it)\nmodel_name = \"CryptoYogi/gemma-2b-tamil-base\"\n\nprint(f\"Using model: {model_name}\")\n\n# Load tokenizer - DO NOT MODIFY IT!\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprint(\"=\" * 50)\nprint(\"TOKENIZER INFO (UNMODIFIED):\")\nprint(\"=\" * 50)\nprint(f\"PAD token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})\")\nprint(f\"EOS token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})\")\nprint(f\"BOS token: {tokenizer.bos_token} (id: {tokenizer.bos_token_id})\")\nprint(f\"\\n⚠️  IMPORTANT: We are NOT modifying the tokenizer!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model in 4-bit\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map={\"\":0},\n    trust_remote_code=True,\n    attn_implementation=\"eager\",\n)\n\n# IMPORTANT: Align model config with tokenizer BEFORE training\n# This prevents the \"tokenizer has new PAD/BOS/EOS tokens\" warning\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.bos_token_id = tokenizer.bos_token_id\nmodel.config.eos_token_id = tokenizer.eos_token_id\n\nif hasattr(model, 'generation_config'):\n    model.generation_config.pad_token_id = tokenizer.pad_token_id\n    model.generation_config.bos_token_id = tokenizer.bos_token_id\n    model.generation_config.eos_token_id = tokenizer.eos_token_id\n\n# Disable cache for training\nmodel.config.use_cache = False\n\nprint(f\"Model loaded! Memory: {model.get_memory_footprint() / 1e9:.2f} GB\")\nprint(f\"Token IDs aligned: PAD={model.config.pad_token_id}, BOS={model.config.bos_token_id}, EOS={model.config.eos_token_id}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test Model BEFORE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def test_model(prompt, max_tokens=100):\n    \"\"\"Test model with Alpaca format\"\"\"\n    full_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Temporarily enable cache for generation if it was disabled\n    original_cache = model.config.use_cache\n    model.config.use_cache = True\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id,  # Use ORIGINAL pad token\n            use_cache=True,  # Explicitly enable for generation\n        )\n    \n    # Restore original cache setting\n    model.config.use_cache = original_cache\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if \"### Response:\" in response:\n        return response.split(\"### Response:\")[-1].strip()\n    return response\n\n# Test questions covering different packs\ntest_questions = [\n    \"தமிழ்நாட்டின் தலைநகரம் எது?\",  # General\n    \"திருக்குறள் யார் எழுதினார்?\",  # Culture\n    \"OTP மோசடி என்றால் என்ன?\",  # Security\n]\n\nprint(\"=\" * 60)\nprint(\"BEFORE TRAINING - Baseline Responses:\")\nprint(\"=\" * 60)\nbefore_responses = []\nfor q in test_questions:\n    resp = test_model(q)\n    before_responses.append(resp)\n    print(f\"\\nQ: {q}\")\n    print(f\"A: {resp[:200]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\nimport re\n\n# Load full dataset\ndataset = load_dataset(\"CryptoYogi/vazhi-tamil-v05\")\nprint(f\"Full dataset: {len(dataset['train'])} training, {len(dataset['validation'])} validation\")\n\ndef format_to_alpaca(example):\n    \"\"\"Convert ChatML to Alpaca format\"\"\"\n    if 'text' not in example or not example['text']:\n        return {\"text\": \"\"}\n    \n    text = example['text']\n    \n    # Convert ChatML to Alpaca\n    if '<|im_start|>' in text:\n        user_match = re.search(r'<\\|im_start\\|>user\\n(.+?)<\\|im_end\\|>', text, re.DOTALL)\n        assistant_match = re.search(r'<\\|im_start\\|>assistant\\n(.+?)<\\|im_end\\|>', text, re.DOTALL)\n        \n        if user_match and assistant_match:\n            instruction = user_match.group(1).strip()\n            output = assistant_match.group(1).strip()\n            # Add EOS token at end for proper training\n            formatted = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}{tokenizer.eos_token}\"\n            return {\"text\": formatted}\n    \n    # Already in correct format\n    if '### Instruction' in text:\n        if not text.endswith(tokenizer.eos_token):\n            text = text + tokenizer.eos_token\n        return {\"text\": text}\n    \n    return {\"text\": \"\"}\n\n# FULL TRAINING - Use ALL samples\nprint(\"\\n\" + \"=\" * 60)\nprint(\"FULL TRAINING MODE - Using all samples\")\nprint(\"=\" * 60)\n\n# Format ALL training data\nformatted_train = dataset['train'].map(format_to_alpaca)\nformatted_train = formatted_train.filter(lambda x: len(x['text']) > 50)\n\n# Format ALL validation data\nformatted_val = dataset['validation'].map(format_to_alpaca)\nformatted_val = formatted_val.filter(lambda x: len(x['text']) > 50)\n\nprint(f\"\\nFormatted: {len(formatted_train)} train, {len(formatted_val)} validation\")\nprint(f\"\\nSample:\")\nprint(formatted_train[0]['text'][:300])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup LoRA with Conservative Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# CONSERVATIVE LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=4,              # Lower rank = gentler adaptation\n",
    "    lora_alpha=8,     # alpha = 2*r is standard\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Only query and value, not all\n",
    "    lora_dropout=0.1,  # Higher dropout for regularization\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training with Ultra-Conservative Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTTrainer, SFTConfig\nimport time\n\n# FULL TRAINING CONFIG - Validated settings from test run\ntraining_config = SFTConfig(\n    output_dir=\"./vazhi-full-training\",\n    \n    # Dataset\n    dataset_text_field=\"text\",\n    max_length=512,\n    packing=False,\n    \n    # Batch size\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,  # Effective batch = 8\n    \n    # Learning rate - validated at 5e-5\n    learning_rate=5e-5,\n    num_train_epochs=1,      # 1 epoch for full dataset\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    \n    # Regularization\n    weight_decay=0.01,\n    max_grad_norm=0.3,\n    \n    # Optimizer\n    optim=\"paged_adamw_8bit\",\n    \n    # Precision\n    fp16=False,\n    bf16=True,\n    \n    # Evaluation - less frequent for large dataset\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    \n    # Logging\n    logging_steps=50,\n    \n    # Checkpointing - save periodically for long training\n    save_strategy=\"steps\",\n    save_steps=200,\n    save_total_limit=3,\n    \n    # Speed\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    \n    seed=42,\n    report_to=\"none\",\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    processing_class=tokenizer,\n    train_dataset=formatted_train,\n    eval_dataset=formatted_val,\n    args=training_config,\n)\n\n# Calculate expected steps\ntotal_steps = (len(formatted_train) // (2 * 4)) * 1  # samples / (batch * accum) * epochs\nprint(f\"=\" * 60)\nprint(f\"FULL TRAINING\")\nprint(f\"=\" * 60)\nprint(f\"  Training samples: {len(formatted_train)}\")\nprint(f\"  Validation samples: {len(formatted_val)}\")\nprint(f\"  Estimated steps: ~{total_steps}\")\nprint(f\"  Learning rate: 5e-5 (validated)\")\nprint(f\"  Epochs: 1\")\nprint(f\"  Expected time: ~2-3 hours\")\nprint(f\"\")\nprint(f\"  Checkpoints saved every 200 steps\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "print(\"Starting training...\")\n",
    "start = time.time()\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"Time: {elapsed / 60:.1f} minutes\")\n",
    "print(f\"Final training loss: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate AFTER Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# IMPORTANT: Prepare model for inference\n# 1. Disable gradient checkpointing (not needed for inference)\n# 2. Re-enable cache (needed for efficient generation)\nmodel.gradient_checkpointing_disable()\nmodel.config.use_cache = True\n\nprint(\"=\" * 60)\nprint(\"AFTER TRAINING - Comparing Responses:\")\nprint(\"=\" * 60)\n\nresults = []\nfor i, q in enumerate(test_questions):\n    after_resp = test_model(q)\n    print(f\"\\n{'='*60}\")\n    print(f\"Q: {q}\")\n    print(f\"\\nBEFORE: {before_responses[i][:150]}\")\n    print(f\"\\nAFTER:  {after_resp[:150]}\")\n    results.append(after_resp)\n\n# Analyze results\nimport re\n\ndef analyze_response(text):\n    \"\"\"Check if response is coherent Tamil\"\"\"\n    if not text or len(text) < 10:\n        return False, 0\n    tamil_chars = len(re.findall(r'[\\u0B80-\\u0BFF]', text))\n    total_chars = len(text)\n    tamil_pct = (tamil_chars / total_chars * 100) if total_chars > 0 else 0\n    \n    # Check for garbage patterns\n    has_spaces = ' ' in text\n    is_coherent = tamil_pct > 30 and has_spaces and len(text) > 20\n    \n    return is_coherent, tamil_pct\n\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"TRAINING RESULTS:\")\nprint(\"=\" * 60)\nprint(f\"Final loss: {trainer_stats.training_loss:.4f}\")\n\nall_coherent = True\nfor i, resp in enumerate(results):\n    coherent, tamil_pct = analyze_response(resp)\n    status = \"✓\" if coherent else \"✗\"\n    print(f\"  Response {i+1}: {status} (Tamil: {tamil_pct:.0f}%)\")\n    if not coherent:\n        all_coherent = False\n\n# Updated success criteria (loss < 3.2 is good for Tamil LM)\nif trainer_stats.training_loss < 3.2 and all_coherent:\n    print(f\"\\n✅ TRAINING SUCCESSFUL!\")\n    print(f\"   - Loss decreased to {trainer_stats.training_loss:.4f}\")\n    print(f\"   - All responses are coherent Tamil\")\n    print(f\"   - Ready to save and convert to GGUF!\")\nelse:\n    print(f\"\\n⚠️  TRAINING COMPLETED WITH ISSUES\")\n    if trainer_stats.training_loss >= 3.2:\n        print(f\"   - Loss could be lower: {trainer_stats.training_loss:.4f}\")\n    if not all_coherent:\n        print(f\"   - Some responses may need review\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: If Passed - Save Adapter\n",
    "\n",
    "Only run this if validation passed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save the LoRA adapter locally\nadapter_path = \"./vazhi-lora-adapter\"\nmodel.save_pretrained(adapter_path)\ntokenizer.save_pretrained(adapter_path)\nprint(f\"Adapter saved to: {adapter_path}\")\n\n# List saved files\nimport os\ntotal_size = 0\nfor f in os.listdir(adapter_path):\n    size = os.path.getsize(os.path.join(adapter_path, f)) / 1e6\n    total_size += size\n    print(f\"  {f}: {size:.2f} MB\")\nprint(f\"\\nTotal adapter size: {total_size:.2f} MB\")"
  },
  {
   "cell_type": "code",
   "source": "## Summary\n\n**Training Configuration:**\n- Base Model: CryptoYogi/gemma-2b-tamil-base\n- Training Samples: ~11,000\n- Learning Rate: 5e-5\n- Epochs: 1\n- LoRA Rank: 4\n\n**Outputs:**\n1. `CryptoYogi/vazhi-lora-v1` - LoRA adapter (~1MB)\n2. `CryptoYogi/vazhi-v1` - Merged full model (~5GB)\n\n**Next Steps:**\n1. Convert merged model to GGUF Q4_K_M\n2. Upload GGUF to `CryptoYogi/vazhi-gguf`\n3. Update app to use new model",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 8: Merge Adapter and Prepare for GGUF\n\nMerge the LoRA adapter with base model, then upload for GGUF conversion.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Merge LoRA adapter with base model\nprint(\"Merging LoRA adapter with base model...\")\n\n# Merge and unload\nmerged_model = model.merge_and_unload()\n\n# Save merged model\nmerged_path = \"./vazhi-merged\"\nmerged_model.save_pretrained(merged_path, safe_serialization=True)\ntokenizer.save_pretrained(merged_path)\n\nprint(f\"✅ Merged model saved to: {merged_path}\")\n\n# Show size\nimport os\ntotal_size = 0\nfor f in os.listdir(merged_path):\n    fpath = os.path.join(merged_path, f)\n    if os.path.isfile(fpath):\n        size = os.path.getsize(fpath) / 1e9\n        total_size += size\n        print(f\"  {f}: {size:.2f} GB\")\nprint(f\"\\nTotal merged model size: {total_size:.2f} GB\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Upload merged model to HuggingFace\nMERGED_REPO = \"CryptoYogi/vazhi-v1\"\n\napi.create_repo(repo_id=MERGED_REPO, exist_ok=True, private=False)\n\nprint(f\"Uploading merged model to {MERGED_REPO}...\")\nprint(\"This may take 10-15 minutes for ~5GB...\")\n\napi.upload_folder(\n    folder_path=merged_path,\n    repo_id=MERGED_REPO,\n    commit_message=\"VAZHI v1.0 - Full merged model trained on 11K Tamil samples\"\n)\n\nprint(f\"\\n✅ Merged model uploaded!\")\nprint(f\"View at: https://huggingface.co/{MERGED_REPO}\")\nprint(f\"\\nNext step: Convert to GGUF using llama.cpp or online converter\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "**Key Differences from Failed Runs:**\n",
    "\n",
    "| Setting | Failed Run | This Run |\n",
    "|---------|-----------|----------|\n",
    "| Tokenizer | Modified (pad=eos) | UNMODIFIED |\n",
    "| Learning Rate | 5e-5 | 1e-6 (50x lower) |\n",
    "| LoRA Rank | 8 | 4 |\n",
    "| Target Modules | q,k,v,o | q,v only |\n",
    "| Warmup | 10% | 30% |\n",
    "| Samples | 50 | 500 |\n",
    "| Evaluation | None | Every 25 steps |\n",
    "| Gradient Clipping | None | 0.3 |\n",
    "\n",
    "**If This Still Fails:**\n",
    "1. Try learning rate 5e-7 or 1e-7\n",
    "2. Use the base Gemma model instead of instruction-tuned\n",
    "3. Consider using QLoRA with different quantization\n",
    "4. Look into using PEFT with different adapters (IA3, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
