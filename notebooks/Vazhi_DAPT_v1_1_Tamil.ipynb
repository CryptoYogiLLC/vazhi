{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VAZHI DAPT v1.1 ‚Äî Tamil Language Adaptation Training\n\n**Pipeline Step 2 of 3:** Train DAPT on cleaned Tamil data using **instruct** model.\n\n```\nStep 1: Data Prep (DONE ‚Äî Vazhi_DAPT_Data_v1_1.ipynb)\n  ‚Üí Produced: CryptoYogi/vazhi-dapt-tamil-v1_1\n    52,664 train blocks + 1,075 val blocks = 55M tokens (NFKC-cleaned)\n\nStep 2 (THIS NOTEBOOK): DAPT Training ‚Äî Kaggle T4 x2 GPU\n  ‚Üí Input:  Packed dataset from HF + Qwen3-0.6B (instruct)\n  ‚Üí Output: CryptoYogi/qwen3-0.6b-tamil-v1_1 (reusable Tamil instruct base)\n           CryptoYogi/qwen3-0.6b-tamil-v1_1-lora (adapter backup)\n\nStep 3: SFT (NEXT)\n  ‚Üí Input:  DAPT'd instruct model + ChatML instruction pairs\n  ‚Üí Output: Final VAZHI model\n```\n\n**Key changes from v1.0 (incorporating multi-agent review + GPT5.2 feedback):**\n1. **Instruct model**, not Base ‚Äî preserves chat capabilities, DAPT adds Tamil fluency\n2. **Higher LR** (5e-5 vs 2e-5) ‚Äî faster learning for instruct model adaptation\n3. **Cleaner data** ‚Äî NFKC normalized, 70% Tamil threshold, \\ufffd stripped\n4. **3.4x more tokens** ‚Äî 55M vs 16M (Sangraha verified only was sufficient)\n5. **No step cap** ‚Äî train ~1 full epoch over entire dataset\n6. **Dual T4** ‚Äî Trainer auto-uses both GPUs via DataParallel (~2x throughput)\n7. **Suppress `<think>` tokens** during eval (Qwen3 instruct artifact)\n8. **Better eval** ‚Äî perplexity + word validity + side-by-side with vanilla instruct\n9. **Checkpoint resume** ‚Äî can resume from checkpoint if Kaggle disconnects\n\n**Target:** Kaggle T4 x2 (2√ó15GB) | ~1,645 steps | Est. ~7 hours with dual GPU","metadata":{}},{"cell_type":"markdown","source":"## 1. Install Dependencies\n\n**After running this cell, RESTART the session** (Runtime ‚Üí Restart session)","metadata":{}},{"cell_type":"code","source":"!pip install -q -U \\\n  \"transformers>=4.45.0,<5.0.0\" \\\n  \"accelerate>=0.34.2\" \\\n  \"peft>=0.12.0\" \\\n  \"datasets>=2.21.0\" \\\n  \"huggingface_hub>=0.24.7\"\n\nprint(\"\\u2705 Dependencies installed\")\nprint(\"\\u26a0\\ufe0f  RESTART THE SESSION NOW (Runtime \\u2192 Restart session)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:56:27.491925Z","iopub.execute_input":"2026-02-13T05:56:27.492204Z","iopub.status.idle":"2026-02-13T05:56:31.727107Z","shell.execute_reply.started":"2026-02-13T05:56:27.492171Z","shell.execute_reply":"2026-02-13T05:56:31.726304Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Dependencies installed\n‚ö†Ô∏è  RESTART THE SESSION NOW (Runtime ‚Üí Restart session)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 2. Configuration","metadata":{}},{"cell_type":"code","source":"# Multi-GPU setup ‚Äî Kaggle T4 x2\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport json\nimport re\nimport random\nimport glob\nimport gc\nimport torch\nimport numpy as np\nfrom dataclasses import dataclass\nfrom collections import Counter\nfrom datasets import load_dataset\nfrom huggingface_hub import login, HfApi\n\nfrom transformers import (\n    AutoModelForCausalLM, AutoTokenizer,\n    TrainerCallback, Trainer, TrainingArguments,\n)\nfrom peft import LoraConfig, get_peft_model, PeftModel\n\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\n# === KEY CONFIG ===\nBASE_MODEL = \"Qwen/Qwen3-0.6B\"       # INSTRUCT model (v1.0 used Base ‚Äî that was wrong)\nDATASET_NAME = \"CryptoYogi/vazhi-dapt-tamil-v1_1\"  # Pre-built by Data Prep v1.1\nOUTPUT_MODEL = \"CryptoYogi/qwen3-0.6b-tamil-v1_1\"  # Merged output\nADAPTER_REPO = \"CryptoYogi/qwen3-0.6b-tamil-v1_1-lora\"  # Adapter backup\n\n# Training config\nMAX_SEQ_LENGTH = 1024        # Must match data prep notebook\nLEARNING_RATE = 5e-5         # 2.5x higher than v1.0 (2e-5 was too gentle)\nLORA_R = 16                  # Conservative rank for 0.6B\nLORA_ALPHA = 32\nBATCH_SIZE = 4               # Per-device batch size\nGRADIENT_ACCUMULATION = 4    # Halved from 8 ‚Äî 2 GPUs compensate: 4 x 2 x 4 = 32 effective\nWARMUP_RATIO = 0.05\nMAX_EPOCHS = 1               # Full single epoch over 55M tokens\n\n# Qwen3 instruct <think> tokens to suppress during generation\nTHINK_TOKEN_IDS = [151667, 151668]  # <think>, </think>\n\n# GPU setup\nn_gpus = torch.cuda.device_count()\n\nprint(f\"\\u2705 Configuration loaded\")\nprint(f\"   PyTorch: {torch.__version__}\")\nprint(f\"   CUDA: {torch.cuda.is_available()}\")\nprint(f\"   GPUs: {n_gpus}\")\nfor i in range(n_gpus):\n    gpu_name = torch.cuda.get_device_name(i)\n    gpu_mem = torch.cuda.get_device_properties(i).total_mem / 1024**3 if hasattr(torch.cuda.get_device_properties(i), 'total_mem') else torch.cuda.get_device_properties(i).total_memory / 1024**3\n    print(f\"   GPU {i}: {gpu_name} ({gpu_mem:.0f} GB)\")\n\neffective_batch = BATCH_SIZE * n_gpus * GRADIENT_ACCUMULATION\nprint()\nprint(f\"\\U0001f4cb DAPT Training v1.1:\")\nprint(f\"   Base model:  {BASE_MODEL} (INSTRUCT \\u2014 preserves chat capability)\")\nprint(f\"   Dataset:     {DATASET_NAME}\")\nprint(f\"   Output:      {OUTPUT_MODEL}\")\nprint(f\"   LR:          {LEARNING_RATE} (2.5x higher than v1.0)\")\nprint(f\"   LoRA:        r={LORA_R}, alpha={LORA_ALPHA}\")\nprint(f\"   Batch:       {BATCH_SIZE} x {n_gpus} GPUs x {GRADIENT_ACCUMULATION} accum = {effective_batch} effective\")\nprint(f\"   Epochs:      {MAX_EPOCHS}\")\nprint(f\"   fp16:        True\")\nprint(f\"   Grad ckpt:   True\")\nprint(f\"   <think> suppress: IDs {THINK_TOKEN_IDS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:56:31.729486Z","iopub.execute_input":"2026-02-13T05:56:31.729745Z","iopub.status.idle":"2026-02-13T05:56:48.618935Z","shell.execute_reply.started":"2026-02-13T05:56:31.729717Z","shell.execute_reply":"2026-02-13T05:56:48.618347Z"}},"outputs":[{"name":"stderr","text":"2026-02-13 05:56:37.759481: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770962197.780326     120 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770962197.786667     120 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770962197.803046     120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770962197.803066     120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770962197.803069     120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770962197.803071     120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Configuration loaded\n   PyTorch: 2.8.0+cu126\n   CUDA: True\n   GPUs: 2\n   GPU 0: Tesla T4 (15 GB)\n   GPU 1: Tesla T4 (15 GB)\n\nüìã DAPT Training v1.1:\n   Base model:  Qwen/Qwen3-0.6B (INSTRUCT ‚Äî preserves chat capability)\n   Dataset:     CryptoYogi/vazhi-dapt-tamil-v1_1\n   Output:      CryptoYogi/qwen3-0.6b-tamil-v1_1\n   LR:          5e-05 (2.5x higher than v1.0)\n   LoRA:        r=16, alpha=32\n   Batch:       4 x 2 GPUs x 4 accum = 32 effective\n   Epochs:      1\n   fp16:        True\n   Grad ckpt:   True\n   <think> suppress: IDs [151667, 151668]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Login to HuggingFace\n# Kaggle: from kaggle_secrets import UserSecretsClient; login(token=UserSecretsClient().get_secret(\"HF_TOKEN\"))\n# Colab: login()\nfrom kaggle_secrets import UserSecretsClient\nsecrets = UserSecretsClient()\nhf_token = secrets.get_secret(\"HF_TOKEN\")\nlogin(token=hf_token)\nprint(\"\\u2705 Logged in to HuggingFace\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:56:48.619733Z","iopub.execute_input":"2026-02-13T05:56:48.620410Z","iopub.status.idle":"2026-02-13T05:56:48.954790Z","shell.execute_reply.started":"2026-02-13T05:56:48.620381Z","shell.execute_reply":"2026-02-13T05:56:48.954180Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Logged in to HuggingFace\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 3. Load Pre-Built Dataset\n\nCreated by `Vazhi_DAPT_Data_v1_1.ipynb` (already run on Colab):\n- Sangraha **verified Tamil only** (93% keep rate ‚Äî highest quality)\n- NFKC normalized, \\ufffd stripped, zero-width chars removed\n- Tamil >= 70%, dedup, repetition filtered\n- 27,105 docs ‚Üí 53,739 blocks of 1024 tokens ‚Üí 55M tokens\n- Train: 52,664 blocks | Validation: 1,075 blocks","metadata":{}},{"cell_type":"code","source":"print(f\"\\U0001f4e5 Loading pre-built dataset from {DATASET_NAME}...\")\nds = load_dataset(DATASET_NAME)\n\ntrain_dataset = ds[\"train\"]\neval_dataset = ds[\"validation\"]\n\nprint(f\"\\u2705 Dataset loaded:\")\nprint(f\"   Train:      {len(train_dataset):,} blocks\")\nprint(f\"   Validation: {len(eval_dataset):,} blocks\")\nprint(f\"   Block size: {len(train_dataset[0]['input_ids'])} tokens\")\nprint(f\"   Columns:    {train_dataset.column_names}\")\n\ntotal_train_tokens = len(train_dataset) * MAX_SEQ_LENGTH\nprint(f\"   Total train tokens: {total_train_tokens:,}\")\n\n# Verify block size matches our config\nassert len(train_dataset[0][\"input_ids\"]) == MAX_SEQ_LENGTH, \\\n    f\"Block size mismatch: dataset has {len(train_dataset[0]['input_ids'])}, config has {MAX_SEQ_LENGTH}\"\nprint(\"\\u2705 Block size verified\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:56:48.955696Z","iopub.execute_input":"2026-02-13T05:56:48.955890Z","iopub.status.idle":"2026-02-13T05:56:56.833420Z","shell.execute_reply.started":"2026-02-13T05:56:48.955869Z","shell.execute_reply":"2026-02-13T05:56:56.832785Z"}},"outputs":[{"name":"stdout","text":"üì• Loading pre-built dataset from CryptoYogi/vazhi-dapt-tamil-v1_1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/475 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"158c45d05fe9484b8c5a146a251ac58a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00002.parquet:   0%|          | 0.00/53.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"165f9b46c5994a118d00417208f28c84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00001-of-00002.parquet:   0%|          | 0.00/53.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"578e5500643e401fab28d143116f158b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/2.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6ba1dff4d6f4c8d949cddec62c717ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/52664 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7f41fa6891148bdbdf18858e7607ebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1075 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ace5dbaa2278430dba0cb2bc8d0081ac"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Dataset loaded:\n   Train:      52,664 blocks\n   Validation: 1,075 blocks\n   Block size: 1024 tokens\n   Columns:    ['input_ids', 'attention_mask', 'labels']\n   Total train tokens: 53,927,936\n‚úÖ Block size verified\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 4. Load Tokenizer","metadata":{}},{"cell_type":"code","source":"print(f\"\\U0001f4e5 Loading tokenizer from {BASE_MODEL}...\")\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\ntokenizer.padding_side = \"right\"\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nprint(f\"\\u2705 Tokenizer ready: {len(tokenizer)} tokens\")\nprint(f\"   eos_token: {tokenizer.eos_token!r} (ID {tokenizer.eos_token_id})\")\nprint(f\"   pad_token: {tokenizer.pad_token!r} (ID {tokenizer.pad_token_id})\")\n\n# Verify <think> token IDs\nfor tid in THINK_TOKEN_IDS:\n    token_str = tokenizer.decode([tid])\n    print(f\"   Token {tid}: {token_str!r}\")\n\n# === HELPER FUNCTIONS ===\ndef count_tamil_chars(text):\n    return sum(1 for c in text if '\\u0B80' <= c <= '\\u0BFF')\n\ndef tamil_char_pct(text):\n    if not text:\n        return 0.0\n    return 100.0 * count_tamil_chars(text) / len(text)\n\ndef count_tamil_words(text):\n    \"\"\"Count words that are predominantly Tamil characters.\"\"\"\n    words = text.split()\n    tamil_words = 0\n    for w in words:\n        if len(w) > 0 and tamil_char_pct(w) > 50:\n            tamil_words += 1\n    return tamil_words, len(words)\n\n# Quick sanity: decode a sample from the dataset\nsample_text = tokenizer.decode(train_dataset[0][\"input_ids\"][:100])\nprint(f\"\\n\\U0001f50d Sample from dataset (first 100 tokens):\")\nprint(f\"   Tamil%: {tamil_char_pct(sample_text):.0f}%\")\nprint(f\"   Text:   {sample_text[:200]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:56:56.834319Z","iopub.execute_input":"2026-02-13T05:56:56.834576Z","iopub.status.idle":"2026-02-13T05:56:58.712120Z","shell.execute_reply.started":"2026-02-13T05:56:56.834552Z","shell.execute_reply":"2026-02-13T05:56:58.711434Z"}},"outputs":[{"name":"stdout","text":"üì• Loading tokenizer from Qwen/Qwen3-0.6B...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd8023900463411ea80e4bc800b448a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9c688b4c9dc42ce81a7085c7cc736ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1dc3783ab864458aa23f9eca3ffb476"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27db0b882c2f433b9fcd8aad88af5ace"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Tokenizer ready: 151669 tokens\n   eos_token: '<|im_end|>' (ID 151645)\n   pad_token: '<|endoftext|>' (ID 151643)\n   Token 151667: '<think>'\n   Token 151668: '</think>'\n\nüîç Sample from dataset (first 100 tokens):\n   Tamil%: 86%\n   Text:   ‡Æø‡Æ§‡Øç‡Æ§‡Æµ‡Æ∞‡Øç. ‡ÆÖ‡Æµ‡Æ∞‡Øç ‡Æí‡Æü‡Øç‡Æü‡Æ≤‡Æø‡Æ≤‡Øç ‡Æö‡ØÅ‡Æ§‡Øç‡Æ§‡ÆÆ‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Æ£‡Æø‡ÆØ‡Æø‡Æ≤‡Øç ‡Æà‡Æü‡ØÅ‡Æ™‡Æü‡Øç‡Æü‡Æµ‡Æ∞‡Øç. ‡ÆÖ‡Æµ‡Æ∞‡Æ§‡ØÅ ‡Æï‡ØÅ‡Æü‡ØÅ‡ÆÆ‡Øç‡Æ™‡ÆÆ‡Øç ‡Æ™‡Æ£ ‡Æµ‡Æö‡Æ§‡Æø ‡Æá‡Æ≤‡Øç‡Æ≤‡Ææ‡Æ§ ‡Æï‡ØÅ‡Æü‡ØÅ‡ÆÆ‡Øç‡Æ™...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 5. Load Instruct Model + LoRA Setup\n\n**v1.1 change:** Using **Instruct** model instead of Base.\n\nRationale (from multi-agent review):\n- Base model produced gibberish even after DAPT ‚Äî no Tamil knowledge to build on\n- Instruct model already has some language capability\n- DAPT on instruct preserves chat behaviors while deepening Tamil fluency\n- `<think>` tokens suppressed during generation, not a problem during training","metadata":{}},{"cell_type":"code","source":"# fp16 ‚Äî no 4-bit quantization!\n# Lesson from v1.0: 4-bit bypasses Tensor Cores on T4, 3x slower\n# Qwen3-0.6B in fp16 = ~1.2GB per GPU, fits easily on T4 x2\n\nprint(f\"\\U0001f4e5 Loading {BASE_MODEL} in fp16 (INSTRUCT model)...\")\nprint(f\"   NO device_map ‚Äî Trainer handles DataParallel across {n_gpus} GPUs\")\n\n# IMPORTANT: Do NOT use device_map here!\n# device_map={\"\":0} sets model.hf_device_map which makes Trainer think\n# the model is already model-parallel ‚Üí skips DataParallel wrapping ‚Üí only 1 GPU used.\n# v1.0 Trainer output confirmed this: \"The model is already on multiple devices. Skipping...\"\n# Instead: load to cuda:0 manually, let Trainer wrap with DataParallel for both GPUs.\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n)\nmodel = model.to(\"cuda:0\")\n\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.eos_token_id = tokenizer.eos_token_id\nmodel.config.use_cache = False  # Disabled for training (incompatible with grad ckpt)\n\n# Enable gradient checkpointing for memory safety\nmodel.gradient_checkpointing_enable()\n\n# Verify no hf_device_map attribute (would prevent DataParallel)\nhas_device_map = hasattr(model, \"hf_device_map\")\nprint(f\"   hf_device_map present: {has_device_map} (must be False for DataParallel)\")\nif has_device_map:\n    print(f\"   ‚ö†Ô∏è  WARNING: hf_device_map detected ‚Äî Trainer will SKIP DataParallel!\")\n\nmem_gb = torch.cuda.memory_allocated(0) / 1024**3\nprint(f\"\\u2705 Model loaded in fp16: {model.num_parameters():,} params\")\nprint(f\"   GPU 0 memory used: {mem_gb:.1f} GB\")\nprint(f\"   This is the INSTRUCT model (has chat capabilities)\")\nprint(f\"   Trainer will use DataParallel across {n_gpus} GPUs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:56:58.713136Z","iopub.execute_input":"2026-02-13T05:56:58.713678Z","iopub.status.idle":"2026-02-13T05:57:03.968573Z","shell.execute_reply.started":"2026-02-13T05:56:58.713652Z","shell.execute_reply":"2026-02-13T05:57:03.967767Z"}},"outputs":[{"name":"stdout","text":"üì• Loading Qwen/Qwen3-0.6B in fp16 (INSTRUCT model)...\n   NO device_map ‚Äî Trainer handles DataParallel across 2 GPUs\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8b48e7f757b404a8e3cbd5768c6f75a"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"267021152af7455aa02944c0b32e4e04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf5939ad53b34d6485e422b4f3047dad"}},"metadata":{}},{"name":"stdout","text":"   hf_device_map present: False (must be False for DataParallel)\n‚úÖ Model loaded in fp16: 596,049,920 params\n   GPU 0 memory used: 1.1 GB\n   This is the INSTRUCT model (has chat capabilities)\n   Trainer will use DataParallel across 2 GPUs\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\nmem_gb = torch.cuda.memory_allocated() / 1024**3\nprint(f\"\\u2705 LoRA applied | GPU: {mem_gb:.1f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:57:03.969569Z","iopub.execute_input":"2026-02-13T05:57:03.969854Z","iopub.status.idle":"2026-02-13T05:57:04.694805Z","shell.execute_reply.started":"2026-02-13T05:57:03.969829Z","shell.execute_reply":"2026-02-13T05:57:04.693709Z"}},"outputs":[{"name":"stdout","text":"trainable params: 10,092,544 || all params: 606,142,464 || trainable%: 1.6650\n‚úÖ LoRA applied | GPU: 1.1 GB\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 6. Compute Training Steps\n\n52,664 train blocks √∑ 32 effective batch (4 per-device √ó 2 GPUs √ó 4 accum) = **~1,645 steps** per epoch.\nWith dual T4 DataParallel, ~4 steps/min ‚Üí **~7 hours**. Should fit in one Kaggle session.","metadata":{}},{"cell_type":"code","source":"tokens_per_step = BATCH_SIZE * n_gpus * MAX_SEQ_LENGTH * GRADIENT_ACCUMULATION\nsteps_per_epoch = len(train_dataset) // (BATCH_SIZE * n_gpus * GRADIENT_ACCUMULATION)\ntotal_steps = steps_per_epoch * MAX_EPOCHS\ntotal_tokens_trained = total_steps * tokens_per_step\n\n# Save/log intervals ‚Äî more frequent checkpoints for Kaggle disconnect protection\nsave_steps = max(total_steps // 6, 50)    # ~6 checkpoints per run\nlog_steps = max(total_steps // 60, 10)    # ~60 log entries\neval_steps = max(total_steps // 10, 25)   # ~10 evals\n\n# Estimate time (~2 steps/min per GPU on T4, with 2 GPUs Trainer uses DataParallel)\nest_steps_per_min = 2 * n_gpus  # ~4 steps/min with dual T4\nest_hours = total_steps / (est_steps_per_min * 60)\n\nprint(f\"\\U0001f4ca Training Plan:\")\nprint(f\"   Dataset tokens:      {len(train_dataset) * MAX_SEQ_LENGTH:,}\")\nprint(f\"   Tokens/step:         {tokens_per_step:,}\")\nprint(f\"   Steps/epoch:         {steps_per_epoch:,}\")\nprint(f\"   Total steps:         {total_steps:,}\")\nprint(f\"   Tokens trained:      {total_tokens_trained:,}\")\nprint(f\"   Save every:          {save_steps} steps\")\nprint(f\"   Log every:           {log_steps} steps\")\nprint(f\"   Eval every:          {eval_steps} steps\")\nprint(f\"\\n   \\u23f1\\ufe0f Est. time: {est_hours:.1f} hours ({n_gpus} GPUs, ~{est_steps_per_min} steps/min)\")\n\nif est_hours > 10:\n    print(f\"   \\u26a0\\ufe0f  May be tight for Kaggle session limit (12h). Checkpoints saved every {save_steps} steps.\")\n    print(f\"   If disconnected, use checkpoint resume in Section 7a.\")\nelse:\n    print(f\"   \\u2705 Should fit within a single Kaggle session (12h limit)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:57:04.696191Z","iopub.execute_input":"2026-02-13T05:57:04.696946Z","iopub.status.idle":"2026-02-13T05:57:04.706439Z","shell.execute_reply.started":"2026-02-13T05:57:04.696889Z","shell.execute_reply":"2026-02-13T05:57:04.705431Z"}},"outputs":[{"name":"stdout","text":"üìä Training Plan:\n   Dataset tokens:      53,927,936\n   Tokens/step:         32,768\n   Steps/epoch:         1,645\n   Total steps:         1,645\n   Tokens trained:      53,903,360\n   Save every:          274 steps\n   Log every:           27 steps\n   Eval every:          164 steps\n\n   ‚è±Ô∏è Est. time: 6.9 hours (2 GPUs, ~4 steps/min)\n   ‚úÖ Should fit within a single Kaggle session (12h limit)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## 7. Train\n\nIf Kaggle disconnects mid-training, skip to **Section 7a** to resume from checkpoint.","metadata":{}},{"cell_type":"code","source":"# === LOSS LOGGING ===\nclass LossLoggingCallback(TrainerCallback):\n    def __init__(self):\n        self.losses = []\n        self.eval_losses = []\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs:\n            if \"loss\" in logs:\n                step = state.global_step\n                loss = logs[\"loss\"]\n                lr = logs.get(\"learning_rate\", 0)\n                self.losses.append((step, loss))\n                print(f\"  Step {step:4d}/{total_steps} | Loss: {loss:.4f} | LR: {lr:.2e}\")\n            if \"eval_loss\" in logs:\n                eval_loss = logs[\"eval_loss\"]\n                ppl = np.exp(min(eval_loss, 20))\n                self.eval_losses.append((state.global_step, eval_loss))\n                print(f\"  \\U0001f4ca Eval Loss: {eval_loss:.4f} | Perplexity: {ppl:.1f}\")\n\nloss_callback = LossLoggingCallback()\n\n# === DATA COLLATOR ===\n@dataclass\nclass PackedDataCollator:\n    \"\"\"Collator for pre-packed, pre-tokenized sequences.\"\"\"\n    def __call__(self, features):\n        return {\n            \"input_ids\": torch.tensor([f[\"input_ids\"] for f in features], dtype=torch.long),\n            \"attention_mask\": torch.tensor([f[\"attention_mask\"] for f in features], dtype=torch.long),\n            \"labels\": torch.tensor([f[\"labels\"] for f in features], dtype=torch.long),\n        }\n\n# === TRAINER ===\nOUTPUT_DIR = \"/kaggle/working/vazhi-dapt-v1_1\"\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=MAX_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n    learning_rate=LEARNING_RATE,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=WARMUP_RATIO,\n    logging_steps=log_steps,\n    save_steps=save_steps,\n    eval_steps=eval_steps,\n    eval_strategy=\"steps\",\n    save_total_limit=3,\n    fp16=True,\n    bf16=False,\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    max_grad_norm=1.0,\n    optim=\"adamw_torch\",\n    report_to=\"none\",\n    seed=RANDOM_SEED,\n    load_best_model_at_end=False,\n    dataloader_pin_memory=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=PackedDataCollator(),\n    callbacks=[loss_callback],\n)\n\nprint(\"\\u2705 Trainer ready\")\nprint(f\"   Steps: ~{total_steps} | LR: {LEARNING_RATE} | Effective BS: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\nprint(f\"   fp16: True | grad_ckpt: True | optimizer: AdamW (torch)\")\nprint(f\"   Model: INSTRUCT (Qwen3-0.6B)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:57:04.707485Z","iopub.execute_input":"2026-02-13T05:57:04.707801Z","iopub.status.idle":"2026-02-13T05:57:04.904896Z","shell.execute_reply.started":"2026-02-13T05:57:04.707765Z","shell.execute_reply":"2026-02-13T05:57:04.904305Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Trainer ready\n   Steps: ~1645 | LR: 5e-05 | Effective BS: 16\n   fp16: True | grad_ckpt: True | optimizer: AdamW (torch)\n   Model: INSTRUCT (Qwen3-0.6B)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(\"\\U0001f680 Starting DAPT v1.1 training...\")\nprint(f\"   ~{total_steps} steps, fp16=True, INSTRUCT model\")\nprint(f\"   Tokens: ~{total_steps * tokens_per_step / 1e6:.0f}M\")\nprint(f\"   Est. time: {est_hours:.1f} hours\")\nprint()\n\ntrain_result = trainer.train()\n\nprint(\"\\n\\u2705 Training complete!\")\nmetrics = train_result.metrics\nfor k, v in metrics.items():\n    print(f\"   {k}: {v}\")\n\n# Final eval\nprint(\"\\n\\U0001f4ca Final eval on held-out blocks...\")\neval_metrics = trainer.evaluate()\nfinal_eval_loss = eval_metrics.get(\"eval_loss\", float(\"inf\"))\nfinal_eval_ppl = np.exp(min(final_eval_loss, 20))\nprint(f\"   Eval Loss:       {final_eval_loss:.4f}\")\nprint(f\"   Eval Perplexity: {final_eval_ppl:.1f}\")\n\n# Loss summary\nif loss_callback.losses:\n    start_loss = loss_callback.losses[0][1]\n    end_loss = loss_callback.losses[-1][1]\n    print(f\"\\n\\U0001f4c8 Loss: {start_loss:.4f} \\u2192 {end_loss:.4f} ({100*(start_loss - end_loss)/start_loss:.1f}% drop)\")\nif loss_callback.eval_losses:\n    first_eval = loss_callback.eval_losses[0][1]\n    last_eval = loss_callback.eval_losses[-1][1]\n    print(f\"   Eval:  {first_eval:.4f} \\u2192 {last_eval:.4f} ({100*(first_eval - last_eval)/first_eval:.1f}% drop)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:57:04.905882Z","iopub.execute_input":"2026-02-13T05:57:04.906619Z","iopub.status.idle":"2026-02-13T15:42:53.019730Z","shell.execute_reply.started":"2026-02-13T05:57:04.906591Z","shell.execute_reply":"2026-02-13T15:42:53.019109Z"}},"outputs":[{"name":"stdout","text":"üöÄ Starting DAPT v1.1 training...\n   ~1645 steps, fp16=True, INSTRUCT model\n   Tokens: ~54M\n   Est. time: 6.9 hours\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1646' max='1646' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1646/1646 9:41:42, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>164</td>\n      <td>1.100100</td>\n      <td>1.095042</td>\n    </tr>\n    <tr>\n      <td>328</td>\n      <td>1.043300</td>\n      <td>1.046491</td>\n    </tr>\n    <tr>\n      <td>492</td>\n      <td>1.028900</td>\n      <td>1.021160</td>\n    </tr>\n    <tr>\n      <td>656</td>\n      <td>1.006700</td>\n      <td>1.003887</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.987600</td>\n      <td>0.992039</td>\n    </tr>\n    <tr>\n      <td>984</td>\n      <td>0.996900</td>\n      <td>0.983142</td>\n    </tr>\n    <tr>\n      <td>1148</td>\n      <td>0.981200</td>\n      <td>0.976714</td>\n    </tr>\n    <tr>\n      <td>1312</td>\n      <td>0.981700</td>\n      <td>0.972855</td>\n    </tr>\n    <tr>\n      <td>1476</td>\n      <td>0.962700</td>\n      <td>0.971036</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.963800</td>\n      <td>0.970709</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"  Step   27/1645 | Loss: 1.4268 | LR: 1.57e-05\n  Step   54/1645 | Loss: 1.2864 | LR: 3.19e-05\n  Step   81/1645 | Loss: 1.1952 | LR: 4.82e-05\n  Step  108/1645 | Loss: 1.1533 | LR: 5.00e-05\n  Step  135/1645 | Loss: 1.1227 | LR: 4.99e-05\n  Step  162/1645 | Loss: 1.1001 | LR: 4.97e-05\n  üìä Eval Loss: 1.0950 | Perplexity: 3.0\n  Step  189/1645 | Loss: 1.0927 | LR: 4.94e-05\n  Step  216/1645 | Loss: 1.0769 | LR: 4.91e-05\n  Step  243/1645 | Loss: 1.0655 | LR: 4.87e-05\n  Step  270/1645 | Loss: 1.0659 | LR: 4.83e-05\n  Step  297/1645 | Loss: 1.0541 | LR: 4.77e-05\n  Step  324/1645 | Loss: 1.0433 | LR: 4.71e-05\n  üìä Eval Loss: 1.0465 | Perplexity: 2.8\n  Step  351/1645 | Loss: 1.0435 | LR: 4.65e-05\n  Step  378/1645 | Loss: 1.0385 | LR: 4.58e-05\n  Step  405/1645 | Loss: 1.0334 | LR: 4.50e-05\n  Step  432/1645 | Loss: 1.0350 | LR: 4.41e-05\n  Step  459/1645 | Loss: 1.0185 | LR: 4.32e-05\n  Step  486/1645 | Loss: 1.0289 | LR: 4.23e-05\n  üìä Eval Loss: 1.0212 | Perplexity: 2.8\n  Step  513/1645 | Loss: 1.0296 | LR: 4.13e-05\n  Step  540/1645 | Loss: 1.0141 | LR: 4.02e-05\n  Step  567/1645 | Loss: 1.0166 | LR: 3.91e-05\n  Step  594/1645 | Loss: 1.0099 | LR: 3.80e-05\n  Step  621/1645 | Loss: 1.0041 | LR: 3.68e-05\n  Step  648/1645 | Loss: 1.0067 | LR: 3.56e-05\n  üìä Eval Loss: 1.0039 | Perplexity: 2.7\n  Step  675/1645 | Loss: 1.0075 | LR: 3.43e-05\n  Step  702/1645 | Loss: 0.9957 | LR: 3.31e-05\n  Step  729/1645 | Loss: 0.9929 | LR: 3.18e-05\n  Step  756/1645 | Loss: 0.9941 | LR: 3.05e-05\n  Step  783/1645 | Loss: 0.9879 | LR: 2.91e-05\n  Step  810/1645 | Loss: 0.9876 | LR: 2.78e-05\n  üìä Eval Loss: 0.9920 | Perplexity: 2.7\n  Step  837/1645 | Loss: 0.9927 | LR: 2.64e-05\n  Step  864/1645 | Loss: 1.0004 | LR: 2.51e-05\n  Step  891/1645 | Loss: 0.9878 | LR: 2.37e-05\n  Step  918/1645 | Loss: 0.9722 | LR: 2.24e-05\n  Step  945/1645 | Loss: 0.9936 | LR: 2.10e-05\n  Step  972/1645 | Loss: 0.9969 | LR: 1.97e-05\n  üìä Eval Loss: 0.9831 | Perplexity: 2.7\n  Step  999/1645 | Loss: 0.9892 | LR: 1.84e-05\n  Step 1026/1645 | Loss: 0.9841 | LR: 1.71e-05\n  Step 1053/1645 | Loss: 0.9959 | LR: 1.58e-05\n  Step 1080/1645 | Loss: 0.9871 | LR: 1.46e-05\n  Step 1107/1645 | Loss: 0.9792 | LR: 1.33e-05\n  Step 1134/1645 | Loss: 0.9812 | LR: 1.22e-05\n  üìä Eval Loss: 0.9767 | Perplexity: 2.7\n  Step 1161/1645 | Loss: 0.9715 | LR: 1.10e-05\n  Step 1215/1645 | Loss: 0.9722 | LR: 8.85e-06\n  Step 1242/1645 | Loss: 0.9738 | LR: 7.84e-06\n  Step 1269/1645 | Loss: 0.9768 | LR: 6.88e-06\n  üìä Eval Loss: 0.9729 | Perplexity: 2.6\n  Step 1323/1645 | Loss: 0.9762 | LR: 5.12e-06\n  Step 1350/1645 | Loss: 0.9803 | LR: 4.32e-06\n  Step 1377/1645 | Loss: 0.9729 | LR: 3.59e-06\n  Step 1404/1645 | Loss: 0.9678 | LR: 2.92e-06\n  Step 1431/1645 | Loss: 0.9665 | LR: 2.32e-06\n  Step 1458/1645 | Loss: 0.9627 | LR: 1.78e-06\n  üìä Eval Loss: 0.9710 | Perplexity: 2.6\n  Step 1485/1645 | Loss: 0.9733 | LR: 1.31e-06\n  Step 1512/1645 | Loss: 0.9695 | LR: 9.15e-07\n  Step 1539/1645 | Loss: 0.9733 | LR: 5.87e-07\n  Step 1566/1645 | Loss: 0.9562 | LR: 3.31e-07\n  Step 1593/1645 | Loss: 0.9673 | LR: 1.47e-07\n  Step 1620/1645 | Loss: 0.9638 | LR: 3.68e-08\n  üìä Eval Loss: 0.9707 | Perplexity: 2.6\n\n‚úÖ Training complete!\n   train_runtime: 34921.807\n   train_samples_per_second: 1.508\n   train_steps_per_second: 0.047\n   total_flos: 1.4578662506496e+17\n   train_loss: 1.0203053195160765\n   epoch: 1.0\n\nüìä Final eval on held-out blocks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [135/135 03:43]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"  üìä Eval Loss: 0.9707 | Perplexity: 2.6\n   Eval Loss:       0.9707\n   Eval Perplexity: 2.6\n\nüìà Loss: 1.4268 ‚Üí 0.9638 (32.5% drop)\n   Eval:  1.0950 ‚Üí 0.9707 (11.4% drop)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 7a. Resume From Checkpoint (if Kaggle disconnected)\n\n**Only run this cell if training was interrupted.** Skip if training completed above.\n\nFind the latest checkpoint and resume training.","metadata":{}},{"cell_type":"code","source":"# === UNCOMMENT AND RUN ONLY IF TRAINING WAS INTERRUPTED ===\n# This cell resumes from the latest checkpoint in OUTPUT_DIR\n\n# import glob\n# checkpoints = sorted(glob.glob(f\"{OUTPUT_DIR}/checkpoint-*\"), key=os.path.getmtime)\n# if checkpoints:\n#     latest_ckpt = checkpoints[-1]\n#     print(f\"\\U0001f504 Resuming from {latest_ckpt}\")\n#     train_result = trainer.train(resume_from_checkpoint=latest_ckpt)\n#     print(\"\\u2705 Training resumed and completed!\")\n#     metrics = train_result.metrics\n#     for k, v in metrics.items():\n#         print(f\"   {k}: {v}\")\n#\n#     # Final eval after resume\n#     eval_metrics = trainer.evaluate()\n#     final_eval_loss = eval_metrics.get(\"eval_loss\", float(\"inf\"))\n#     final_eval_ppl = np.exp(min(final_eval_loss, 20))\n#     print(f\"   Eval Loss:       {final_eval_loss:.4f}\")\n#     print(f\"   Eval Perplexity: {final_eval_ppl:.1f}\")\n# else:\n#     print(\"\\u274c No checkpoints found. Run training from scratch (Section 7).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T15:42:53.020803Z","iopub.execute_input":"2026-02-13T15:42:53.021210Z","iopub.status.idle":"2026-02-13T15:42:53.024798Z","shell.execute_reply.started":"2026-02-13T15:42:53.021181Z","shell.execute_reply":"2026-02-13T15:42:53.024132Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## 8. Save & Upload LoRA Adapter","metadata":{}},{"cell_type":"code","source":"ADAPTER_PATH = \"/kaggle/working/vazhi-dapt-v1_1-lora\"\n\nprint(\"\\U0001f4be Saving LoRA adapter...\")\ntrainer.save_model(ADAPTER_PATH)\ntokenizer.save_pretrained(ADAPTER_PATH)\n\nadapter_files = glob.glob(f\"{ADAPTER_PATH}/*\")\nprint(f\"   Files: {[os.path.basename(f) for f in adapter_files]}\")\nassert any('adapter' in f for f in adapter_files), \"No adapter files!\"\nprint(\"\\u2705 Adapter saved\")\n\n# Upload adapter backup\napi = HfApi()\napi.create_repo(ADAPTER_REPO, exist_ok=True)\nprint(f\"\\U0001f4e4 Uploading adapter to {ADAPTER_REPO}...\")\napi.upload_folder(\n    folder_path=ADAPTER_PATH,\n    repo_id=ADAPTER_REPO,\n    commit_message=f\"DAPT v1.1 adapter: Sangraha Tamil (NFKC), instruct model, r={LORA_R}, lr={LEARNING_RATE}\",\n)\nprint(f\"\\u2705 Adapter uploaded: https://huggingface.co/{ADAPTER_REPO}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T15:42:53.025921Z","iopub.execute_input":"2026-02-13T15:42:53.026164Z","iopub.status.idle":"2026-02-13T15:42:58.292715Z","shell.execute_reply.started":"2026-02-13T15:42:53.026141Z","shell.execute_reply":"2026-02-13T15:42:58.292017Z"}},"outputs":[{"name":"stdout","text":"üíæ Saving LoRA adapter...\n   Files: ['tokenizer_config.json', 'README.md', 'special_tokens_map.json', 'chat_template.jinja', 'added_tokens.json', 'vocab.json', 'tokenizer.json', 'adapter_model.safetensors', 'adapter_config.json', 'merges.txt', 'training_args.bin']\n‚úÖ Adapter saved\nüì§ Uploading adapter to CryptoYogi/qwen3-0.6b-tamil-v1_1-lora...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25030af3890c4086963226c6e28de08a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b28e293ee22641edbfce7d2a9fd8f89a"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Adapter uploaded: https://huggingface.co/CryptoYogi/qwen3-0.6b-tamil-v1_1-lora\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 9. Merge LoRA in FP16\n\n**Hard rule (Lesson #39):** NEVER merge into 4-bit. Reload base in fp16, merge there.","metadata":{}},{"cell_type":"code","source":"# Free training model to make room for merge\ndel model\ndel trainer\ngc.collect()\ntorch.cuda.empty_cache()\nprint(\"\\U0001f5d1\\ufe0f Training model freed\")\n\n# Reload fresh base in fp16 for clean merge\nprint(f\"\\U0001f517 Loading fresh {BASE_MODEL} in fp16 for merge...\")\nbase_model_fp16 = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=torch.float16,\n    device_map={\"\":0},\n    trust_remote_code=True,\n)\n\npeft_model = PeftModel.from_pretrained(base_model_fp16, ADAPTER_PATH)\npeft_model.gradient_checkpointing_disable()\npeft_model.config.use_cache = True\npeft_model.eval()\n\nprint(\"\\U0001f500 Merging LoRA in fp16...\")\nmerged_model = peft_model.merge_and_unload()\nprint(f\"\\u2705 Merged: {merged_model.num_parameters():,} params\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T15:42:58.295566Z","iopub.execute_input":"2026-02-13T15:42:58.295779Z","iopub.status.idle":"2026-02-13T15:43:00.383051Z","shell.execute_reply.started":"2026-02-13T15:42:58.295757Z","shell.execute_reply":"2026-02-13T15:43:00.382279Z"}},"outputs":[{"name":"stdout","text":"üóëÔ∏è Training model freed\nüîó Loading fresh Qwen/Qwen3-0.6B in fp16 for merge...\nüîÄ Merging LoRA in fp16...\n‚úÖ Merged: 596,049,920 params\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 10. DAPT Evaluation ‚Äî Quality Metrics\n\n**v1.1 improvements over v1.0:**\n1. **Perplexity** on held-out blocks (measures language modeling quality)\n2. **Tamil word validity** ‚Äî not just char %, but actual word-level quality\n3. **Side-by-side with vanilla instruct** ‚Äî did DAPT actually help?\n4. **`<think>` token suppression** via `bad_words_ids`\n\nThis is an **instruct** model ‚Äî it may try to generate in chat format, but for DAPT eval\nwe test raw text continuation (same as v1.0 for comparability).","metadata":{}},{"cell_type":"code","source":"merged_model.eval()\nmerged_model.config.use_cache = True\n\n# Compute perplexity on held-out validation blocks\nprint(\"\\U0001f4ca Computing perplexity on validation set...\")\neval_losses = []\nn_eval_samples = min(200, len(eval_dataset))  # Cap for speed\n\nfor i in range(n_eval_samples):\n    input_ids = torch.tensor([eval_dataset[i][\"input_ids\"]], dtype=torch.long).to(merged_model.device)\n    labels = input_ids.clone()\n    with torch.no_grad():\n        outputs = merged_model(input_ids=input_ids, labels=labels)\n        eval_losses.append(outputs.loss.item())\n\navg_eval_loss = np.mean(eval_losses)\neval_ppl = np.exp(min(avg_eval_loss, 20))\n\nprint(f\"   Samples:    {n_eval_samples}\")\nprint(f\"   Avg Loss:   {avg_eval_loss:.4f}\")\nprint(f\"   Perplexity: {eval_ppl:.1f}\")\n\nif eval_ppl < 5:\n    print(f\"   \\u2705 Excellent ‚Äî low perplexity indicates strong Tamil fluency\")\nelif eval_ppl < 15:\n    print(f\"   \\u2705 Good ‚Äî reasonable perplexity for adapted model\")\nelif eval_ppl < 50:\n    print(f\"   \\u26a0\\ufe0f  Moderate ‚Äî DAPT helped somewhat but room for improvement\")\nelse:\n    print(f\"   \\u274c High perplexity ‚Äî DAPT may not have converged\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T15:43:00.384051Z","iopub.execute_input":"2026-02-13T15:43:00.384370Z","iopub.status.idle":"2026-02-13T15:43:57.819133Z","shell.execute_reply.started":"2026-02-13T15:43:00.384329Z","shell.execute_reply":"2026-02-13T15:43:57.818468Z"}},"outputs":[{"name":"stdout","text":"üìä Computing perplexity on validation set...\n   Samples:    200\n   Avg Loss:   0.9993\n   Perplexity: 2.7\n   ‚úÖ Excellent ‚Äî low perplexity indicates strong Tamil fluency\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# === GENERATION EVAL ===\neval_prompts = [\n    (\"prose\", \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\\u0ba8\\u0bbe\\u0b9f\\u0bc1 \\u0b87\\u0ba8\\u0bcd\\u0ba4\\u0bbf\\u0baf\\u0bbe\\u0bb5\\u0bbf\\u0ba9\\u0bcd \\u0ba4\\u0bc6\\u0ba9\\u0bcd \\u0baa\\u0b95\\u0bc1\\u0ba4\\u0bbf\\u0baf\\u0bbf\\u0bb2\\u0bcd \\u0b85\\u0bae\\u0bc8\\u0ba8\\u0bcd\\u0ba4\\u0bc1\\u0bb3\\u0bcd\\u0bb3 \\u0b92\\u0bb0\\u0bc1 \\u0bae\\u0bbe\\u0ba8\\u0bbf\\u0bb2\\u0bae\\u0bcd.\"),\n    (\"prose\", \"\\u0baa\\u0bca\\u0b99\\u0bcd\\u0b95\\u0bb2\\u0bcd \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bb0\\u0bcd\\u0b95\\u0bb3\\u0bbf\\u0ba9\\u0bcd \\u0bae\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bbf\\u0baf \\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0ba8\\u0bbe\\u0bb3\\u0bcd.\"),\n    (\"literature\", \"\\u0bb5\\u0bb3\\u0bcd\\u0bb3\\u0bc1\\u0bb5\\u0bb0\\u0bcd \\u0b95\\u0bc2\\u0bb1\\u0bbf\\u0baf \\u0b85\\u0bb1\\u0bae\\u0bcd, \\u0baa\\u0bca\\u0bb0\\u0bc1\\u0bb3\\u0bcd, \\u0b87\\u0ba9\\u0bcd\\u0baa\\u0bae\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0bb1 \\u0bae\\u0bc2\\u0ba9\\u0bcd\\u0bb1\\u0bc1\"),\n    (\"knowledge\", \"\\u0b9a\\u0bbf\\u0ba4\\u0bcd\\u0ba4 \\u0bae\\u0bb0\\u0bc1\\u0ba4\\u0bcd\\u0ba4\\u0bc1\\u0bb5\\u0bae\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0baa\\u0ba4\\u0bc1 \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd \\u0bae\\u0b95\\u0bcd\\u0b95\\u0bb3\\u0bbf\\u0ba9\\u0bcd \\u0baa\\u0bbe\\u0bb0\\u0bae\\u0bcd\\u0baa\\u0bb0\\u0bbf\\u0baf\"),\n    (\"daily\", \"\\u0b95\\u0bbe\\u0bb2\\u0bc8\\u0baf\\u0bbf\\u0bb2\\u0bcd \\u0b8e\\u0bb4\\u0bc1\\u0ba8\\u0bcd\\u0ba4\\u0ba4\\u0bc1\\u0bae\\u0bcd \\u0bae\\u0bc1\\u0ba4\\u0bb2\\u0bbf\\u0bb2\\u0bcd\"),\n    (\"short\", \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\"),\n    (\"short\", \"\\u0ba8\\u0ba9\\u0bcd\\u0bb1\\u0bbf\"),\n    (\"mixed\", \"India has many languages. \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd is one of the\"),\n]\n\nprint(f\"\\n{'='*60}\")\nprint(f\"\\U0001f9ea DAPT v1.1 EVAL: {len(eval_prompts)} Tamil text continuations\")\nprint(f\"   (Instruct model \\u2014 <think> suppressed with bad_words_ids)\")\nprint(f\"{'='*60}\")\n\neval_results = []\n\nfor category, prompt_text in eval_prompts:\n    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(merged_model.device)\n\n    with torch.no_grad():\n        outputs = merged_model.generate(\n            **inputs,\n            max_new_tokens=150,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            repetition_penalty=1.2,\n            no_repeat_ngram_size=4,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n            bad_words_ids=[[tid] for tid in THINK_TOKEN_IDS],  # Suppress <think> tokens\n        )\n\n    generated_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n\n    # Char-level metrics\n    t_pct = tamil_char_pct(response)\n    words = response.split()\n    unique_ratio = len(set(words)) / max(len(words), 1)\n\n    # Word-level Tamil quality (NEW in v1.1)\n    tamil_words, total_words = count_tamil_words(response)\n    tamil_word_pct = 100.0 * tamil_words / max(total_words, 1)\n\n    # Quality flags\n    is_repetitive = unique_ratio < 0.3 and len(words) > 10\n    is_empty = len(response.strip()) < 10\n    is_code = any(kw in response[:100] for kw in ['def ', 'class ', 'import ', '{\"', 'var '])\n    has_think = '<think>' in response or '</think>' in response\n\n    status = \"\\u2705\"\n    if is_empty: status = \"\\u274c EMPTY\"\n    elif is_code: status = \"\\u274c CODE\"\n    elif has_think: status = \"\\u274c THINK\"\n    elif is_repetitive: status = \"\\u26a0\\ufe0f LOOP\"\n    elif t_pct < 20 and category != \"mixed\": status = \"\\u26a0\\ufe0f LOW TAMIL\"\n\n    eval_results.append({\n        \"category\": category,\n        \"prompt\": prompt_text,\n        \"response\": response[:300],\n        \"status\": status,\n        \"tamil_char_pct\": t_pct,\n        \"tamil_word_pct\": tamil_word_pct,\n        \"unique_ratio\": unique_ratio,\n    })\n\n    print(f\"\\n[{category.upper()}] {status} (Char: {t_pct:.0f}%, Words: {tamil_word_pct:.0f}%, Unique: {unique_ratio:.0%})\")\n    print(f\"  Prompt: {prompt_text[:60]}\")\n    print(f\"  Output: {response[:300]}\")\n    print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T15:43:57.820096Z","iopub.execute_input":"2026-02-13T15:43:57.820371Z","iopub.status.idle":"2026-02-13T15:44:33.467478Z","shell.execute_reply.started":"2026-02-13T15:43:57.820343Z","shell.execute_reply":"2026-02-13T15:44:33.466725Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nüß™ DAPT v1.1 EVAL: 8 Tamil text continuations\n   (Instruct model ‚Äî <think> suppressed with bad_words_ids)\n============================================================\n\n[PROSE] ‚úÖ (Char: 81%, Words: 94%, Unique: 100%)\n  Prompt: ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡ØÅ ‡Æá‡Æ®‡Øç‡Æ§‡Æø‡ÆØ‡Ææ‡Æµ‡Æø‡Æ©‡Øç ‡Æ§‡ØÜ‡Æ©‡Øç ‡Æ™‡Æï‡ØÅ‡Æ§‡Æø‡ÆØ‡Æø‡Æ≤‡Øç ‡ÆÖ‡ÆÆ‡Øà‡Æ®‡Øç‡Æ§‡ØÅ‡Æ≥‡Øç‡Æ≥ ‡Æí‡Æ∞‡ØÅ ‡ÆÆ‡Ææ‡Æ®‡Æø‡Æ≤‡ÆÆ‡Øç.\n  Output:  ‡Æé‡Æ≤‡ÆØ‡ØÇ‡Æï‡Æø, 30-‡Æá‡Æ±‡Æ™‡Øç‡Æ™‡ØÅ ‡Æï‡Æ£‡Øç‡Æü‡Æø‡Æ∞‡ØÅ‡Æ™‡Øç‡Æ™‡Æ§‡Æô‡Øç‡Æï‡Æ≥‡Øà ‡Æâ‡Æ≥‡Øç‡Æ≥‡Ææ‡Æü‡Øç‡Æö‡Æø ‡Æµ‡Æ¥‡Æô‡Øç‡Æ±‡ØÅ‡Æ≥‡Øç‡ÆΩ.\n‡Æö‡ØÄ‡Æ©‡Ææ‡Æµ‡Øã, ‡Æú‡ØÜ‡Æ™‡Æø‡Æê‡Æµ‡Øã - ‡Æ§‡Øä‡Æ≤‡Øç‡Æ≤‡Øà!\n‡Æé‡Æ≤‡ÆØ‡Øã‡Æü‡ØÅ ‡Æ§‡Øá‡Æ∞‡Øç‡Æµ‡ØÅ‡ÆÉ\n‡ÆÖ‡Æ∏‡Øç‡Æ∏‡Ææ‡Æ∞‡Øç ‡Æì‡Æü‡Øç‡Æü‡Æø‡ÆØ ‡Æ™‡Øã‡Æ∞‡Øç ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æ∑‡Øç‡Æü‡ÆÆ‡Ææ‡Æï ‡Æâ‡ÆØ‡Æ∞‡Øç‡Æ§ÔøΩ\n--------------------------------------------------\n\n[PROSE] ‚ùå EMPTY (Char: 0%, Words: 0%, Unique: 0%)\n  Prompt: ‡Æ™‡Øä‡Æô‡Øç‡Æï‡Æ≤‡Øç ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Æ∞‡Øç‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æ®‡Ææ‡Æ≥‡Øç.\n  Output: \n--------------------------------------------------\n\n[LITERATURE] ‚úÖ (Char: 76%, Words: 100%, Unique: 100%)\n  Prompt: ‡Æµ‡Æ≥‡Øç‡Æ≥‡ØÅ‡Æµ‡Æ∞‡Øç ‡Æï‡ØÇ‡Æ±‡Æø‡ÆØ ‡ÆÖ‡Æ±‡ÆÆ‡Øç, ‡Æ™‡Øä‡Æ∞‡ØÅ‡Æ≥‡Øç, ‡Æá‡Æ©‡Øç‡Æ™‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ± ‡ÆÆ‡ØÇ‡Æ©‡Øç‡Æ±‡ØÅ\n  Output:  ‡Æö‡Ææ‡Æ§‡Æï‡Æ™‡Øç ‡Æ™‡Æ£‡Øç‡Æ£‡Øà‡ÆØ‡Æø‡Æ©‡Øç 3-‡ÆÜ‡Æµ‡Æ§‡ØÅ ‡Æ§‡Æü‡Øà.\n--------------------------------------------------\n\n[KNOWLEDGE] ‚úÖ (Char: 80%, Words: 80%, Unique: 100%)\n  Prompt: ‡Æö‡Æø‡Æ§‡Øç‡Æ§ ‡ÆÆ‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ™‡Æ§‡ØÅ ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡Æ™‡Ææ‡Æ∞‡ÆÆ‡Øç‡Æ™‡Æ∞‡Æø‡ÆØ\n  Output: ‡Æ™‡Øç‡Æ™‡Æü‡Æø 2014-‡Æ≤‡Øç, \"‡ÆÖ‡Æ±‡Æø‡Æµ‡Øã ‡ÆÖ‡Æ¥‡ØÅ‡Æ§‡Øç‡Æü‡ØÅ\" (‡ÆÜ‡Æ£.‡Æú.) - ‡Æá‡ÆØ‡Æï‡Øç‡Æï‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡Æ®‡Æü‡Æø‡Æ§‡Øç‡Æö‡Ææ‡Æ©‡Øç ‡Æ∞‡Æô‡Øç‡Æï‡Æ®‡Ææ‡Æ§‡Øç, ‡ÆÜ‡Æ∞‡Ææ‡ÆØ‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æâ‡Æ±‡ØÅ‡Æ™‡Øç‡Æ™‡Æø‡Æ©‡Æ∞‡Øç ‡Æ∏‡Øç‡Æ∞‡ØÄ‡Æµ‡Øà‡Æ§‡Øç‡Æ§‡ØÇ‡Æ∞‡Øç ‡Æê‡Æ∏‡Øç‡Æµ‡Æ∞‡Øç‡ÆØ‡Ææ ‡Æì‡Æü‡Øç‡Æü‡ÆÆ‡Øç ‡Æè‡Æ±‡Øç‡Æ±‡ØÅ‡Æï‡Øç‡Æï‡Øä‡Æ£‡Øç‡Æü‡Æø‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡Æ©\n--------------------------------------------------\n\n[DAILY] ‚úÖ (Char: 51%, Words: 60%, Unique: 100%)\n  Prompt: ‡Æï‡Ææ‡Æ≤‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡Æé‡Æ¥‡ØÅ‡Æ®‡Øç‡Æ§‡Æ§‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Æø‡Æ≤‡Øç\n  Output:  40% (16:32) ‡Æï‡Æ£‡Æµ‡Æ∞‡Øç ‡Æ™‡Æü‡Æø, ‡ÆÖ‡Æ™‡Øç‡Æ≥‡Øá‡Æ©‡Æø‡ÆØ‡Æ∏‡Øç.\n--------------------------------------------------\n\n[SHORT] ‚úÖ (Char: 73%, Words: 74%, Unique: 100%)\n  Prompt: ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç\n  Output:  ‡Æ§‡ØÜ‡Æ©‡Øç‡Æ©‡Ææ‡Æ∞‡Æö‡ØÅ‡Æµ‡Æ≤‡Æï‡ÆÆ‡Øç, \"‡ÆÖ‡Æü‡Æø‡Æµ‡Æ≥‡Æø\" (‡ÆÜ‡Æ™‡Æø‡Æ∑‡Øá‡Æï) - 1042.\n‡Æá‡Æ®‡Øç‡Æ®‡Æø‡Æ≤‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡ÆÖ‡Æ£‡Øç‡Æ£‡Æ©‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Æ¥‡Æô‡Øç‡Æï‡Ææ‡Æ≤‡ÆÆ‡Øç ‡Æâ‡ÆØ‡Æø‡Æ∞‡Øç-‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æí‡Æ∞‡ØÅ ‡Æé‡Æ£‡Øç‡Æ£‡Æø‡ÆØ ‡Æú‡ØÄ. . ‡Æ∏‡Øç‡Æü‡Ææ‡Æ≤‡Æø‡Æ©‡Øã‡Æ∏‡Øç(‡Æú‡ØÇ. ) , '‡Æ™‡Øä‡ÆØ‡Øç‡ÆØ‡Ææ' ‡Æ®‡Æø‡Æ∞‡Øç‡Æµ\n--------------------------------------------------\n\n[SHORT] ‚úÖ (Char: 62%, Words: 64%, Unique: 100%)\n  Prompt: ‡Æ®‡Æ©‡Øç‡Æ±‡Æø\n  Output:  ‡Æé‡Æ¥‡ØÅ‡Æ§‡Æø‡Æ©‡Øá‡Æ©‡Øç. . !\n‡Æ™‡Æü‡ÆÆ‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æ™‡ÆØ‡Æ£‡Æö‡Øç‡Æö‡Æï‡Æ≤‡Ææ‡Æ© ‡ÆÖ‡Æ∞‡Æµ‡Æø‡Æï‡Øç‡Æ∞‡Æπ ‡Æµ‡Æ≥‡Æô‡Øç‡Æï‡Æ≥‡Æø‡Æ≤‡Øç, '‡Æá‡Æ§‡Øç‡Æ§‡Æø‡Æ£‡Øà' (The Sutra of the Indian Forests) ‡Æí‡Æ©‡Øç‡Æ±‡Øã‡Æü‡ØÅ 1280 ‡ÆÜ‡Æ£‡Øç‡Æü‡ØÅ‡Æï‡Æ≥‡Øç ‡Æâ‡Æ£‡Æµ‡ØÅ ‡ÆØ‡ØÇ‡Æ∏‡Æø‡Æ≤‡Øç ‡Æè‡Æ¥‡Øà ‡Æ∏‡Øç‡Æ™‡ØÄ-‡Æé‡ÆÆ‡Øç ‡Æê‡Æö‡Æø‡Æ™‡Æø‡Æ≥‡Øç ‡Æú‡Øã‡Æü‡Æø‡ÆØ‡Ææ‡Æï ‡Æá\n--------------------------------------------------\n\n[MIXED] ‚úÖ (Char: 51%, Words: 55%, Unique: 100%)\n  Prompt: India has many languages. ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç is one of the\n  Output:  most widely spoken language in India, with more than 10 million speakers.\n‡Æï‡ØÇ‡Æü‡Øç‡Æü‡ØÅ‡Æ§‡Æ≤‡Ææ‡Æ≥‡Æ∞‡Øç\n‡Æµ‡Æ©‡Æö‡Æø : \"‡Æá‡Æ®‡Øç‡Æ§ ‡ÆÆ‡Æ£‡Øç‡Æ£‡Øà‡ÆØ‡Ææ‡Æ≤‡Øç ‡Æá‡Æ±‡Æ™‡Øç‡Æ™‡Æ§‡Æ±‡Øç‡Æï‡ØÅ ‡Æâ‡Æü‡Øç‡Æ™‡Æü‡Øç‡Æü ‡ÆÜ‡Æ¥‡Øç‡Æµ‡Ææ‡Æï ‡ÆÖ‡Æô‡Øç‡Æï‡ØÄ‡Æï‡Æ∞‡Æø‡ÆØ‡ØÅ‡ÆÆ‡Øç ‡Æö‡Øä‡Æ©‡Øç‡Æ© ‡Æí‡Æ∞‡ØÅ ‡Æé‡Æ£‡Øç‡Æ£‡ÆÆ‡Øç\" ‡Æ∞‡Æ∏‡Øç‡Æ∏‡Æø‡Æ≤‡Øç ‡Æì‡Æü‡Æø‡ÆØ ‡Æï‡Øã‡Æ≤‡Øà ‡Æè‡Æ©‡Øç? !\n‡ÆÖ‡Æü‡Øç‡Æü‡Øà ‡ÆÆ‡Æï\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# === EVAL SUMMARY ===\nprint(f\"\\n{'='*60}\")\nprint(f\"\\U0001f4ca DAPT v1.1 EVAL SUMMARY\")\nprint(f\"{'='*60}\")\n\npass_count = sum(1 for r in eval_results if r['status'] == \"\\u2705\")\navg_tamil_char = np.mean([r['tamil_char_pct'] for r in eval_results])\navg_tamil_word = np.mean([r['tamil_word_pct'] for r in eval_results])\navg_unique = np.mean([r['unique_ratio'] for r in eval_results])\n\nprint(f\"   Passed:         {pass_count}/{len(eval_results)}\")\nprint(f\"   Avg Tamil char: {avg_tamil_char:.0f}%\")\nprint(f\"   Avg Tamil word: {avg_tamil_word:.0f}% (NEW \\u2014 more meaningful than char%)\")\nprint(f\"   Avg Unique:     {avg_unique:.0%}\")\nprint(f\"   Eval PPL:       {eval_ppl:.1f}\")\nprint()\n\nprint(f\"{'Status':<10} {'Category':<12} {'Char%':>6} {'Word%':>6} {'Unique':>7}\")\nprint(\"-\" * 50)\nfor r in eval_results:\n    print(f\"{r['status']:<10} [{r['category']}] {r['tamil_char_pct']:>5.0f}% {r['tamil_word_pct']:>5.0f}% {r['unique_ratio']:>6.0%}\")\n\nprint(f\"\\n\\U0001f4cb v1.0 Baseline for comparison:\")\nprint(f\"   v1.0: 8/8 pass, avg Tamil char 66%, PPL ~2.8 (base model)\")\nprint(f\"   v1.0 comparison: DAPT vs Base was -2% (DAPT didn't help)\")\n\nif pass_count >= len(eval_results) * 0.7 and avg_tamil_word > 40:\n    print(f\"\\n\\U0001f389 DAPT v1.1 successful! Proceed to SFT.\")\nelif pass_count >= len(eval_results) * 0.5:\n    print(f\"\\n\\u26a0\\ufe0f  Partial. May still be better than v1.0 ‚Äî run comparison notebook.\")\nelse:\n    print(f\"\\n\\u274c DAPT v1.1 failed. Check loss curve, data quality.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T15:44:33.468464Z","iopub.execute_input":"2026-02-13T15:44:33.468701Z","iopub.status.idle":"2026-02-13T15:44:33.476425Z","shell.execute_reply.started":"2026-02-13T15:44:33.468676Z","shell.execute_reply":"2026-02-13T15:44:33.475742Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nüìä DAPT v1.1 EVAL SUMMARY\n============================================================\n   Passed:         7/8\n   Avg Tamil char: 59%\n   Avg Tamil word: 66% (NEW ‚Äî more meaningful than char%)\n   Avg Unique:     88%\n   Eval PPL:       2.7\n\nStatus     Category      Char%  Word%  Unique\n--------------------------------------------------\n‚úÖ          [prose]    81%    94%   100%\n‚ùå EMPTY    [prose]     0%     0%     0%\n‚úÖ          [literature]    76%   100%   100%\n‚úÖ          [knowledge]    80%    80%   100%\n‚úÖ          [daily]    51%    60%   100%\n‚úÖ          [short]    73%    74%   100%\n‚úÖ          [short]    62%    64%   100%\n‚úÖ          [mixed]    51%    55%   100%\n\nüìã v1.0 Baseline for comparison:\n   v1.0: 8/8 pass, avg Tamil char 66%, PPL ~2.8 (base model)\n   v1.0 comparison: DAPT vs Base was -2% (DAPT didn't help)\n\nüéâ DAPT v1.1 successful! Proceed to SFT.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## 10a. Side-by-Side: DAPT v1.1 vs Vanilla Instruct\n\nLoad vanilla Qwen3-0.6B instruct and run the same prompts.\nThis tells us if DAPT actually improved Tamil over the base instruct model.","metadata":{}},{"cell_type":"code","source":"# Free merged model to load vanilla\ndel merged_model\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(f\"\\U0001f4e5 Loading vanilla {BASE_MODEL} for comparison...\")\nvanilla_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=torch.float16,\n    device_map={\"\":0},\n    trust_remote_code=True,\n)\nvanilla_model.eval()\nvanilla_model.config.use_cache = True\nprint(\"\\u2705 Vanilla instruct model loaded\")\n\nvanilla_results = []\nfor category, prompt_text in eval_prompts:\n    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(vanilla_model.device)\n    with torch.no_grad():\n        outputs = vanilla_model.generate(\n            **inputs,\n            max_new_tokens=150,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            repetition_penalty=1.2,\n            no_repeat_ngram_size=4,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n            bad_words_ids=[[tid] for tid in THINK_TOKEN_IDS],\n        )\n    generated_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n    t_pct = tamil_char_pct(response)\n    tamil_words, total_words = count_tamil_words(response)\n    tamil_word_pct = 100.0 * tamil_words / max(total_words, 1)\n    words = response.split()\n    unique_ratio = len(set(words)) / max(len(words), 1)\n    vanilla_results.append({\n        \"category\": category,\n        \"response\": response[:300],\n        \"tamil_char_pct\": t_pct,\n        \"tamil_word_pct\": tamil_word_pct,\n        \"unique_ratio\": unique_ratio,\n    })\n\ndel vanilla_model\ngc.collect()\ntorch.cuda.empty_cache()\nprint(\"\\U0001f5d1\\ufe0f Vanilla model freed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T15:44:33.477468Z","iopub.execute_input":"2026-02-13T15:44:33.477708Z","iopub.status.idle":"2026-02-13T15:45:26.418182Z","shell.execute_reply.started":"2026-02-13T15:44:33.477686Z","shell.execute_reply":"2026-02-13T15:45:26.417488Z"}},"outputs":[{"name":"stdout","text":"üì• Loading vanilla Qwen/Qwen3-0.6B for comparison...\n‚úÖ Vanilla instruct model loaded\nüóëÔ∏è Vanilla model freed\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# === COMPARISON TABLE ===\nprint(\"=\" * 80)\nprint(\"\\U0001f4ca SIDE-BY-SIDE: Vanilla Instruct vs DAPT v1.1\")\nprint(\"=\" * 80)\n\nprint(f\"\\n{'Category':<12} {'Van Char%':>10} {'DAPT Char%':>11} {'Van Word%':>10} {'DAPT Word%':>11} {'Winner':>8}\")\nprint(\"-\" * 80)\n\ndapt_wins_char = 0\ndapt_wins_word = 0\n\nfor v, d in zip(vanilla_results, eval_results):\n    # Use word% as primary metric (more meaningful than char%)\n    winner = \"DAPT\" if d[\"tamil_word_pct\"] > v[\"tamil_word_pct\"] + 5 else (\n        \"VANILLA\" if v[\"tamil_word_pct\"] > d[\"tamil_word_pct\"] + 5 else \"TIE\"\n    )\n    if d[\"tamil_char_pct\"] > v[\"tamil_char_pct\"]: dapt_wins_char += 1\n    if d[\"tamil_word_pct\"] > v[\"tamil_word_pct\"]: dapt_wins_word += 1\n\n    print(f\"{v['category']:<12} {v['tamil_char_pct']:>8.0f}% {d['tamil_char_pct']:>9.0f}% {v['tamil_word_pct']:>8.0f}% {d['tamil_word_pct']:>9.0f}%  {winner:>8}\")\n\nprint(\"-\" * 80)\n\navg_van_char = np.mean([r[\"tamil_char_pct\"] for r in vanilla_results])\navg_dapt_char = np.mean([r[\"tamil_char_pct\"] for r in eval_results])\navg_van_word = np.mean([r[\"tamil_word_pct\"] for r in vanilla_results])\navg_dapt_word = np.mean([r[\"tamil_word_pct\"] for r in eval_results])\n\nprint(f\"{'AVERAGE':<12} {avg_van_char:>8.0f}% {avg_dapt_char:>9.0f}% {avg_van_word:>8.0f}% {avg_dapt_word:>9.0f}%\")\n\nprint(f\"\\n\\U0001f3c6 DAPT wins (char%): {dapt_wins_char}/{len(eval_prompts)}\")\nprint(f\"   DAPT wins (word%): {dapt_wins_word}/{len(eval_prompts)}\")\nprint(f\"   Char% change: {avg_van_char:.0f}% \\u2192 {avg_dapt_char:.0f}% ({avg_dapt_char - avg_van_char:+.0f}%)\")\nprint(f\"   Word% change: {avg_van_word:.0f}% \\u2192 {avg_dapt_word:.0f}% ({avg_dapt_word - avg_van_word:+.0f}%)\")\n\nif avg_dapt_word > avg_van_word + 10:\n    print(f\"\\n\\U0001f389 DAPT v1.1 CLEARLY improved Tamil! Proceed to SFT.\")\nelif avg_dapt_word > avg_van_word + 3:\n    print(f\"\\n\\u2705 DAPT v1.1 shows meaningful improvement. Proceed to SFT.\")\nelif avg_dapt_word > avg_van_word:\n    print(f\"\\n\\u26a0\\ufe0f  Marginal improvement. May need more training tokens.\")\nelse:\n    print(f\"\\n\\u274c DAPT v1.1 didn't improve Tamil word quality. Investigate.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T15:45:26.419184Z","iopub.execute_input":"2026-02-13T15:45:26.419439Z","iopub.status.idle":"2026-02-13T15:45:26.428674Z","shell.execute_reply.started":"2026-02-13T15:45:26.419415Z","shell.execute_reply":"2026-02-13T15:45:26.428023Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüìä SIDE-BY-SIDE: Vanilla Instruct vs DAPT v1.1\n================================================================================\n\nCategory      Van Char%  DAPT Char%  Van Word%  DAPT Word%   Winner\n--------------------------------------------------------------------------------\nprose               0%        81%        0%        94%      DAPT\nprose               8%         0%        6%         0%   VANILLA\nliterature          5%        76%        4%       100%      DAPT\nknowledge          11%        80%        7%        80%      DAPT\ndaily               5%        51%        1%        60%      DAPT\nshort               3%        73%        2%        74%      DAPT\nshort               0%        62%        0%        64%      DAPT\nmixed               0%        51%        0%        55%      DAPT\n--------------------------------------------------------------------------------\nAVERAGE             4%        59%        3%        66%\n\nüèÜ DAPT wins (char%): 7/8\n   DAPT wins (word%): 7/8\n   Char% change: 4% ‚Üí 59% (+55%)\n   Word% change: 3% ‚Üí 66% (+63%)\n\nüéâ DAPT v1.1 CLEARLY improved Tamil! Proceed to SFT.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# === DETAILED SIDE-BY-SIDE OUTPUT ===\nprint(\"=\" * 70)\nprint(\"\\U0001f50d DETAILED OUTPUT COMPARISON\")\nprint(\"=\" * 70)\n\nfor v, d in zip(vanilla_results, eval_results):\n    print(f\"\\n\\u250c\\u2500 [{d['category'].upper()}] Prompt: {d['prompt'][:60]}\")\n    print(f\"\\u2502\")\n    print(f\"\\u2502 VANILLA (Char {v['tamil_char_pct']:.0f}%, Word {v['tamil_word_pct']:.0f}%):\")\n    print(f\"\\u2502   {v['response'][:200]}\")\n    print(f\"\\u2502\")\n    print(f\"\\u2502 DAPT v1.1 (Char {d['tamil_char_pct']:.0f}%, Word {d['tamil_word_pct']:.0f}%):\")\n    print(f\"\\u2502   {d['response'][:200]}\")\n    print(f\"\\u2514{'\\u2500' * 69}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T15:45:26.429559Z","iopub.execute_input":"2026-02-13T15:45:26.429888Z","iopub.status.idle":"2026-02-13T15:45:26.448206Z","shell.execute_reply.started":"2026-02-13T15:45:26.429864Z","shell.execute_reply":"2026-02-13T15:45:26.447601Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüîç DETAILED OUTPUT COMPARISON\n======================================================================\n\n‚îå‚îÄ [PROSE] Prompt: ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡ØÅ ‡Æá‡Æ®‡Øç‡Æ§‡Æø‡ÆØ‡Ææ‡Æµ‡Æø‡Æ©‡Øç ‡Æ§‡ØÜ‡Æ©‡Øç ‡Æ™‡Æï‡ØÅ‡Æ§‡Æø‡ÆØ‡Æø‡Æ≤‡Øç ‡ÆÖ‡ÆÆ‡Øà‡Æ®‡Øç‡Æ§‡ØÅ‡Æ≥‡Øç‡Æ≥ ‡Æí‡Æ∞‡ØÅ ‡ÆÆ‡Ææ‡Æ®‡Æø‡Æ≤‡ÆÆ‡Øç.\n‚îÇ\n‚îÇ VANILLA (Char 0%, Word 0%):\n‚îÇ    1780-2356, 495 - 2355 (1850)‡ÆÜ‡Æï a time period that is between the end of 17th and beginning of 18th century in India's history.\n\nThis text says something about how the Indian state was structured duri\n‚îÇ\n‚îÇ DAPT v1.1 (Char 81%, Word 94%):\n‚îÇ    ‡Æé‡Æ≤‡ÆØ‡ØÇ‡Æï‡Æø, 30-‡Æá‡Æ±‡Æ™‡Øç‡Æ™‡ØÅ ‡Æï‡Æ£‡Øç‡Æü‡Æø‡Æ∞‡ØÅ‡Æ™‡Øç‡Æ™‡Æ§‡Æô‡Øç‡Æï‡Æ≥‡Øà ‡Æâ‡Æ≥‡Øç‡Æ≥‡Ææ‡Æü‡Øç‡Æö‡Æø ‡Æµ‡Æ¥‡Æô‡Øç‡Æ±‡ØÅ‡Æ≥‡Øç‡ÆΩ.\n‡Æö‡ØÄ‡Æ©‡Ææ‡Æµ‡Øã, ‡Æú‡ØÜ‡Æ™‡Æø‡Æê‡Æµ‡Øã - ‡Æ§‡Øä‡Æ≤‡Øç‡Æ≤‡Øà!\n‡Æé‡Æ≤‡ÆØ‡Øã‡Æü‡ØÅ ‡Æ§‡Øá‡Æ∞‡Øç‡Æµ‡ØÅ‡ÆÉ\n‡ÆÖ‡Æ∏‡Øç‡Æ∏‡Ææ‡Æ∞‡Øç ‡Æì‡Æü‡Øç‡Æü‡Æø‡ÆØ ‡Æ™‡Øã‡Æ∞‡Øç ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æ∑‡Øç‡Æü‡ÆÆ‡Ææ‡Æï ‡Æâ‡ÆØ‡Æ∞‡Øç‡Æ§ÔøΩ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n‚îå‚îÄ [PROSE] Prompt: ‡Æ™‡Øä‡Æô‡Øç‡Æï‡Æ≤‡Øç ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Æ∞‡Øç‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æ®‡Ææ‡Æ≥‡Øç.\n‚îÇ\n‚îÇ VANILLA (Char 8%, Word 6%):\n‚îÇ    2019-2023‡Æá‡Æ©‡Øç ‡ÆÜ‡Æ£‡Æø - ‡∞á‡∞Ç‡∞ü‡ØÅ\n‡ÆÖ‡Æ§‡±Å‡∞∞‡∞ï‡µç‡¥∑‡∞≤‡¥ø‡¥Ø‡µã, ‡∞¶‡∞∏‡≥ç‡∞§‡∞≤‡¥∏‡¥®‡∞ø‡ÆØ‡±Ä‡∞® ‡¥Ö‡¥§‡µç, ‡∞™‡±å‡¥∞‡∞∂‡±É‡∞£‡∞Ø ‡∞ó‡∞æ‡∞µ‡∞°‡∞Ç... ‡∞â‡∞¶‡±ç‡∞¨‡±ç‡∞∞‡∞ö‡±Å, ‡∞à ‡∞ú‡∞ó‡∞ß‡±Å‡∞≤‡∞Ç ‡∞≤‡±ã ‡∞é‡∞Ç‡∞™‡∞¢‡±Å‡∞≤‡±Å, 2564-2573 ‡∞µ‡∞∞‡±ç‡∞∑‡∞Æ‡±Å\n‚îÇ\n‚îÇ DAPT v1.1 (Char 0%, Word 0%):\n‚îÇ   \n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n‚îå‚îÄ [LITERATURE] Prompt: ‡Æµ‡Æ≥‡Øç‡Æ≥‡ØÅ‡Æµ‡Æ∞‡Øç ‡Æï‡ØÇ‡Æ±‡Æø‡ÆØ ‡ÆÖ‡Æ±‡ÆÆ‡Øç, ‡Æ™‡Øä‡Æ∞‡ØÅ‡Æ≥‡Øç, ‡Æá‡Æ©‡Øç‡Æ™‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ± ‡ÆÆ‡ØÇ‡Æ©‡Øç‡Æ±‡ØÅ\n‚îÇ\n‚îÇ VANILLA (Char 5%, Word 4%):\n‚îÇ   ‡Æï‡Øç‡Æ∑‡Æ§‡Ææ‡Æ£‡Æø ‡Æ§‡ÆØ‡Ææ‡Æö‡Æø‡Æ≤‡Øç 10000 ‡Æö‡ÆÆ‡ØÄ‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü. What is the total number of days in which he worked?  \nA) 250  \nB) 367  \nC) 480  \nD) 900  \n\nAnswer: \\boxed{B}\n\nLet me think... Well, I need to figure out how many days\n‚îÇ\n‚îÇ DAPT v1.1 (Char 76%, Word 100%):\n‚îÇ    ‡Æö‡Ææ‡Æ§‡Æï‡Æ™‡Øç ‡Æ™‡Æ£‡Øç‡Æ£‡Øà‡ÆØ‡Æø‡Æ©‡Øç 3-‡ÆÜ‡Æµ‡Æ§‡ØÅ ‡Æ§‡Æü‡Øà.\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n‚îå‚îÄ [KNOWLEDGE] Prompt: ‡Æö‡Æø‡Æ§‡Øç‡Æ§ ‡ÆÆ‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ™‡Æ§‡ØÅ ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡Æ™‡Ææ‡Æ∞‡ÆÆ‡Øç‡Æ™‡Æ∞‡Æø‡ÆØ\n‚îÇ\n‚îÇ VANILLA (Char 11%, Word 7%):\n‚îÇ    ‡ÆÜ‡ÆØ‡Øç‡Æµ‡Æ±‡Øç‡Æ±‡Øà‡Æï‡Æ≥‡Æø‡Æ≤‡Øç, ‡∞®‡∞ø‡∞≤‡±Å‡∞ï‡Æø ‡Æá‡Æ∞‡Æ£‡Øç‡Æü‡ØÅ (2) 2019-2023 ‡§ö‡§≤‡§æ‡§è ‡§π‡•Å‡§à ‡§¨‡•â‡§∞‡•ç‡§°‡§∞ ‡§∏‡§Ç‡§ó‡§†‡§® ‡§ï‡•á ‡§Æ‡•á‡§Ç‡§∏‡§ø‡§∏‡•Ä‡§è‡§≤ ‡§™‡•ç‡§∞‡§§‡§ø ‡§ú‡§Ø‡•ã‡§¶‡§æ ‡§ó‡§°‡§º‡§™ ‡§ñ‡§ö‡§ï‡§æ‡§∞ ‡§ï‡§æ‡§∞‡§ñanks‡§Æ ‡§¶‡§¨‡§æ‡§ä ‡§´‡•á‡§Ç‡§ï‡§®‡§æ ‡§Ö‡§•‡§µ‡§æ ‡¶∏‡ßç‡¶•‡¶ø‡¶∞ ‡§∏‡•ç‡§•‡¶æ‡¶® ‡§ì‡§¢‡§º‡§®‡§æ‡•§\n\nThis is the text. Now write a\n‚îÇ\n‚îÇ DAPT v1.1 (Char 80%, Word 80%):\n‚îÇ   ‡Æ™‡Øç‡Æ™‡Æü‡Æø 2014-‡Æ≤‡Øç, \"‡ÆÖ‡Æ±‡Æø‡Æµ‡Øã ‡ÆÖ‡Æ¥‡ØÅ‡Æ§‡Øç‡Æü‡ØÅ\" (‡ÆÜ‡Æ£.‡Æú.) - ‡Æá‡ÆØ‡Æï‡Øç‡Æï‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡Æ®‡Æü‡Æø‡Æ§‡Øç‡Æö‡Ææ‡Æ©‡Øç ‡Æ∞‡Æô‡Øç‡Æï‡Æ®‡Ææ‡Æ§‡Øç, ‡ÆÜ‡Æ∞‡Ææ‡ÆØ‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æâ‡Æ±‡ØÅ‡Æ™‡Øç‡Æ™‡Æø‡Æ©‡Æ∞‡Øç ‡Æ∏‡Øç‡Æ∞‡ØÄ‡Æµ‡Øà‡Æ§‡Øç‡Æ§‡ØÇ‡Æ∞‡Øç ‡Æê‡Æ∏‡Øç‡Æµ‡Æ∞‡Øç‡ÆØ‡Ææ ‡Æì‡Æü‡Øç‡Æü‡ÆÆ‡Øç ‡Æè‡Æ±‡Øç‡Æ±‡ØÅ‡Æï‡Øç‡Æï‡Øä‡Æ£‡Øç‡Æü‡Æø‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡Æ©\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n‚îå‚îÄ [DAILY] Prompt: ‡Æï‡Ææ‡Æ≤‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡Æé‡Æ¥‡ØÅ‡Æ®‡Øç‡Æ§‡Æ§‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Æø‡Æ≤‡Øç\n‚îÇ\n‚îÇ VANILLA (Char 5%, Word 1%):\n‚îÇ    (‡Æé‡Æü‡Æø)‡Æµ‡Æø‡Æ∞‡Øã‡Æ©‡Æ∞‡Øç 500-1200‡ÆÜ‡Æ£‡Ææ‡Æ§‡Æø‡Æï‡Æ≥‡Øç, ‡∞¶‡±ç‡∞∞‡∞ø‡∞µ‡∞≤‡±Å 476 - 380‡ÆÜ‡Æ£‡Øç. 9/10th of the number is equal to 10, what can we say about the total? \n\nLet's think step by step.\nAnswer:\n\nAssistant: To solve this problem, let‚Äôs\n‚îÇ\n‚îÇ DAPT v1.1 (Char 51%, Word 60%):\n‚îÇ    40% (16:32) ‡Æï‡Æ£‡Æµ‡Æ∞‡Øç ‡Æ™‡Æü‡Æø, ‡ÆÖ‡Æ™‡Øç‡Æ≥‡Øá‡Æ©‡Æø‡ÆØ‡Æ∏‡Øç.\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n‚îå‚îÄ [SHORT] Prompt: ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç\n‚îÇ\n‚îÇ VANILLA (Char 3%, Word 2%):\n‚îÇ    ‡ÆÆ‡Æï‡Øç‡ÆØ‡Ææ‡Æ©‡Øà‡ÆØ‡Æø‡Æ≤‡Øç, ‡∞ï‡±á‡∞Ç‡∞¶‡∞∞‡±ç‡∞∂ ‡∞Ö‡∞®‡∞ø ‡∞∏‡∞Æ‡∞∏‡∞æ‡∞ï‡±Å‡∞≤‡∞™‡±ã‡∞µ‡∞°‡∞Ç ‡∞ö‡±Ü‡∞ü‡±ç‡∞§‡±Å‡∞≤‡±ã ‡∞â‡Æ≥‡±ç‡∞™‡∞æ‡∞§‡∞æ‡∞®‡∞Ç‡∞ó‡∞æ ‡∞§‡∞∞‡∞™‡∞ø‡∞Ç‡∞ö‡∞¨‡∞°‡∞æ?\nAnswer:\n\nAssistant: The **Tamil Nadu** state is known for its rich cultural heritage and historical significance. It has a long h\n‚îÇ\n‚îÇ DAPT v1.1 (Char 73%, Word 74%):\n‚îÇ    ‡Æ§‡ØÜ‡Æ©‡Øç‡Æ©‡Ææ‡Æ∞‡Æö‡ØÅ‡Æµ‡Æ≤‡Æï‡ÆÆ‡Øç, \"‡ÆÖ‡Æü‡Æø‡Æµ‡Æ≥‡Æø\" (‡ÆÜ‡Æ™‡Æø‡Æ∑‡Øá‡Æï) - 1042.\n‡Æá‡Æ®‡Øç‡Æ®‡Æø‡Æ≤‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡ÆÖ‡Æ£‡Øç‡Æ£‡Æ©‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Æ¥‡Æô‡Øç‡Æï‡Ææ‡Æ≤‡ÆÆ‡Øç ‡Æâ‡ÆØ‡Æø‡Æ∞‡Øç-‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æí‡Æ∞‡ØÅ ‡Æé‡Æ£‡Øç‡Æ£‡Æø‡ÆØ ‡Æú‡ØÄ. . ‡Æ∏‡Øç‡Æü‡Ææ‡Æ≤‡Æø‡Æ©‡Øã‡Æ∏‡Øç(‡Æú‡ØÇ. ) , '‡Æ™‡Øä‡ÆØ‡Øç‡ÆØ‡Ææ' ‡Æ®‡Æø‡Æ∞‡Øç‡Æµ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n‚îå‚îÄ [SHORT] Prompt: ‡Æ®‡Æ©‡Øç‡Æ±‡Æø\n‚îÇ\n‚îÇ VANILLA (Char 0%, Word 0%):\n‚îÇ   , 2018 - 30.6.25\n\n# 1479: ‡§è‡§ï ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§ï‡•á ‡§Ö‡§Ç‡§§‡§ó‡•Å‡§£‡§®‡•Ä‡§Ø ‡§∏‡§Æ‡•Ç‡§π ‡§ú‡•ç‡§û‡§æ‡§§ ‡§ï‡§∞‡•á‡§Ç‡•§ \n\n## (A) $\\left\\{ \\frac{n}{n+2} + \\frac{n-1}{n} \\right\\}$\n\n(B) $\\left( n^{\\frac{1}{3}} + n^{2} \\right)^{-1} $\n\n(C) $n(n-1)(n-2)$\n\n(D) \n‚îÇ\n‚îÇ DAPT v1.1 (Char 62%, Word 64%):\n‚îÇ    ‡Æé‡Æ¥‡ØÅ‡Æ§‡Æø‡Æ©‡Øá‡Æ©‡Øç. . !\n‡Æ™‡Æü‡ÆÆ‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æ™‡ÆØ‡Æ£‡Æö‡Øç‡Æö‡Æï‡Æ≤‡Ææ‡Æ© ‡ÆÖ‡Æ∞‡Æµ‡Æø‡Æï‡Øç‡Æ∞‡Æπ ‡Æµ‡Æ≥‡Æô‡Øç‡Æï‡Æ≥‡Æø‡Æ≤‡Øç, '‡Æá‡Æ§‡Øç‡Æ§‡Æø‡Æ£‡Øà' (The Sutra of the Indian Forests) ‡Æí‡Æ©‡Øç‡Æ±‡Øã‡Æü‡ØÅ 1280 ‡ÆÜ‡Æ£‡Øç‡Æü‡ØÅ‡Æï‡Æ≥‡Øç ‡Æâ‡Æ£‡Æµ‡ØÅ ‡ÆØ‡ØÇ‡Æ∏‡Æø‡Æ≤‡Øç ‡Æè‡Æ¥‡Øà ‡Æ∏‡Øç‡Æ™‡ØÄ-‡Æé‡ÆÆ‡Øç ‡Æê‡Æö‡Æø‡Æ™‡Æø‡Æ≥‡Øç ‡Æú‡Øã‡Æü‡Æø‡ÆØ‡Ææ‡Æï ‡Æá\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n‚îå‚îÄ [MIXED] Prompt: India has many languages. ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç is one of the\n‚îÇ\n‚îÇ VANILLA (Char 0%, Word 0%):\n‚îÇ    major ones, but there are other countries where it's spoken more than in India.\n\nBut if you go to a country with only two or three people who speak Tamil, what do you expect that?\n\nA) The population \n‚îÇ\n‚îÇ DAPT v1.1 (Char 51%, Word 55%):\n‚îÇ    most widely spoken language in India, with more than 10 million speakers.\n‡Æï‡ØÇ‡Æü‡Øç‡Æü‡ØÅ‡Æ§‡Æ≤‡Ææ‡Æ≥‡Æ∞‡Øç\n‡Æµ‡Æ©‡Æö‡Æø : \"‡Æá‡Æ®‡Øç‡Æ§ ‡ÆÆ‡Æ£‡Øç‡Æ£‡Øà‡ÆØ‡Ææ‡Æ≤‡Øç ‡Æá‡Æ±‡Æ™‡Øç‡Æ™‡Æ§‡Æ±‡Øç‡Æï‡ØÅ ‡Æâ‡Æü‡Øç‡Æ™‡Æü‡Øç‡Æü ‡ÆÜ‡Æ¥‡Øç‡Æµ‡Ææ‡Æï ‡ÆÖ‡Æô‡Øç‡Æï‡ØÄ‡Æï‡Æ∞‡Æø‡ÆØ‡ØÅ‡ÆÆ‡Øç ‡Æö‡Øä‡Æ©‡Øç‡Æ© ‡Æí‡Æ∞‡ØÅ ‡Æé‡Æ£‡Øç‡Æ£‡ÆÆ‡Øç\" ‡Æ∞‡Æ∏‡Øç‡Æ∏‡Æø‡Æ≤‡Øç ‡Æì‡Æü‡Æø‡ÆØ ‡Æï‡Øã‡Æ≤‡Øà ‡Æè‡Æ©‡Øç? !\n‡ÆÖ‡Æü‡Øç‡Æü‡Øà ‡ÆÆ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 11. Upload Merged Model","metadata":{}},{"cell_type":"code","source":"# Reload the merged model for upload (was freed for vanilla comparison)\nprint(f\"\\U0001f517 Reloading merged model for upload...\")\nbase_model_fp16 = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=torch.float16,\n    device_map={\"\":0},\n    trust_remote_code=True,\n)\npeft_model = PeftModel.from_pretrained(base_model_fp16, ADAPTER_PATH)\npeft_model.eval()\nmerged_model = peft_model.merge_and_unload()\nprint(f\"\\u2705 Merged model ready: {merged_model.num_parameters():,} params\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T15:45:26.449030Z","iopub.execute_input":"2026-02-13T15:45:26.449338Z","iopub.status.idle":"2026-02-13T15:45:27.901823Z","shell.execute_reply.started":"2026-02-13T15:45:26.449301Z","shell.execute_reply":"2026-02-13T15:45:27.901038Z"}},"outputs":[{"name":"stdout","text":"üîó Reloading merged model for upload...\n‚úÖ Merged model ready: 596,049,920 params\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"api = HfApi()\napi.create_repo(OUTPUT_MODEL, exist_ok=True)\n\nprint(f\"\\U0001f4e4 Pushing merged fp16 model to {OUTPUT_MODEL}...\")\nmerged_model.push_to_hub(\n    OUTPUT_MODEL,\n    private=False,\n    commit_message=(\n        f\"DAPT v1.1: Tamil-adapted Qwen3-0.6B (instruct), \"\n        f\"Sangraha NFKC-cleaned, LoRA r={LORA_R}, lr={LEARNING_RATE}\"\n    ),\n)\ntokenizer.push_to_hub(OUTPUT_MODEL)\n\nprint(f\"\\n\\u2705 Model: https://huggingface.co/{OUTPUT_MODEL}\")\nprint(f\"\\u2705 Adapter: https://huggingface.co/{ADAPTER_REPO}\")\nprint(f\"\\n\\U0001f449 Next: Run SFT notebook with BASE_MODEL = \\\"{OUTPUT_MODEL}\\\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T15:45:27.902851Z","iopub.execute_input":"2026-02-13T15:45:27.903221Z","iopub.status.idle":"2026-02-13T15:45:58.424234Z","shell.execute_reply.started":"2026-02-13T15:45:27.903190Z","shell.execute_reply":"2026-02-13T15:45:58.423488Z"}},"outputs":[{"name":"stdout","text":"üì§ Pushing merged fp16 model to CryptoYogi/qwen3-0.6b-tamil-v1_1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b30a81f7ba6b4a329412bea39e83e82f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad7a10a460a44f5daa6e8e55b77353ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e49926de9c042348fef98a74054926e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ad4d5cbb9a040d59d9f0053ea561ac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb5d23b4c0ab4634b26978851da77411"}},"metadata":{}},{"name":"stdout","text":"\n‚úÖ Model: https://huggingface.co/CryptoYogi/qwen3-0.6b-tamil-v1_1\n‚úÖ Adapter: https://huggingface.co/CryptoYogi/qwen3-0.6b-tamil-v1_1-lora\n\nüëâ Next: Run SFT notebook with BASE_MODEL = \"CryptoYogi/qwen3-0.6b-tamil-v1_1\"\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Summary\n\n| Artifact | Repo | Purpose |\n|----------|------|---------|\n| Packed DAPT data | `CryptoYogi/vazhi-dapt-tamil-v1_1` | Cleaned training data (NFKC, 70% Tamil) |\n| Merged fp16 model | `CryptoYogi/qwen3-0.6b-tamil-v1_1` | Reusable Tamil instruct base for SFT |\n| LoRA adapter | `CryptoYogi/qwen3-0.6b-tamil-v1_1-lora` | Recovery backup |\n\n### Changes from v1.0\n| Dimension | v1.0 | v1.1 |\n|-----------|------|------|\n| Model | Qwen3-0.6B-**Base** | Qwen3-0.6B (**Instruct**) |\n| Data | 16M tokens, Tamil >= 50% | **55M tokens**, Tamil >= 70%, NFKC |\n| Sources | Sangraha verified only | Sangraha verified (sufficient at 55M) |\n| Docs/Blocks | ~16K / 31,599 | 27,105 / **52,664** |\n| LR | 2e-5 | **5e-5** |\n| GPUs | T4 x1 (single) | **T4 x2** (DataParallel) |\n| Steps | 500 (capped) | **~1,645** (full epoch) |\n| Est. time | ~4h | **~7h** (dual T4) |\n| Eval | Tamil char% only | Char% + **word%** + perplexity + comparison |\n\n### Next: SFT (Stage 3)\n```python\nBASE_MODEL = \"CryptoYogi/qwen3-0.6b-tamil-v1_1\"  # THIS model\nDATASET = \"CryptoYogi/vazhi-tamil-sft-v4_0\"       # ChatML instruction pairs\n```\n\n### If DAPT v1.1 failed\n1. Check loss curve ‚Äî did it plateau early? May need higher LR or more epochs\n2. Check word% ‚Äî if char% is high but word% is low, model is generating Tamil-like gibberish\n3. Compare with vanilla ‚Äî if no improvement, data quality may still be an issue\n4. Fallback: Sarvam-1 IQ3_M (1.17GB, proven Tamil capability)","metadata":{}}]}
