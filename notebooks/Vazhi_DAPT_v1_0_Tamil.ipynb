{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VAZHI DAPT v1.0 тАФ Tamil Language Adaptation Training\n\n**Pipeline Step 2 of 3:** Train DAPT on pre-built packed Tamil data.\n\n```\nStep 1: Data Prep (DONE тАФ Vazhi_DAPT_Data_v1_0.ipynb)\n  тЖТ Produced: CryptoYogi/vazhi-dapt-tamil-v1_0 (packed 1024-token blocks)\n\nStep 2 (THIS NOTEBOOK): DAPT Training тАФ Kaggle P100 GPU\n  тЖТ Input:  Packed dataset from HF + Qwen3-0.6B-Base\n  тЖТ Output: CryptoYogi/qwen3-0.6b-tamil (reusable Tamil base)\n           CryptoYogi/qwen3-0.6b-tamil-lora (adapter backup)\n\nStep 3: SFT (NEXT тАФ Vazhi_SFT_v3_9_OnDAPT.ipynb)\n  тЖТ Input:  DAPT'd model + ChatML instruction pairs\n  тЖТ Output: CryptoYogi/vazhi-qwen3-v3_9\n```\n\n**Key design (incorporating GPT5.2 review):**\n1. Base model, not Instruct (cleaner DAPT)\n2. Token-budgeted training (max_steps from token count)\n3. Data already packed into 1024-token blocks (no padding waste)\n4. QLoRA r=16 (conservative for 0.6B)\n5. Eval: perplexity on held-out blocks + Tamil generation quality\n6. Adapter + merged model saved separately for recovery\n\n**Target:** Kaggle P100 (16GB) | Est. 2-4 hours","metadata":{}},{"cell_type":"markdown","source":"## 1. Install Dependencies\n\n**After running this cell, RESTART the session** (Runtime тЖТ Restart session)","metadata":{}},{"cell_type":"code","source":"!pip install -q -U \\\n  \"transformers>=4.45.0,<5.0.0\" \\\n  \"accelerate>=0.34.2\" \\\n  \"peft>=0.12.0\" \\\n  \"bitsandbytes>=0.43.3\" \\\n  \"datasets>=2.21.0\" \\\n  \"huggingface_hub>=0.24.7\"\n\nprint(\"\\u2705 Dependencies installed\")\nprint(\"\\u26a0\\ufe0f  RESTART THE SESSION NOW (Runtime \\u2192 Restart session)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T00:33:16.412761Z","iopub.execute_input":"2026-02-13T00:33:16.413254Z","iopub.status.idle":"2026-02-13T00:33:20.935070Z","shell.execute_reply.started":"2026-02-13T00:33:16.413220Z","shell.execute_reply":"2026-02-13T00:33:20.934274Z"}},"outputs":[{"name":"stdout","text":"тЬЕ Dependencies installed\nтЪая╕П  RESTART THE SESSION NOW (Runtime тЖТ Restart session)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 2. Configuration","metadata":{}},{"cell_type":"code","source":"# Force single GPU BEFORE importing torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport json\nimport random\nimport glob\nimport gc\nimport torch\nimport numpy as np\nfrom dataclasses import dataclass\nfrom datasets import load_dataset\nfrom huggingface_hub import login, HfApi\n\nfrom transformers import (\n    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n    TrainerCallback, Trainer, TrainingArguments,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\n# === KEY CONFIG ===\nBASE_MODEL = \"Qwen/Qwen3-0.6B-Base\"  # Base model for DAPT (GPT5.2 #1)\nDATASET_NAME = \"CryptoYogi/vazhi-dapt-tamil-v1_0\"  # Pre-built by Data Prep notebook\nOUTPUT_MODEL = \"CryptoYogi/qwen3-0.6b-tamil\"  # Reusable Tamil base\nADAPTER_REPO = \"CryptoYogi/qwen3-0.6b-tamil-lora\"  # Adapter backup (GPT5.2 #9)\n\n# Training config\nMAX_SEQ_LENGTH = 1024        # Must match data prep notebook\nLEARNING_RATE = 2e-5         # Low LR for gentle adaptation\nLORA_R = 16                  # Conservative rank (GPT5.2 #4)\nLORA_ALPHA = 32\nBATCH_SIZE = 4               # batch 8 OOMs on T4 (Qwen3's 151K vocab = huge logits tensor)\nGRADIENT_ACCUMULATION = 8    # Effective batch = 32\nWARMUP_RATIO = 0.05\nMAX_STEPS_CAP = 500          # Cap to fit compute budget (~16M tokens)\n\nprint(f\"\\u2705 Configuration loaded\")\nprint(f\"   PyTorch: {torch.__version__}\")\nprint(f\"   CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"   GPU: {gpu_name} ({gpu_mem:.0f} GB)\")\n    print(f\"   fp16: ENABLED\")\nprint()\nprint(f\"\\U0001f4cb DAPT Training v1.0:\")\nprint(f\"   Base model:  {BASE_MODEL}\")\nprint(f\"   Dataset:     {DATASET_NAME}\")\nprint(f\"   Output:      {OUTPUT_MODEL}\")\nprint(f\"   LR:          {LEARNING_RATE}\")\nprint(f\"   LoRA:        r={LORA_R}, alpha={LORA_ALPHA}\")\nprint(f\"   Batch:       {BATCH_SIZE} x {GRADIENT_ACCUMULATION} = {BATCH_SIZE * GRADIENT_ACCUMULATION} effective\")\nprint(f\"   Max steps:   {MAX_STEPS_CAP} (~{MAX_STEPS_CAP * BATCH_SIZE * GRADIENT_ACCUMULATION * MAX_SEQ_LENGTH / 1e6:.0f}M tokens)\")\nprint(f\"   fp16:        True\")\nprint(f\"   Grad ckpt:   True (needed for T4 15GB)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T00:33:20.936319Z","iopub.execute_input":"2026-02-13T00:33:20.936602Z","iopub.status.idle":"2026-02-13T00:33:50.557493Z","shell.execute_reply.started":"2026-02-13T00:33:20.936571Z","shell.execute_reply":"2026-02-13T00:33:50.556651Z"}},"outputs":[{"name":"stderr","text":"2026-02-13 00:33:35.375160: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770942815.563951     113 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770942815.618549     113 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770942816.067993     113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770942816.068043     113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770942816.068049     113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770942816.068052     113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"тЬЕ Configuration loaded\n   PyTorch: 2.8.0+cu126\n   CUDA: True\n   GPU: Tesla T4 (15 GB)\n   fp16: ENABLED\n\nЁЯУЛ DAPT Training v1.0:\n   Base model:  Qwen/Qwen3-0.6B-Base\n   Dataset:     CryptoYogi/vazhi-dapt-tamil-v1_0\n   Output:      CryptoYogi/qwen3-0.6b-tamil\n   LR:          2e-05\n   LoRA:        r=16, alpha=32\n   Batch:       4 x 8 = 32 effective\n   Max steps:   500 (~16M tokens)\n   fp16:        True\n   Grad ckpt:   True (needed for T4 15GB)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Login to HuggingFace\nfrom kaggle_secrets import UserSecretsClient\nsecrets = UserSecretsClient()\nhf_token = secrets.get_secret(\"HF_TOKEN\")\nlogin(token=hf_token)\nprint(\"\\u2705 Logged in to HuggingFace\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T00:33:50.558533Z","iopub.execute_input":"2026-02-13T00:33:50.559242Z","iopub.status.idle":"2026-02-13T00:33:50.804178Z","shell.execute_reply.started":"2026-02-13T00:33:50.559212Z","shell.execute_reply":"2026-02-13T00:33:50.803461Z"}},"outputs":[{"name":"stdout","text":"тЬЕ Logged in to HuggingFace\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 3. Load Pre-Built Dataset\n\nDataset was created by `Vazhi_DAPT_Data_v1_0.ipynb`:\n- Sangraha verified Tamil, filtered (Tamil >= 40%, dedup, no repetition)\n- Packed into 1024-token blocks\n- Already split into train/validation","metadata":{}},{"cell_type":"code","source":"print(f\"\\U0001f4e5 Loading pre-built dataset from {DATASET_NAME}...\")\nds = load_dataset(DATASET_NAME)\n\ntrain_dataset = ds[\"train\"]\neval_dataset = ds[\"validation\"]\n\nprint(f\"\\u2705 Dataset loaded:\")\nprint(f\"   Train:      {len(train_dataset):,} blocks\")\nprint(f\"   Validation: {len(eval_dataset):,} blocks\")\nprint(f\"   Block size: {len(train_dataset[0]['input_ids'])} tokens\")\nprint(f\"   Columns:    {train_dataset.column_names}\")\n\ntotal_train_tokens = len(train_dataset) * MAX_SEQ_LENGTH\nprint(f\"   Total train tokens: {total_train_tokens:,}\")\n\n# Verify block size matches our config\nassert len(train_dataset[0][\"input_ids\"]) == MAX_SEQ_LENGTH, \\\n    f\"Block size mismatch: dataset has {len(train_dataset[0]['input_ids'])}, config has {MAX_SEQ_LENGTH}\"\nprint(\"\\u2705 Block size verified\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T00:33:50.804985Z","iopub.execute_input":"2026-02-13T00:33:50.805218Z","iopub.status.idle":"2026-02-13T00:33:54.883251Z","shell.execute_reply.started":"2026-02-13T00:33:50.805195Z","shell.execute_reply":"2026-02-13T00:33:54.882335Z"}},"outputs":[{"name":"stdout","text":"ЁЯУе Loading pre-built dataset from CryptoYogi/vazhi-dapt-tamil-v1_0...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1cc2e27d0e14835b162954359a4e4ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/65.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3da20e4c10b245e78add8fc688b55603"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d08d820c714b4e78850ba346d25a127f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/31599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"429143e9da0c4936a25dc6d8e2f6d6aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/645 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d968342b4bd4ed8aa85aa30aab8ccac"}},"metadata":{}},{"name":"stdout","text":"тЬЕ Dataset loaded:\n   Train:      31,599 blocks\n   Validation: 645 blocks\n   Block size: 1024 tokens\n   Columns:    ['input_ids', 'attention_mask', 'labels']\n   Total train tokens: 32,357,376\nтЬЕ Block size verified\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 4. Load Tokenizer","metadata":{}},{"cell_type":"code","source":"print(f\"\\U0001f4e5 Loading tokenizer from {BASE_MODEL}...\")\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\ntokenizer.padding_side = \"right\"\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nprint(f\"\\u2705 Tokenizer ready: {len(tokenizer)} tokens\")\nprint(f\"   eos_token: {tokenizer.eos_token!r} (ID {tokenizer.eos_token_id})\")\nprint(f\"   pad_token: {tokenizer.pad_token!r} (ID {tokenizer.pad_token_id})\")\n\n# Quick sanity: decode a sample from the dataset\ndef count_tamil_chars(text):\n    return sum(1 for c in text if '\\u0B80' <= c <= '\\u0BFF')\n\ndef tamil_char_pct(text):\n    if not text:\n        return 0.0\n    return 100.0 * count_tamil_chars(text) / len(text)\n\nsample_text = tokenizer.decode(train_dataset[0][\"input_ids\"][:100])\nprint(f\"\\n\\U0001f50d Sample from dataset (first 100 tokens):\")\nprint(f\"   Tamil%: {tamil_char_pct(sample_text):.0f}%\")\nprint(f\"   Text:   {sample_text[:200]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T00:33:54.884613Z","iopub.execute_input":"2026-02-13T00:33:54.885128Z","iopub.status.idle":"2026-02-13T00:33:56.349509Z","shell.execute_reply.started":"2026-02-13T00:33:54.885088Z","shell.execute_reply":"2026-02-13T00:33:56.348715Z"}},"outputs":[{"name":"stdout","text":"ЁЯУе Loading tokenizer from Qwen/Qwen3-0.6B-Base...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92ee5608547d441483dad5fec0a4ac6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e870c2459f54f7489cfbbb27ca58636"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4680bec68d3849d4977320da5f90d096"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"606094d9293344188173a14e07cf6f0e"}},"metadata":{}},{"name":"stdout","text":"тЬЕ Tokenizer ready: 151669 tokens\n   eos_token: '<|endoftext|>' (ID 151643)\n   pad_token: '<|endoftext|>' (ID 151643)\n\nЁЯФН Sample from dataset (first 100 tokens):\n   Tamil%: 85%\n   Text:   я┐╜родро╛ро▓рпН родроирпНродрпИропрпИроХрпН роХро╛рогро╛род роХрпБро┤роирпНродрпИ роЕро┤ роЖро░роорпНрокро┐родрпНродродрпБ. рокроЪро┐ропрпБроорпН ро╡ро╛роЯрпНроЯро┐ропродрпБ. роХрпБро┤роирпНродрпИропро┐ройрпН роЕро┤рпБроХрпБро░ро▓рпН родро┐ро░...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 5. Load Model + QLoRA Setup\n\n**Using Base model** (GPT5.2 #1): DAPT from Base is cleaner.\nInstruction-following will be restored in SFT stage.","metadata":{}},{"cell_type":"code","source":"# No 4-bit quantization needed! Qwen3-0.6B in fp16 = ~1.2GB\n# Fits easily on T4 (15GB) or P100 (16GB)\n# 4-bit was causing bitsandbytes dequantization overhead that\n# bypassed Tensor Cores тАФ the root cause of 0.03 it/s speed\n\nprint(f\"\\U0001f4e5 Loading {BASE_MODEL} in fp16 (no quantization)...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n    trust_remote_code=True,\n)\n\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.eos_token_id = tokenizer.eos_token_id\nmodel.config.use_cache = False\n\n# Enable gradient checkpointing for memory safety\nmodel.gradient_checkpointing_enable()\n\nmem_gb = torch.cuda.memory_allocated() / 1024**3\nprint(f\"\\u2705 Model loaded in fp16: {model.num_parameters():,} params\")\nprint(f\"   GPU memory used: {mem_gb:.1f} GB\")\nprint(f\"   No 4-bit = full Tensor Core speed on T4\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T00:33:56.350440Z","iopub.execute_input":"2026-02-13T00:33:56.350717Z","iopub.status.idle":"2026-02-13T00:34:01.155347Z","shell.execute_reply.started":"2026-02-13T00:33:56.350692Z","shell.execute_reply":"2026-02-13T00:34:01.154464Z"}},"outputs":[{"name":"stdout","text":"ЁЯУе Loading Qwen/Qwen3-0.6B-Base in fp16 (no quantization)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"131c4168fce24d6e9cc0612fbd6d3a04"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66e58e12c3f747379d350edc117aa2ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"278a4baf13404db986e58ae7868d721f"}},"metadata":{}},{"name":"stdout","text":"тЬЕ Model loaded in fp16: 596,049,920 params\n   GPU memory used: 1.1 GB\n   No 4-bit = full Tensor Core speed on T4\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\nmem_gb = torch.cuda.memory_allocated() / 1024**3\nprint(f\"\\u2705 LoRA applied | GPU: {mem_gb:.1f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T00:34:01.156491Z","iopub.execute_input":"2026-02-13T00:34:01.156755Z","iopub.status.idle":"2026-02-13T00:34:07.711259Z","shell.execute_reply.started":"2026-02-13T00:34:01.156731Z","shell.execute_reply":"2026-02-13T00:34:07.710443Z"}},"outputs":[{"name":"stdout","text":"trainable params: 10,092,544 || all params: 606,142,464 || trainable%: 1.6650\nтЬЕ LoRA applied | GPU: 1.1 GB\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 6. Compute Training Steps\n\n**GPT5.2 #3:** Control by token budget / max_steps, not arbitrary epoch count.\nCap at MAX_EPOCHS to prevent catastrophic forgetting.","metadata":{}},{"cell_type":"code","source":"tokens_per_step = BATCH_SIZE * MAX_SEQ_LENGTH * GRADIENT_ACCUMULATION\nsteps_per_epoch = len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION)\n\n# Cap steps to fit compute budget\nmax_steps = min(steps_per_epoch, MAX_STEPS_CAP)\ntotal_tokens_trained = max_steps * tokens_per_step\n\n# Save/log intervals\nsave_steps = max(max_steps // 4, 50)\nlog_steps = max(max_steps // 40, 10)\neval_steps = max(max_steps // 8, 25)\n\nprint(f\"\\U0001f4ca Training Plan:\")\nprint(f\"   Dataset tokens:      {len(train_dataset) * MAX_SEQ_LENGTH:,}\")\nprint(f\"   Tokens/step:         {tokens_per_step:,}\")\nprint(f\"   Steps/epoch:         {steps_per_epoch:,}\")\nprint(f\"   Max steps (capped):  {max_steps:,}\")\nprint(f\"   Tokens to train on: {total_tokens_trained:,}\")\nprint(f\"   Coverage:            {100 * max_steps / steps_per_epoch:.0f}% of dataset\")\nprint(f\"   Save every:          {save_steps} steps\")\nprint(f\"   Log every:           {log_steps} steps\")\nprint(f\"   Eval every:          {eval_steps} steps\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T00:34:07.712333Z","iopub.execute_input":"2026-02-13T00:34:07.713253Z","iopub.status.idle":"2026-02-13T00:34:07.720422Z","shell.execute_reply.started":"2026-02-13T00:34:07.713193Z","shell.execute_reply":"2026-02-13T00:34:07.719423Z"}},"outputs":[{"name":"stdout","text":"ЁЯУК Training Plan:\n   Dataset tokens:      32,357,376\n   Tokens/step:         32,768\n   Steps/epoch:         987\n   Max steps (capped):  500\n   Tokens to train on: 16,384,000\n   Coverage:            51% of dataset\n   Save every:          125 steps\n   Log every:           12 steps\n   Eval every:          62 steps\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## 7. Train","metadata":{}},{"cell_type":"code","source":"# === LOSS LOGGING ===\nclass LossLoggingCallback(TrainerCallback):\n    def __init__(self):\n        self.losses = []\n        self.eval_losses = []\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs:\n            if \"loss\" in logs:\n                step = state.global_step\n                loss = logs[\"loss\"]\n                lr = logs.get(\"learning_rate\", 0)\n                self.losses.append((step, loss))\n                print(f\"  Step {step:4d}/{max_steps} | Loss: {loss:.4f} | LR: {lr:.2e}\")\n            if \"eval_loss\" in logs:\n                eval_loss = logs[\"eval_loss\"]\n                ppl = np.exp(min(eval_loss, 20))\n                self.eval_losses.append((state.global_step, eval_loss))\n                print(f\"  \\U0001f4ca Eval Loss: {eval_loss:.4f} | Perplexity: {ppl:.1f}\")\n\nloss_callback = LossLoggingCallback()\n\n# === DATA COLLATOR ===\n@dataclass\nclass PackedDataCollator:\n    \"\"\"Collator for pre-packed, pre-tokenized sequences.\"\"\"\n    def __call__(self, features):\n        return {\n            \"input_ids\": torch.tensor([f[\"input_ids\"] for f in features], dtype=torch.long),\n            \"attention_mask\": torch.tensor([f[\"attention_mask\"] for f in features], dtype=torch.long),\n            \"labels\": torch.tensor([f[\"labels\"] for f in features], dtype=torch.long),\n        }\n\n# === TRAINER ===\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/vazhi-dapt-v1_0\",\n    max_steps=max_steps,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n    learning_rate=LEARNING_RATE,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=WARMUP_RATIO,\n    logging_steps=log_steps,\n    save_steps=save_steps,\n    eval_steps=eval_steps,\n    eval_strategy=\"steps\",\n    save_total_limit=3,\n    fp16=True,                    # T4 Tensor Cores for real fp16 speedup\n    bf16=False,\n    gradient_checkpointing=True,  # Needed for memory safety\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    max_grad_norm=1.0,\n    optim=\"adamw_torch\",          # Standard AdamW (no bitsandbytes needed for fp16 model)\n    report_to=\"none\",\n    seed=RANDOM_SEED,\n    load_best_model_at_end=False,\n    dataloader_pin_memory=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=PackedDataCollator(),\n    callbacks=[loss_callback],\n)\n\nprint(\"\\u2705 Trainer ready\")\nprint(f\"   Steps: {max_steps} | LR: {LEARNING_RATE} | Effective BS: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\nprint(f\"   fp16: True | grad_ckpt: True | optimizer: AdamW (torch)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T00:34:07.722711Z","iopub.execute_input":"2026-02-13T00:34:07.722948Z","iopub.status.idle":"2026-02-13T00:34:07.791162Z","shell.execute_reply.started":"2026-02-13T00:34:07.722925Z","shell.execute_reply":"2026-02-13T00:34:07.790248Z"}},"outputs":[{"name":"stderr","text":"The model is already on multiple devices. Skipping the move to device specified in `args`.\n","output_type":"stream"},{"name":"stdout","text":"тЬЕ Trainer ready\n   Steps: 500 | LR: 2e-05 | Effective BS: 32\n   fp16: True | grad_ckpt: True | optimizer: AdamW (torch)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(\"\\U0001f680 Starting DAPT training...\")\nprint(f\"   {max_steps} steps, fp16=True, no gradient checkpointing\")\nprint(f\"   Tokens: ~{max_steps * BATCH_SIZE * GRADIENT_ACCUMULATION * MAX_SEQ_LENGTH / 1e6:.0f}M\")\nprint()\n\ntrain_result = trainer.train()\n\nprint(\"\\n\\u2705 Training complete!\")\nmetrics = train_result.metrics\nfor k, v in metrics.items():\n    print(f\"   {k}: {v}\")\n\n# Final eval\nprint(\"\\n\\U0001f4ca Final eval on held-out blocks...\")\neval_metrics = trainer.evaluate()\neval_loss = eval_metrics.get(\"eval_loss\", float(\"inf\"))\neval_ppl = np.exp(min(eval_loss, 20))\nprint(f\"   Eval Loss:       {eval_loss:.4f}\")\nprint(f\"   Eval Perplexity: {eval_ppl:.1f}\")\n\n# Loss summary\nif loss_callback.losses:\n    start_loss = loss_callback.losses[0][1]\n    end_loss = loss_callback.losses[-1][1]\n    print(f\"\\n\\U0001f4c8 Loss: {start_loss:.4f} \\u2192 {end_loss:.4f} ({100*(start_loss - end_loss)/start_loss:.1f}% drop)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T00:34:07.792362Z","iopub.execute_input":"2026-02-13T00:34:07.792755Z","iopub.status.idle":"2026-02-13T04:12:05.001009Z","shell.execute_reply.started":"2026-02-13T00:34:07.792713Z","shell.execute_reply":"2026-02-13T04:12:04.999697Z"}},"outputs":[{"name":"stdout","text":"ЁЯЪА Starting DAPT training...\n   500 steps, fp16=True, no gradient checkpointing\n   Tokens: ~16M\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='378' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [378/500 3:37:15 < 1:10:29, 0.03 it/s, Epoch 0.38/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>62</td>\n      <td>1.059600</td>\n      <td>1.044861</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>1.044200</td>\n      <td>1.033765</td>\n    </tr>\n    <tr>\n      <td>186</td>\n      <td>1.042800</td>\n      <td>1.025660</td>\n    </tr>\n    <tr>\n      <td>248</td>\n      <td>1.042400</td>\n      <td>1.019685</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.027300</td>\n      <td>1.015504</td>\n    </tr>\n    <tr>\n      <td>372</td>\n      <td>1.027300</td>\n      <td>1.013053</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"  Step   12/500 | Loss: 1.0799 | LR: 8.80e-06\n  Step   24/500 | Loss: 1.0665 | LR: 1.84e-05\n  Step   36/500 | Loss: 1.0844 | LR: 2.00e-05\n  Step   48/500 | Loss: 1.0698 | LR: 1.99e-05\n  Step   60/500 | Loss: 1.0596 | LR: 1.97e-05\n  ЁЯУК Eval Loss: 1.0449 | Perplexity: 2.8\n  Step   72/500 | Loss: 1.0514 | LR: 1.95e-05\n  Step   84/500 | Loss: 1.0427 | LR: 1.93e-05\n  Step   96/500 | Loss: 1.0523 | LR: 1.89e-05\n  Step  108/500 | Loss: 1.0446 | LR: 1.86e-05\n  Step  120/500 | Loss: 1.0442 | LR: 1.81e-05\n  ЁЯУК Eval Loss: 1.0338 | Perplexity: 2.8\n  Step  132/500 | Loss: 1.0571 | LR: 1.76e-05\n  Step  144/500 | Loss: 1.0553 | LR: 1.71e-05\n  Step  156/500 | Loss: 1.0660 | LR: 1.65e-05\n  Step  168/500 | Loss: 1.0549 | LR: 1.59e-05\n  Step  180/500 | Loss: 1.0428 | LR: 1.52e-05\n  ЁЯУК Eval Loss: 1.0257 | Perplexity: 2.8\n  Step  192/500 | Loss: 1.0567 | LR: 1.46e-05\n  Step  204/500 | Loss: 1.0326 | LR: 1.38e-05\n  Step  216/500 | Loss: 1.0640 | LR: 1.31e-05\n  Step  228/500 | Loss: 1.0412 | LR: 1.23e-05\n  Step  240/500 | Loss: 1.0424 | LR: 1.15e-05\n  ЁЯУК Eval Loss: 1.0197 | Perplexity: 2.8\n  Step  252/500 | Loss: 1.0519 | LR: 1.08e-05\n  Step  264/500 | Loss: 1.0490 | LR: 9.97e-06\n  Step  276/500 | Loss: 1.0371 | LR: 9.17e-06\n  Step  288/500 | Loss: 1.0299 | LR: 8.39e-06\n  Step  300/500 | Loss: 1.0273 | LR: 7.61e-06\n  ЁЯУК Eval Loss: 1.0155 | Perplexity: 2.8\n  Step  312/500 | Loss: 1.0271 | LR: 6.85e-06\n  Step  324/500 | Loss: 1.0224 | LR: 6.10e-06\n  Step  336/500 | Loss: 1.0169 | LR: 5.39e-06\n  Step  348/500 | Loss: 1.0245 | LR: 4.70e-06\n  Step  360/500 | Loss: 1.0417 | LR: 4.04e-06\n  Step  372/500 | Loss: 1.0273 | LR: 3.42e-06\n  ЁЯУК Eval Loss: 1.0131 | Perplexity: 2.8\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_113/3429021602.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\u2705 Training complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2846\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2847\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2848\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2849\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"markdown","source":"## 8. Save & Upload LoRA Adapter","metadata":{}},{"cell_type":"code","source":"ADAPTER_PATH = \"/kaggle/working/vazhi-dapt-v1_0-lora\"\n\nprint(\"\\U0001f4be Saving LoRA adapter...\")\ntrainer.save_model(ADAPTER_PATH)\ntokenizer.save_pretrained(ADAPTER_PATH)\n\nadapter_files = glob.glob(f\"{ADAPTER_PATH}/*\")\nprint(f\"   Files: {[os.path.basename(f) for f in adapter_files]}\")\nassert any('adapter' in f for f in adapter_files), \"No adapter files!\"\nprint(\"\\u2705 Adapter saved\")\n\n# Upload adapter backup (GPT5.2 #9)\napi = HfApi()\napi.create_repo(ADAPTER_REPO, exist_ok=True)\nprint(f\"\\U0001f4e4 Uploading adapter to {ADAPTER_REPO}...\")\napi.upload_folder(\n    folder_path=ADAPTER_PATH,\n    repo_id=ADAPTER_REPO,\n    commit_message=f\"DAPT v1.0 adapter: Sangraha Tamil, r={LORA_R}, lr={LEARNING_RATE}\",\n)\nprint(f\"\\u2705 Adapter uploaded: https://huggingface.co/{ADAPTER_REPO}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T04:12:38.269178Z","iopub.execute_input":"2026-02-13T04:12:38.269525Z","iopub.status.idle":"2026-02-13T04:12:42.554288Z","shell.execute_reply.started":"2026-02-13T04:12:38.269498Z","shell.execute_reply":"2026-02-13T04:12:42.553474Z"}},"outputs":[{"name":"stdout","text":"ЁЯТ╛ Saving LoRA adapter...\n   Files: ['tokenizer_config.json', 'merges.txt', 'chat_template.jinja', 'tokenizer.json', 'adapter_config.json', 'vocab.json', 'adapter_model.safetensors', 'added_tokens.json', 'README.md', 'training_args.bin', 'special_tokens_map.json']\nтЬЕ Adapter saved\nЁЯУд Uploading adapter to CryptoYogi/qwen3-0.6b-tamil-lora...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65764591a0864e82a181a95bb5cc79d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1c4f91c524649a4a714e3917cf8285d"}},"metadata":{}},{"name":"stdout","text":"тЬЕ Adapter uploaded: https://huggingface.co/CryptoYogi/qwen3-0.6b-tamil-lora\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# No need to free and reload тАФ model is already in fp16!\n# (We removed 4-bit quantization, so merge can happen directly)\nprint(\"\\u2705 Model already in fp16 тАФ no reload needed for merge\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T04:12:59.523631Z","iopub.execute_input":"2026-02-13T04:12:59.524548Z","iopub.status.idle":"2026-02-13T04:12:59.528872Z","shell.execute_reply.started":"2026-02-13T04:12:59.524512Z","shell.execute_reply":"2026-02-13T04:12:59.528096Z"}},"outputs":[{"name":"stdout","text":"тЬЕ Model already in fp16 тАФ no reload needed for merge\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 9. Merge LoRA in FP16\n\n**Hard rule (Lesson #39):** NEVER merge into 4-bit. Reload base in fp16.","metadata":{}},{"cell_type":"code","source":"# Model is already fp16 тАФ just merge the LoRA adapter directly\nprint(f\"\\U0001f517 Loading LoRA adapter for merge...\")\nbase_model_fp16 = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n    trust_remote_code=True,\n)\n\npeft_model = PeftModel.from_pretrained(base_model_fp16, ADAPTER_PATH)\npeft_model.gradient_checkpointing_disable()\npeft_model.config.use_cache = True\npeft_model.eval()\n\nprint(\"\\U0001f500 Merging LoRA in fp16...\")\nmerged_model = peft_model.merge_and_unload()\nprint(f\"\\u2705 Merged: {merged_model.num_parameters():,} params\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T04:13:07.897264Z","iopub.execute_input":"2026-02-13T04:13:07.898009Z","iopub.status.idle":"2026-02-13T04:13:09.290547Z","shell.execute_reply.started":"2026-02-13T04:13:07.897979Z","shell.execute_reply":"2026-02-13T04:13:09.289821Z"}},"outputs":[{"name":"stdout","text":"ЁЯФЧ Loading LoRA adapter for merge...\nЁЯФА Merging LoRA in fp16...\nтЬЕ Merged: 596,049,920 params\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 10. DAPT Evaluation\n\n**GPT5.2 #7:** Proper eval, not just a quick test.\n\nThis is a Base model after DAPT тАФ it won't follow instructions.\nIt should generate coherent Tamil text continuations.","metadata":{}},{"cell_type":"code","source":"merged_model.eval()\nmerged_model.config.use_cache = True\n\neval_prompts = [\n    (\"prose\", \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\\u0ba8\\u0bbe\\u0b9f\\u0bc1 \\u0b87\\u0ba8\\u0bcd\\u0ba4\\u0bbf\\u0baf\\u0bbe\\u0bb5\\u0bbf\\u0ba9\\u0bcd \\u0ba4\\u0bc6\\u0ba9\\u0bcd \\u0baa\\u0b95\\u0bc1\\u0ba4\\u0bbf\\u0baf\\u0bbf\\u0bb2\\u0bcd \\u0b85\\u0bae\\u0bc8\\u0ba8\\u0bcd\\u0ba4\\u0bc1\\u0bb3\\u0bcd\\u0bb3 \\u0b92\\u0bb0\\u0bc1 \\u0bae\\u0bbe\\u0ba8\\u0bbf\\u0bb2\\u0bae\\u0bcd.\"),\n    (\"prose\", \"\\u0baa\\u0bca\\u0b99\\u0bcd\\u0b95\\u0bb2\\u0bcd \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bb0\\u0bcd\\u0b95\\u0bb3\\u0bbf\\u0ba9\\u0bcd \\u0bae\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bbf\\u0baf \\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0ba8\\u0bbe\\u0bb3\\u0bcd.\"),\n    (\"literature\", \"\\u0bb5\\u0bb3\\u0bcd\\u0bb3\\u0bc1\\u0bb5\\u0bb0\\u0bcd \\u0b95\\u0bc2\\u0bb1\\u0bbf\\u0baf \\u0b85\\u0bb1\\u0bae\\u0bcd, \\u0baa\\u0bca\\u0bb0\\u0bc1\\u0bb3\\u0bcd, \\u0b87\\u0ba9\\u0bcd\\u0baa\\u0bae\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0bb1 \\u0bae\\u0bc2\\u0ba9\\u0bcd\\u0bb1\\u0bc1\"),\n    (\"knowledge\", \"\\u0b9a\\u0bbf\\u0ba4\\u0bcd\\u0ba4 \\u0bae\\u0bb0\\u0bc1\\u0ba4\\u0bcd\\u0ba4\\u0bc1\\u0bb5\\u0bae\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0baa\\u0ba4\\u0bc1 \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd \\u0bae\\u0b95\\u0bcd\\u0b95\\u0bb3\\u0bbf\\u0ba9\\u0bcd \\u0baa\\u0bbe\\u0bb0\\u0bae\\u0bcd\\u0baa\\u0bb0\\u0bbf\\u0baf\"),\n    (\"daily\", \"\\u0b95\\u0bbe\\u0bb2\\u0bc8\\u0baf\\u0bbf\\u0bb2\\u0bcd \\u0b8e\\u0bb4\\u0bc1\\u0ba8\\u0bcd\\u0ba4\\u0ba4\\u0bc1\\u0bae\\u0bcd \\u0bae\\u0bc1\\u0ba4\\u0bb2\\u0bbf\\u0bb2\\u0bcd\"),\n    (\"short\", \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\"),\n    (\"short\", \"\\u0ba8\\u0ba9\\u0bcd\\u0bb1\\u0bbf\"),\n    (\"mixed\", \"India has many languages. \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd is one of the\"),\n]\n\nprint(f\"\\n{'='*60}\")\nprint(f\"\\U0001f9ea DAPT EVAL: {len(eval_prompts)} Tamil text continuations\")\nprint(f\"   (Base model \\u2014 expect text continuation, not chat)\")\nprint(f\"{'='*60}\")\n\neval_results = []\n\nfor category, prompt_text in eval_prompts:\n    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(merged_model.device)\n\n    with torch.no_grad():\n        outputs = merged_model.generate(\n            **inputs,\n            max_new_tokens=150,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            repetition_penalty=1.2,\n            no_repeat_ngram_size=4,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n\n    generated_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n\n    t_pct = tamil_char_pct(response)\n    words = response.split()\n    unique_ratio = len(set(words)) / max(len(words), 1)\n    is_repetitive = unique_ratio < 0.3 and len(words) > 10\n    is_empty = len(response.strip()) < 10\n    is_code = any(kw in response[:100] for kw in ['def ', 'class ', 'import ', '{\"', 'var '])\n\n    status = \"\\u2705\"\n    if is_empty: status = \"\\u274c EMPTY\"\n    elif is_code: status = \"\\u274c CODE\"\n    elif is_repetitive: status = \"\\u26a0\\ufe0f LOOP\"\n    elif t_pct < 20 and category != \"mixed\": status = \"\\u26a0\\ufe0f LOW TAMIL\"\n\n    eval_results.append((category, prompt_text, response[:200], status, t_pct, unique_ratio))\n\n    print(f\"\\n[{category.upper()}] {status} (Tamil: {t_pct:.0f}%, Unique: {unique_ratio:.0%})\")\n    print(f\"  Prompt: {prompt_text[:60]}\")\n    print(f\"  Output: {response[:300]}\")\n    print(\"-\" * 50)\n\n# Summary\nprint(f\"\\n{'='*60}\")\nprint(f\"\\U0001f4ca DAPT EVAL SUMMARY\")\nprint(f\"{'='*60}\")\npass_count = sum(1 for r in eval_results if r[3] == \"\\u2705\")\navg_tamil = np.mean([r[4] for r in eval_results])\navg_unique = np.mean([r[5] for r in eval_results])\nprint(f\"   Passed:      {pass_count}/{len(eval_results)}\")\nprint(f\"   Avg Tamil%:  {avg_tamil:.0f}%\")\nprint(f\"   Avg Unique:  {avg_unique:.0%}\")\nprint(f\"   Eval PPL:    {eval_ppl:.1f}\")\n\nfor cat, prompt, resp, status, tamil, uniq in eval_results:\n    print(f\"   {status} [{cat}] Tamil:{tamil:.0f}% Uniq:{uniq:.0%}\")\n\nif pass_count >= len(eval_results) * 0.7 and avg_tamil > 30:\n    print(f\"\\n\\U0001f389 DAPT successful! Proceed to SFT.\")\nelif pass_count >= len(eval_results) * 0.4:\n    print(f\"\\n\\u26a0\\ufe0f  Partial. Try more tokens or check loss curve.\")\nelse:\n    print(f\"\\n\\u274c DAPT failed. Check loss curve, data quality, try r=32.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T04:13:19.594884Z","iopub.execute_input":"2026-02-13T04:13:19.595237Z","iopub.status.idle":"2026-02-13T04:14:16.943640Z","shell.execute_reply.started":"2026-02-13T04:13:19.595203Z","shell.execute_reply":"2026-02-13T04:14:16.942625Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nЁЯзк DAPT EVAL: 8 Tamil text continuations\n   (Base model тАФ expect text continuation, not chat)\n============================================================\n\n[PROSE] тЬЕ (Tamil: 68%, Unique: 100%)\n  Prompt: родрооро┐ро┤рпНроиро╛роЯрпБ роЗроирпНродро┐ропро╛ро╡ро┐ройрпН родрпЖройрпН рокроХрпБродро┐ропро┐ро▓рпН роЕроорпИроирпНродрпБро│рпНро│ роТро░рпБ рооро╛роиро┐ро▓роорпН.\n  Output:  1952-ро▓рпН, роЪрокро┐роЯро┐ роХрогроХрпНро╖роЩрпНроХроЪро┐ро░ро╛роо ро╡ро┤ро┐роХро╛роЯрпНроЪро┐ (Census of India) роОройрпНро▒ роЙро▒рпБрокрпНрокро┐ройро░рпН роЬрпЛро╕рпНро╕рпВроЯро╛ропрпН(Jossuatai), роЕроЩрпНроЧрпНро░рпАроЪрпН рокрпКро░рпБро│ро╛родро╛ро░ роУро░рпБроЯрпНроЯрпБ рокроХрпНрогроорпН роПро▒рпНрокроЯрпБродрпНродро┐рой.\nроЗроирпНроиро┐ро▓рпИропро┐ро▓\n--------------------------------------------------\n\n[PROSE] тЬЕ (Tamil: 69%, Unique: 94%)\n  Prompt: рокрпКроЩрпНроХро▓рпН родрооро┐ро┤ро░рпНроХро│ро┐ройрпН роорпБроХрпНроХро┐роп родро┐ро░рпБроиро╛ро│рпН.\n  Output:  . .\nроЗройро┐, роЗродро▒рпНрокроЯро┐роЪрпНроЪрпЖропрпНро╡рпЛроорпН!\nроЕро╡роЪро░ роТро┤рпБроЩрпНроЯрпБ - роЕройрпНро▒рпБ роОро╕рпНроПроОро╕рпН (роОро╕рпНроГ3) ро╕рпНро▓ро┐роВроГ\nроорогро┐роорпАрод ро╣рпИ-роЖроГрокрпН! роЪрпЖройрпНройрпИроГ 10:45 AM\n--------------------------------------------------\n\n[LITERATURE] тЬЕ (Tamil: 76%, Unique: 88%)\n  Prompt: ро╡ро│рпНро│рпБро╡ро░рпН роХрпВро▒ро┐роп роЕро▒роорпН, рокрпКро░рпБро│рпН, роЗройрпНрокроорпН роОройрпНро▒ роорпВройрпНро▒рпБ\n  Output:  ро╡ро╛роХрпНроХродрпНродро┐ройрпНроЬро▓рпИропроЯро┐. \"роЕрокрпНро░роЪро┐роирпНродрпЛро┤роЩрпНроХро│рпН\" (роОрогрпНрогрпЖропро┐ро▓рпН) - 'роЗройрпНро╕рпНроЯро┐роЪрпНроЯро┐роПро╕рпН' : 40mg\n- роТройрпНро▒ро╛роХ роЙроЯройроЯро┐ропро╛роХ, роЗродрпБ роиро▓рпНро▓родрпБ.\n- роРройрпНро╣ро╛ро░ро┐: роУроЯрпНроЯрпБроХрпНроХро╛рой ро╡роЮрпНроЪрокрпН рокро░ро┐роЪрпАро▓ро┐роХро│\n--------------------------------------------------\n\n[KNOWLEDGE] тЬЕ (Tamil: 62%, Unique: 100%)\n  Prompt: роЪро┐родрпНрод рооро░рпБродрпНродрпБро╡роорпН роОройрпНрокродрпБ родрооро┐ро┤рпН роороХрпНроХро│ро┐ройрпН рокро╛ро░роорпНрокро░ро┐роп\n  Output: рокрпН рокрпЛро▓рпАро╕рпН, роЕро▒ро┐ро╡роЯрпИропро╛родро┐ роЖрогрпНроЯрпБ 1970-роЗро▓рпН роиро┐ро▒рпИроп роЙродро╡ро┐роЪрпН роЪрпКро▓рпНро▓рпБроорпН. роЗроЩрпНроЧрпНро░ро┐ро╖рпНроЯ рокро┤рпИропроорпВро▓роорпН \"The Sinhala Medical Service\" (S.M.S.) роОрой ро╕рпНроеро┐родро┐ропро┐ро▓рпН роТро░рпБ роиро┐роЬроорпН роПро▒рпНрокроЯрпНроЯродрпБ.\nроЖроЯро▓ рооро╣ро╛роЬройрпН ро╣\n--------------------------------------------------\n\n[DAILY] тЬЕ (Tamil: 77%, Unique: 100%)\n  Prompt: роХро╛ро▓рпИропро┐ро▓рпН роОро┤рпБроирпНродродрпБроорпН роорпБродро▓ро┐ро▓рпН\n  Output: , роХроЯройрпН роЕро│ро╡рпБроХрпНроХрпБ рокро░рокро░рокрпНрокрпБ. 50-72% роЪрпЖройрпНройрпИ (роЖрогрпНроЯрпБ) ро╡роЪродро┐, роЙро▒рпНро▒рпБро░рпИропро╛роХ роТро░рпБ ро╕рпНро░рпАройро╛рооро╕рпНропрпЛроЯрпН роЗроЯроорпН роПро▒рпНрокроЯрпНроЯродрпБ.\nроЗродройро╛ро▓рпН, ро╡ро┐ро│роХрпНроХроорпН, роиро╛ройрпН роЪрпКро▓рпНро▓рокрпНрокроЯрпБроХро┐ро▒родрпБ. роЗродя┐╜\n--------------------------------------------------\n\n[SHORT] тЬЕ (Tamil: 75%, Unique: 100%)\n  Prompt: родрооро┐ро┤рпН\n  Output:  роХро▓рпНро╡ро┐ рокропройро░рпБроХро│рпИ, 'роиро╛роЪроЩрпНроХрпВроЯроорпН'ро╡ро╛ро▒рпБ роЕро┤рпИрокрпНрокродро╛роХ родрпЖро░ро┐ропро╡роирпНродрпБро│рпНро│родрпБ. 2015-роЖроорпН роЖрогрпНроЯро┐ро▓рпН, \"роЗроЮрпНроЪро┐\" (inches) роОройрпНро▒ роорпКро┤ро┐ропрпИ роЗро╖рпНро░ро╕рпНро╕ро╛ройрпН роТро░рпБ роЙро▓роХродрпН родро┐ройроорпН роПро▒рпНрокроЯрпБрод\n--------------------------------------------------\n\n[SHORT] тЬЕ (Tamil: 65%, Unique: 95%)\n  Prompt: роиройрпНро▒ро┐\n  Output:  роЗро╡ро░рпИропрпБроорпН родро┐ро░рпБрокрпНрокродро▒рпНроХрпБро│рпНро│ро╛роХ роЕроЯро┐ро╡ро┤ро┐ роЪрпЖро▓рпНро▓ роЙрогрпНроЯрпБ.\nроЗ. 210, II-497\nроЕроЪродрпНропроорпН - роороЩрпНроХрпВро░рпНро╕рпАро╕рпБроорпН; (ро╖) роПроГрокрпН ;(роЖ)роОродрпБ ;\nроЕрокрпНроЬрпБроорпН - 'рокроЬ' , роОройрпНрой?\n--------------------------------------------------\n\n[MIXED] тЬЕ (Tamil: 36%, Unique: 100%)\n  Prompt: India has many languages. родрооро┐ро┤рпН is one of the\n  Output:  most widely spoken in India and a very rich language with over 250 words, but there are also more than four hundred other varieties.\n\nродрпКроЯро░рпНрокро╛ройродрпБ ро╡роХрпИропроЩрпНроХро│рпБроХрпНроХрпБрокрпН рокро▒рпНро▒ро┐ роЗро╡рпЖро▓рпНро▓ро╛роорпН роОро┤рпБродро┐ропрпБро│рпНро│ роиро╛роороЪрпАро░рпН - роЕро╡ро░рпНроорпЛроЬрпН (Narayanacharya) роЪро┐ройрпНройроорпН роТро░рпБ роПро▒рпНрокроЮрпНроЪрогрпНроЯрпБ 'рокро┐ро░роХрпВро░рпН\n--------------------------------------------------\n\n============================================================\nЁЯУК DAPT EVAL SUMMARY\n============================================================\n   Passed:      8/8\n   Avg Tamil%:  66%\n   Avg Unique:  97%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_113/2487636791.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Avg Tamil%:  {avg_tamil:.0f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Avg Unique:  {avg_unique:.0%}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Eval PPL:    {eval_ppl:.1f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtamil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'eval_ppl' is not defined"],"ename":"NameError","evalue":"name 'eval_ppl' is not defined","output_type":"error"}],"execution_count":14},{"cell_type":"markdown","source":"## 11. Upload Merged Model","metadata":{}},{"cell_type":"code","source":"api = HfApi()\napi.create_repo(OUTPUT_MODEL, exist_ok=True)\n\nprint(f\"\\U0001f4e4 Pushing merged fp16 model to {OUTPUT_MODEL}...\")\nmerged_model.push_to_hub(\n    OUTPUT_MODEL,\n    private=False,\n    commit_message=f\"DAPT v1.0: Tamil-adapted Qwen3-0.6B (Sangraha, QLoRA r={LORA_R})\",\n)\ntokenizer.push_to_hub(OUTPUT_MODEL)\n\nprint(f\"\\n\\u2705 Model: https://huggingface.co/{OUTPUT_MODEL}\")\nprint(f\"\\u2705 Adapter: https://huggingface.co/{ADAPTER_REPO}\")\nprint(f\"\\n\\U0001f449 Next: Run SFT notebook with BASE_MODEL = \\\"{OUTPUT_MODEL}\\\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T04:14:36.020944Z","iopub.execute_input":"2026-02-13T04:14:36.021598Z","iopub.status.idle":"2026-02-13T04:14:57.076232Z","shell.execute_reply.started":"2026-02-13T04:14:36.021569Z","shell.execute_reply":"2026-02-13T04:14:57.075256Z"}},"outputs":[{"name":"stdout","text":"ЁЯУд Pushing merged fp16 model to CryptoYogi/qwen3-0.6b-tamil...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fab73958358b4e83aea8b11955175f4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fb27464d959484093dd862e241a0f20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"503866b4cfcf42938ebe222a6c7a17e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0912fd1c0964574a989782eecab2399"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59bb27c68d2c454cbf28160ab126f16c"}},"metadata":{}},{"name":"stdout","text":"\nтЬЕ Model: https://huggingface.co/CryptoYogi/qwen3-0.6b-tamil\nтЬЕ Adapter: https://huggingface.co/CryptoYogi/qwen3-0.6b-tamil-lora\n\nЁЯСЙ Next: Run SFT notebook with BASE_MODEL = \"CryptoYogi/qwen3-0.6b-tamil\"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Summary\n\n| Artifact | Repo | Purpose |\n|----------|------|---------|\n| Packed DAPT data | `CryptoYogi/vazhi-dapt-tamil-v1_0` | Reusable training data |\n| Merged fp16 model | `CryptoYogi/qwen3-0.6b-tamil` | Reusable Tamil base for SFT |\n| LoRA adapter | `CryptoYogi/qwen3-0.6b-tamil-lora` | Recovery backup |\n\n### Next: SFT (Stage 3)\n```python\nBASE_MODEL = \"CryptoYogi/qwen3-0.6b-tamil\"  # THIS model\nDATASET = \"CryptoYogi/vazhi-tamil-sft-v4_0\"  # or combined v3.6 + v4.0\n```\n\n### If DAPT failed\n1. Loss didn't decrease тЖТ data may be too noisy, check filters\n2. Tamil% low тЖТ increase TARGET_TOKENS in data prep, re-run\n3. Repetitive output тЖТ try r=32 in this notebook (just change LORA_R)\n4. All else fails тЖТ try Instruct model with very low LR (1e-5)","metadata":{}}]}
