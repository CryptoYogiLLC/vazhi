{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI DAPT v1.0 — Tamil Language Adaptation Training\n",
    "\n",
    "**Pipeline Step 2 of 3:** Train DAPT on pre-built packed Tamil data.\n",
    "\n",
    "```\n",
    "Step 1: Data Prep (DONE — Vazhi_DAPT_Data_v1_0.ipynb)\n",
    "  → Produced: CryptoYogi/vazhi-dapt-tamil-v1_0 (packed 1024-token blocks)\n",
    "\n",
    "Step 2 (THIS NOTEBOOK): DAPT Training — Kaggle P100 GPU\n",
    "  → Input:  Packed dataset from HF + Qwen3-0.6B-Base\n",
    "  → Output: CryptoYogi/qwen3-0.6b-tamil (reusable Tamil base)\n",
    "           CryptoYogi/qwen3-0.6b-tamil-lora (adapter backup)\n",
    "\n",
    "Step 3: SFT (NEXT — Vazhi_SFT_v3_9_OnDAPT.ipynb)\n",
    "  → Input:  DAPT'd model + ChatML instruction pairs\n",
    "  → Output: CryptoYogi/vazhi-qwen3-v3_9\n",
    "```\n",
    "\n",
    "**Key design (incorporating GPT5.2 review):**\n",
    "1. Base model, not Instruct (cleaner DAPT)\n",
    "2. Token-budgeted training (max_steps from token count)\n",
    "3. Data already packed into 1024-token blocks (no padding waste)\n",
    "4. QLoRA r=16 (conservative for 0.6B)\n",
    "5. Eval: perplexity on held-out blocks + Tamil generation quality\n",
    "6. Adapter + merged model saved separately for recovery\n",
    "\n",
    "**Target:** Kaggle P100 (16GB) | Est. 2-4 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "**After running this cell, RESTART the session** (Runtime → Restart session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q -U \\\n  \"transformers>=4.45.0,<5.0.0\" \\\n  \"accelerate>=0.34.2\" \\\n  \"peft>=0.12.0\" \\\n  \"bitsandbytes>=0.43.3\" \\\n  \"datasets>=2.21.0\" \\\n  \"huggingface_hub>=0.24.7\"\n\nprint(\"\\u2705 Dependencies installed\")\nprint(\"\\u26a0\\ufe0f  RESTART THE SESSION NOW (Runtime \\u2192 Restart session)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Force single GPU BEFORE importing torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport json\nimport random\nimport glob\nimport gc\nimport torch\nimport numpy as np\nfrom dataclasses import dataclass\nfrom datasets import load_dataset\nfrom huggingface_hub import login, HfApi\n\nfrom transformers import (\n    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n    TrainerCallback, Trainer, TrainingArguments,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\n# === KEY CONFIG ===\nBASE_MODEL = \"Qwen/Qwen3-0.6B-Base\"  # Base model for DAPT (GPT5.2 #1)\nDATASET_NAME = \"CryptoYogi/vazhi-dapt-tamil-v1_0\"  # Pre-built by Data Prep notebook\nOUTPUT_MODEL = \"CryptoYogi/qwen3-0.6b-tamil\"  # Reusable Tamil base\nADAPTER_REPO = \"CryptoYogi/qwen3-0.6b-tamil-lora\"  # Adapter backup (GPT5.2 #9)\n\n# Training config\nMAX_SEQ_LENGTH = 1024        # Must match data prep notebook\nLEARNING_RATE = 2e-5         # Low LR for gentle adaptation\nLORA_R = 16                  # Conservative rank (GPT5.2 #4)\nLORA_ALPHA = 32\nBATCH_SIZE = 4               # batch 8 OOMs on T4 (Qwen3's 151K vocab = huge logits tensor)\nGRADIENT_ACCUMULATION = 8    # Effective batch = 32\nWARMUP_RATIO = 0.05\nMAX_STEPS_CAP = 500          # Cap to fit compute budget (~16M tokens)\n\nprint(f\"\\u2705 Configuration loaded\")\nprint(f\"   PyTorch: {torch.__version__}\")\nprint(f\"   CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"   GPU: {gpu_name} ({gpu_mem:.0f} GB)\")\n    print(f\"   fp16: ENABLED\")\nprint()\nprint(f\"\\U0001f4cb DAPT Training v1.0:\")\nprint(f\"   Base model:  {BASE_MODEL}\")\nprint(f\"   Dataset:     {DATASET_NAME}\")\nprint(f\"   Output:      {OUTPUT_MODEL}\")\nprint(f\"   LR:          {LEARNING_RATE}\")\nprint(f\"   LoRA:        r={LORA_R}, alpha={LORA_ALPHA}\")\nprint(f\"   Batch:       {BATCH_SIZE} x {GRADIENT_ACCUMULATION} = {BATCH_SIZE * GRADIENT_ACCUMULATION} effective\")\nprint(f\"   Max steps:   {MAX_STEPS_CAP} (~{MAX_STEPS_CAP * BATCH_SIZE * GRADIENT_ACCUMULATION * MAX_SEQ_LENGTH / 1e6:.0f}M tokens)\")\nprint(f\"   fp16:        True\")\nprint(f\"   Grad ckpt:   True (needed for T4 15GB)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "print(\"\\u2705 Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pre-Built Dataset\n",
    "\n",
    "Dataset was created by `Vazhi_DAPT_Data_v1_0.ipynb`:\n",
    "- Sangraha verified Tamil, filtered (Tamil >= 40%, dedup, no repetition)\n",
    "- Packed into 1024-token blocks\n",
    "- Already split into train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\U0001f4e5 Loading pre-built dataset from {DATASET_NAME}...\")\n",
    "ds = load_dataset(DATASET_NAME)\n",
    "\n",
    "train_dataset = ds[\"train\"]\n",
    "eval_dataset = ds[\"validation\"]\n",
    "\n",
    "print(f\"\\u2705 Dataset loaded:\")\n",
    "print(f\"   Train:      {len(train_dataset):,} blocks\")\n",
    "print(f\"   Validation: {len(eval_dataset):,} blocks\")\n",
    "print(f\"   Block size: {len(train_dataset[0]['input_ids'])} tokens\")\n",
    "print(f\"   Columns:    {train_dataset.column_names}\")\n",
    "\n",
    "total_train_tokens = len(train_dataset) * MAX_SEQ_LENGTH\n",
    "print(f\"   Total train tokens: {total_train_tokens:,}\")\n",
    "\n",
    "# Verify block size matches our config\n",
    "assert len(train_dataset[0][\"input_ids\"]) == MAX_SEQ_LENGTH, \\\n",
    "    f\"Block size mismatch: dataset has {len(train_dataset[0]['input_ids'])}, config has {MAX_SEQ_LENGTH}\"\n",
    "print(\"\\u2705 Block size verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\U0001f4e5 Loading tokenizer from {BASE_MODEL}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"\\u2705 Tokenizer ready: {len(tokenizer)} tokens\")\n",
    "print(f\"   eos_token: {tokenizer.eos_token!r} (ID {tokenizer.eos_token_id})\")\n",
    "print(f\"   pad_token: {tokenizer.pad_token!r} (ID {tokenizer.pad_token_id})\")\n",
    "\n",
    "# Quick sanity: decode a sample from the dataset\n",
    "def count_tamil_chars(text):\n",
    "    return sum(1 for c in text if '\\u0B80' <= c <= '\\u0BFF')\n",
    "\n",
    "def tamil_char_pct(text):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    return 100.0 * count_tamil_chars(text) / len(text)\n",
    "\n",
    "sample_text = tokenizer.decode(train_dataset[0][\"input_ids\"][:100])\n",
    "print(f\"\\n\\U0001f50d Sample from dataset (first 100 tokens):\")\n",
    "print(f\"   Tamil%: {tamil_char_pct(sample_text):.0f}%\")\n",
    "print(f\"   Text:   {sample_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model + QLoRA Setup\n",
    "\n",
    "**Using Base model** (GPT5.2 #1): DAPT from Base is cleaner.\n",
    "Instruction-following will be restored in SFT stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"\\U0001f4e5 Loading {BASE_MODEL} in 4-bit (for training memory)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\":0},\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(f\"\\u2705 Model loaded: {model.num_parameters():,} params\")\n",
    "print(f\"   4-bit is for training memory ONLY — will merge LoRA in fp16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Convert bf16 to fp16 for P100 compatibility\n",
    "bf16_count = sum(1 for _, p in model.named_parameters() if p.dtype == torch.bfloat16)\n",
    "if bf16_count > 0:\n",
    "    print(f\"\\u26a0\\ufe0f  Converting {bf16_count} bf16 parameters to fp16\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dtype == torch.bfloat16:\n",
    "            param.data = param.data.to(torch.float16)\n",
    "else:\n",
    "    print(\"\\u2705 No bf16 parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Training Steps\n",
    "\n",
    "**GPT5.2 #3:** Control by token budget / max_steps, not arbitrary epoch count.\n",
    "Cap at MAX_EPOCHS to prevent catastrophic forgetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "tokens_per_step = BATCH_SIZE * MAX_SEQ_LENGTH * GRADIENT_ACCUMULATION\nsteps_per_epoch = len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION)\n\n# Cap steps to fit compute budget\nmax_steps = min(steps_per_epoch, MAX_STEPS_CAP)\ntotal_tokens_trained = max_steps * tokens_per_step\n\n# Save/log intervals\nsave_steps = max(max_steps // 4, 50)\nlog_steps = max(max_steps // 40, 10)\neval_steps = max(max_steps // 8, 25)\n\nprint(f\"\\U0001f4ca Training Plan:\")\nprint(f\"   Dataset tokens:      {len(train_dataset) * MAX_SEQ_LENGTH:,}\")\nprint(f\"   Tokens/step:         {tokens_per_step:,}\")\nprint(f\"   Steps/epoch:         {steps_per_epoch:,}\")\nprint(f\"   Max steps (capped):  {max_steps:,}\")\nprint(f\"   Tokens to train on: {total_tokens_trained:,}\")\nprint(f\"   Coverage:            {100 * max_steps / steps_per_epoch:.0f}% of dataset\")\nprint(f\"   Save every:          {save_steps} steps\")\nprint(f\"   Log every:           {log_steps} steps\")\nprint(f\"   Eval every:          {eval_steps} steps\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === LOSS LOGGING ===\nclass LossLoggingCallback(TrainerCallback):\n    def __init__(self):\n        self.losses = []\n        self.eval_losses = []\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs:\n            if \"loss\" in logs:\n                step = state.global_step\n                loss = logs[\"loss\"]\n                lr = logs.get(\"learning_rate\", 0)\n                self.losses.append((step, loss))\n                print(f\"  Step {step:4d}/{max_steps} | Loss: {loss:.4f} | LR: {lr:.2e}\")\n            if \"eval_loss\" in logs:\n                eval_loss = logs[\"eval_loss\"]\n                ppl = np.exp(min(eval_loss, 20))\n                self.eval_losses.append((state.global_step, eval_loss))\n                print(f\"  \\U0001f4ca Eval Loss: {eval_loss:.4f} | Perplexity: {ppl:.1f}\")\n\nloss_callback = LossLoggingCallback()\n\n# === DATA COLLATOR ===\n@dataclass\nclass PackedDataCollator:\n    \"\"\"Collator for pre-packed, pre-tokenized sequences.\"\"\"\n    def __call__(self, features):\n        return {\n            \"input_ids\": torch.tensor([f[\"input_ids\"] for f in features], dtype=torch.long),\n            \"attention_mask\": torch.tensor([f[\"attention_mask\"] for f in features], dtype=torch.long),\n            \"labels\": torch.tensor([f[\"labels\"] for f in features], dtype=torch.long),\n        }\n\n# === TRAINER ===\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/vazhi-dapt-v1_0\",\n    max_steps=max_steps,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n    learning_rate=LEARNING_RATE,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=WARMUP_RATIO,\n    logging_steps=log_steps,\n    save_steps=save_steps,\n    eval_steps=eval_steps,\n    eval_strategy=\"steps\",\n    save_total_limit=3,\n    fp16=True,                    # T4/P100 both support fp16\n    bf16=False,\n    gradient_checkpointing=True,  # Needed for batch_size=8 on T4 (15GB)\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Suppress warning\n    max_grad_norm=1.0,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",\n    seed=RANDOM_SEED,\n    load_best_model_at_end=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=PackedDataCollator(),\n    callbacks=[loss_callback],\n)\n\nprint(\"\\u2705 Trainer ready\")\nprint(f\"   Steps: {max_steps} | LR: {LEARNING_RATE} | Effective BS: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\nprint(f\"   fp16: True | gradient_checkpointing: True (needed for batch 8 on T4)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\U0001f680 Starting DAPT training...\")\nprint(f\"   {max_steps} steps, fp16=True, no gradient checkpointing\")\nprint(f\"   Tokens: ~{max_steps * BATCH_SIZE * GRADIENT_ACCUMULATION * MAX_SEQ_LENGTH / 1e6:.0f}M\")\nprint()\n\ntrain_result = trainer.train()\n\nprint(\"\\n\\u2705 Training complete!\")\nmetrics = train_result.metrics\nfor k, v in metrics.items():\n    print(f\"   {k}: {v}\")\n\n# Final eval\nprint(\"\\n\\U0001f4ca Final eval on held-out blocks...\")\neval_metrics = trainer.evaluate()\neval_loss = eval_metrics.get(\"eval_loss\", float(\"inf\"))\neval_ppl = np.exp(min(eval_loss, 20))\nprint(f\"   Eval Loss:       {eval_loss:.4f}\")\nprint(f\"   Eval Perplexity: {eval_ppl:.1f}\")\n\n# Loss summary\nif loss_callback.losses:\n    start_loss = loss_callback.losses[0][1]\n    end_loss = loss_callback.losses[-1][1]\n    print(f\"\\n\\U0001f4c8 Loss: {start_loss:.4f} \\u2192 {end_loss:.4f} ({100*(start_loss - end_loss)/start_loss:.1f}% drop)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save & Upload LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_PATH = \"/kaggle/working/vazhi-dapt-v1_0-lora\"\n",
    "\n",
    "print(\"\\U0001f4be Saving LoRA adapter...\")\n",
    "trainer.save_model(ADAPTER_PATH)\n",
    "tokenizer.save_pretrained(ADAPTER_PATH)\n",
    "\n",
    "adapter_files = glob.glob(f\"{ADAPTER_PATH}/*\")\n",
    "print(f\"   Files: {[os.path.basename(f) for f in adapter_files]}\")\n",
    "assert any('adapter' in f for f in adapter_files), \"No adapter files!\"\n",
    "print(\"\\u2705 Adapter saved\")\n",
    "\n",
    "# Upload adapter backup (GPT5.2 #9)\n",
    "api = HfApi()\n",
    "api.create_repo(ADAPTER_REPO, exist_ok=True)\n",
    "print(f\"\\U0001f4e4 Uploading adapter to {ADAPTER_REPO}...\")\n",
    "api.upload_folder(\n",
    "    folder_path=ADAPTER_PATH,\n",
    "    repo_id=ADAPTER_REPO,\n",
    "    commit_message=f\"DAPT v1.0 adapter: Sangraha Tamil, r={LORA_R}, lr={LEARNING_RATE}\",\n",
    ")\n",
    "print(f\"\\u2705 Adapter uploaded: https://huggingface.co/{ADAPTER_REPO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free 4-bit model\n",
    "print(\"\\U0001f5d1\\ufe0f  Freeing 4-bit model...\")\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(f\"\\u2705 GPU memory: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Merge LoRA in FP16\n",
    "\n",
    "**Hard rule (Lesson #39):** NEVER merge into 4-bit. Reload base in fp16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\U0001f4e5 Reloading {BASE_MODEL} in fp16...\")\n",
    "base_model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\":0},\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(f\"\\u2705 Base loaded: {base_model_fp16.num_parameters():,} params\")\n",
    "\n",
    "print(f\"\\U0001f517 Applying LoRA adapter...\")\n",
    "peft_model = PeftModel.from_pretrained(base_model_fp16, ADAPTER_PATH)\n",
    "peft_model.gradient_checkpointing_disable()\n",
    "peft_model.config.use_cache = True\n",
    "peft_model.eval()\n",
    "\n",
    "print(\"\\U0001f500 Merging in fp16...\")\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "print(f\"\\u2705 Merged: {merged_model.num_parameters():,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. DAPT Evaluation\n",
    "\n",
    "**GPT5.2 #7:** Proper eval, not just a quick test.\n",
    "\n",
    "This is a Base model after DAPT — it won't follow instructions.\n",
    "It should generate coherent Tamil text continuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.eval()\n",
    "merged_model.config.use_cache = True\n",
    "\n",
    "eval_prompts = [\n",
    "    (\"prose\", \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\\u0ba8\\u0bbe\\u0b9f\\u0bc1 \\u0b87\\u0ba8\\u0bcd\\u0ba4\\u0bbf\\u0baf\\u0bbe\\u0bb5\\u0bbf\\u0ba9\\u0bcd \\u0ba4\\u0bc6\\u0ba9\\u0bcd \\u0baa\\u0b95\\u0bc1\\u0ba4\\u0bbf\\u0baf\\u0bbf\\u0bb2\\u0bcd \\u0b85\\u0bae\\u0bc8\\u0ba8\\u0bcd\\u0ba4\\u0bc1\\u0bb3\\u0bcd\\u0bb3 \\u0b92\\u0bb0\\u0bc1 \\u0bae\\u0bbe\\u0ba8\\u0bbf\\u0bb2\\u0bae\\u0bcd.\"),\n",
    "    (\"prose\", \"\\u0baa\\u0bca\\u0b99\\u0bcd\\u0b95\\u0bb2\\u0bcd \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bb0\\u0bcd\\u0b95\\u0bb3\\u0bbf\\u0ba9\\u0bcd \\u0bae\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bbf\\u0baf \\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0ba8\\u0bbe\\u0bb3\\u0bcd.\"),\n",
    "    (\"literature\", \"\\u0bb5\\u0bb3\\u0bcd\\u0bb3\\u0bc1\\u0bb5\\u0bb0\\u0bcd \\u0b95\\u0bc2\\u0bb1\\u0bbf\\u0baf \\u0b85\\u0bb1\\u0bae\\u0bcd, \\u0baa\\u0bca\\u0bb0\\u0bc1\\u0bb3\\u0bcd, \\u0b87\\u0ba9\\u0bcd\\u0baa\\u0bae\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0bb1 \\u0bae\\u0bc2\\u0ba9\\u0bcd\\u0bb1\\u0bc1\"),\n",
    "    (\"knowledge\", \"\\u0b9a\\u0bbf\\u0ba4\\u0bcd\\u0ba4 \\u0bae\\u0bb0\\u0bc1\\u0ba4\\u0bcd\\u0ba4\\u0bc1\\u0bb5\\u0bae\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0baa\\u0ba4\\u0bc1 \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd \\u0bae\\u0b95\\u0bcd\\u0b95\\u0bb3\\u0bbf\\u0ba9\\u0bcd \\u0baa\\u0bbe\\u0bb0\\u0bae\\u0bcd\\u0baa\\u0bb0\\u0bbf\\u0baf\"),\n",
    "    (\"daily\", \"\\u0b95\\u0bbe\\u0bb2\\u0bc8\\u0baf\\u0bbf\\u0bb2\\u0bcd \\u0b8e\\u0bb4\\u0bc1\\u0ba8\\u0bcd\\u0ba4\\u0ba4\\u0bc1\\u0bae\\u0bcd \\u0bae\\u0bc1\\u0ba4\\u0bb2\\u0bbf\\u0bb2\\u0bcd\"),\n",
    "    (\"short\", \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\"),\n",
    "    (\"short\", \"\\u0ba8\\u0ba9\\u0bcd\\u0bb1\\u0bbf\"),\n",
    "    (\"mixed\", \"India has many languages. \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd is one of the\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"\\U0001f9ea DAPT EVAL: {len(eval_prompts)} Tamil text continuations\")\n",
    "print(f\"   (Base model \\u2014 expect text continuation, not chat)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "eval_results = []\n",
    "\n",
    "for category, prompt_text in eval_prompts:\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(merged_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = merged_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=4,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    generated_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    t_pct = tamil_char_pct(response)\n",
    "    words = response.split()\n",
    "    unique_ratio = len(set(words)) / max(len(words), 1)\n",
    "    is_repetitive = unique_ratio < 0.3 and len(words) > 10\n",
    "    is_empty = len(response.strip()) < 10\n",
    "    is_code = any(kw in response[:100] for kw in ['def ', 'class ', 'import ', '{\"', 'var '])\n",
    "\n",
    "    status = \"\\u2705\"\n",
    "    if is_empty: status = \"\\u274c EMPTY\"\n",
    "    elif is_code: status = \"\\u274c CODE\"\n",
    "    elif is_repetitive: status = \"\\u26a0\\ufe0f LOOP\"\n",
    "    elif t_pct < 20 and category != \"mixed\": status = \"\\u26a0\\ufe0f LOW TAMIL\"\n",
    "\n",
    "    eval_results.append((category, prompt_text, response[:200], status, t_pct, unique_ratio))\n",
    "\n",
    "    print(f\"\\n[{category.upper()}] {status} (Tamil: {t_pct:.0f}%, Unique: {unique_ratio:.0%})\")\n",
    "    print(f\"  Prompt: {prompt_text[:60]}\")\n",
    "    print(f\"  Output: {response[:300]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"\\U0001f4ca DAPT EVAL SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "pass_count = sum(1 for r in eval_results if r[3] == \"\\u2705\")\n",
    "avg_tamil = np.mean([r[4] for r in eval_results])\n",
    "avg_unique = np.mean([r[5] for r in eval_results])\n",
    "print(f\"   Passed:      {pass_count}/{len(eval_results)}\")\n",
    "print(f\"   Avg Tamil%:  {avg_tamil:.0f}%\")\n",
    "print(f\"   Avg Unique:  {avg_unique:.0%}\")\n",
    "print(f\"   Eval PPL:    {eval_ppl:.1f}\")\n",
    "\n",
    "for cat, prompt, resp, status, tamil, uniq in eval_results:\n",
    "    print(f\"   {status} [{cat}] Tamil:{tamil:.0f}% Uniq:{uniq:.0%}\")\n",
    "\n",
    "if pass_count >= len(eval_results) * 0.7 and avg_tamil > 30:\n",
    "    print(f\"\\n\\U0001f389 DAPT successful! Proceed to SFT.\")\n",
    "elif pass_count >= len(eval_results) * 0.4:\n",
    "    print(f\"\\n\\u26a0\\ufe0f  Partial. Try more tokens or check loss curve.\")\n",
    "else:\n",
    "    print(f\"\\n\\u274c DAPT failed. Check loss curve, data quality, try r=32.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Upload Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "api.create_repo(OUTPUT_MODEL, exist_ok=True)\n",
    "\n",
    "print(f\"\\U0001f4e4 Pushing merged fp16 model to {OUTPUT_MODEL}...\")\n",
    "merged_model.push_to_hub(\n",
    "    OUTPUT_MODEL,\n",
    "    private=False,\n",
    "    commit_message=f\"DAPT v1.0: Tamil-adapted Qwen3-0.6B (Sangraha, QLoRA r={LORA_R})\",\n",
    ")\n",
    "tokenizer.push_to_hub(OUTPUT_MODEL)\n",
    "\n",
    "print(f\"\\n\\u2705 Model: https://huggingface.co/{OUTPUT_MODEL}\")\n",
    "print(f\"\\u2705 Adapter: https://huggingface.co/{ADAPTER_REPO}\")\n",
    "print(f\"\\n\\U0001f449 Next: Run SFT notebook with BASE_MODEL = \\\"{OUTPUT_MODEL}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Artifact | Repo | Purpose |\n",
    "|----------|------|---------|\n",
    "| Packed DAPT data | `CryptoYogi/vazhi-dapt-tamil-v1_0` | Reusable training data |\n",
    "| Merged fp16 model | `CryptoYogi/qwen3-0.6b-tamil` | Reusable Tamil base for SFT |\n",
    "| LoRA adapter | `CryptoYogi/qwen3-0.6b-tamil-lora` | Recovery backup |\n",
    "\n",
    "### Next: SFT (Stage 3)\n",
    "```python\n",
    "BASE_MODEL = \"CryptoYogi/qwen3-0.6b-tamil\"  # THIS model\n",
    "DATASET = \"CryptoYogi/vazhi-tamil-sft-v4_0\"  # or combined v3.6 + v4.0\n",
    "```\n",
    "\n",
    "### If DAPT failed\n",
    "1. Loss didn't decrease → data may be too noisy, check filters\n",
    "2. Tamil% low → increase TARGET_TOKENS in data prep, re-run\n",
    "3. Repetitive output → try r=32 in this notebook (just change LORA_R)\n",
    "4. All else fails → try Instruct model with very low LR (1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
