{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI Eval v4.0 ‚Äî Generation & Evaluation\n",
    "\n",
    "Standalone eval notebook for the SFT v4.0 merged model.\n",
    "\n",
    "**Why separate?** SFT training completed and model was uploaded to HF, but `<think>` token\n",
    "suppression failed during eval ‚Äî `suppress_tokens` kwarg doesn't work in transformers 2.8.0.\n",
    "This notebook fixes generation and re-evaluates without retraining.\n",
    "\n",
    "**Model:** `CryptoYogi/vazhi-v4_0` (SFT on DAPT v1.1, already on HuggingFace)\n",
    "\n",
    "**Fix:** Use `LogitsProcessorList` with `SuppressTokensLogitsProcessor` instead of\n",
    "`suppress_tokens` kwarg. Also strip `<think>...</think>` as belt-and-suspenders fallback.\n",
    "\n",
    "**Platform:** Kaggle T4 or Colab T4 (single GPU, ~1.2GB for fp16 model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U \\\n",
    "  \"transformers>=4.45.0,<5.0.0\" \\\n",
    "  \"accelerate>=0.34.2\"\n",
    "\n",
    "print(\"\\u2705 Dependencies installed\")\n",
    "print(\"\\u26a0\\ufe0f  RESTART THE SESSION NOW (Runtime \\u2192 Restart session)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport re\nimport torch\nimport numpy as np\nfrom transformers import (\n    AutoModelForCausalLM, AutoTokenizer,\n    LogitsProcessorList,\n)\n\nSFT_MODEL = \"CryptoYogi/vazhi-v4_0\"             # Merged SFT model on HF\nDAPT_MODEL = \"CryptoYogi/qwen3-0.6b-tamil-v1_1\"  # For comparison (optional)\nVANILLA_MODEL = \"Qwen/Qwen3-0.6B\"                # For comparison (optional)\n\n# Qwen3 instruct <think> token IDs (verified in training notebook)\nTHINK_TOKEN_IDS = [151667, 151668]\n\nSYSTEM_PROMPT = (\n    \"\\u0ba8\\u0bc0\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd VAZHI (\\u0bb5\\u0bb4\\u0bbf), \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd \\u0bae\\u0b95\\u0bcd\\u0b95\\u0bb3\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bbe\\u0ba9 AI \\u0b89\\u0ba4\\u0bb5\\u0bbf\\u0baf\\u0bbe\\u0bb3\\u0bb0\\u0bcd. \"\n    \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bbf\\u0bb2\\u0bcd \\u0ba4\\u0bc6\\u0bb3\\u0bbf\\u0bb5\\u0bbe\\u0b95\\u0bb5\\u0bc1\\u0bae\\u0bcd \\u0b89\\u0ba4\\u0bb5\\u0bbf\\u0baf\\u0bbe\\u0b95\\u0bb5\\u0bc1\\u0bae\\u0bcd \\u0baa\\u0ba4\\u0bbf\\u0bb2\\u0bb3\\u0bbf\\u0baf\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd. \"\n    '\\u0ba4\\u0bc6\\u0bb0\\u0bbf\\u0baf\\u0bbe\\u0bb5\\u0bbf\\u0b9f\\u0bcd\\u0b9f\\u0bbe\\u0bb2\\u0bcd \"\\u0ba4\\u0bc6\\u0bb0\\u0bbf\\u0baf\\u0bb5\\u0bbf\\u0bb2\\u0bcd\\u0bb2\\u0bc8\" \\u0b8e\\u0ba9\\u0bcd\\u0bb1\\u0bc1 \\u0b9a\\u0bca\\u0bb2\\u0bcd\\u0bb2\\u0bc1\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd.'\n)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"\\u2705 Config ready\")\nprint(f\"   Device: {device}\")\nif device == \"cuda\":\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"   Model: {SFT_MODEL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\U0001f4e5 Loading {SFT_MODEL}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\":0} if device == \"cuda\" else None,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "\n",
    "print(f\"\\u2705 Model loaded: {model.num_parameters():,} params\")\n",
    "print(f\"   Tokenizer: {len(tokenizer)} tokens\")\n",
    "print(f\"   eos: {tokenizer.eos_token!r} (ID {tokenizer.eos_token_id})\")\n",
    "\n",
    "# Verify <think> token IDs\n",
    "for tid in THINK_TOKEN_IDS:\n",
    "    decoded = tokenizer.decode([tid])\n",
    "    print(f\"   Token {tid}: {decoded!r}\")\n",
    "    assert decoded in ['<think>', '</think>'], f\"Token {tid} decodes to {decoded!r}, not a think tag!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fix `<think>` Suppression\n",
    "\n",
    "**Root cause:** `suppress_tokens` kwarg in `generate()` doesn't work in transformers 2.8.0.\n",
    "The SFT training notebook used it, but all 12 eval responses still started with `<think>`.\n",
    "\n",
    "**Fix:** Use `LogitsProcessorList` with `SuppressTokensLogitsProcessor` ‚Äî this is the\n",
    "explicit, version-safe way to suppress specific tokens during generation.\n",
    "\n",
    "**Belt & suspenders:** Also strip `<think>...</think>` from output text as fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build a custom logits processor that suppresses <think> tokens\n# SuppressTokensLogitsProcessor has a device mismatch bug in transformers 2.8.0\n# (stores token IDs on CPU while logits are on CUDA), so we roll our own.\n\nclass SuppressThinkTokens:\n    \"\"\"Suppress specific token IDs by setting their logits to -inf.\"\"\"\n    def __init__(self, token_ids, device):\n        self.suppress_ids = torch.tensor(token_ids, dtype=torch.long, device=device)\n    \n    def __call__(self, input_ids, scores):\n        scores[:, self.suppress_ids] = float('-inf')\n        return scores\n\nthink_suppressor = SuppressThinkTokens(THINK_TOKEN_IDS, device)\n\n# Verify suppression works\ntest_logits = torch.zeros(1, len(tokenizer)).to(device)\ntest_input_ids = torch.tensor([[151644]]).to(device)\nprocessed = think_suppressor(test_input_ids, test_logits)\nfor tid in THINK_TOKEN_IDS:\n    val = processed[0, tid].item()\n    print(f\"   Token {tid} logit after suppression: {val}\")\n    assert val == float('-inf'), f\"Suppression failed for token {tid}!\"\n\nprint(f\"\\u2705 Custom think suppressor verified (device: {device})\")\n\n\n# Check if tokenizer supports enable_thinking\ntry:\n    test = tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": \"test\"}],\n        tokenize=False, add_generation_prompt=True, enable_thinking=False,\n    )\n    USE_THINKING_FLAG = True\n    print(f\"\\u2705 Tokenizer supports enable_thinking=False\")\nexcept TypeError:\n    USE_THINKING_FLAG = False\n    print(f\"\\u26a0\\ufe0f enable_thinking not supported, using manual template\")\n\n\ndef build_chat_prompt(user_text):\n    msgs = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": user_text},\n    ]\n    if USE_THINKING_FLAG:\n        return tokenizer.apply_chat_template(\n            msgs, tokenize=False, add_generation_prompt=True, enable_thinking=False,\n        )\n    else:\n        return (\n            f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n\"\n            f\"<|im_start|>user\\n{user_text}<|im_end|>\\n\"\n            f\"<|im_start|>assistant\\n\"\n        )\n\n\ndef strip_think_tags(text):\n    \"\"\"Remove <think>...</think> blocks from response (belt & suspenders fallback).\"\"\"\n    text = re.sub(r'<think>.*?</think>\\s*', '', text, flags=re.DOTALL)\n    text = re.sub(r'</?think>', '', text)\n    return text.strip()\n\n\ndef extract_response(full_text):\n    \"\"\"Extract assistant response from generated text, stripping think tags.\"\"\"\n    if \"<|im_start|>assistant\" in full_text:\n        resp = full_text.split(\"<|im_start|>assistant\")[-1]\n        resp = resp.split(\"<|im_end|>\")[0].strip()\n        if resp.startswith(\"\\n\"):\n            resp = resp[1:]\n    else:\n        resp = full_text\n    return strip_think_tags(resp)\n\n\ndef count_tamil_chars(text):\n    return sum(1 for c in text if '\\u0B80' <= c <= '\\u0BFF')\n\ndef tamil_char_pct(text):\n    if not text: return 0.0\n    return 100.0 * count_tamil_chars(text) / len(text)\n\ndef compute_repeat_ratio(text, n=3):\n    \"\"\"Fraction of tokens in repeated n-gram chains. >0.2 is bad.\"\"\"\n    words = text.split()\n    if len(words) < n:\n        return 0.0\n    ngrams = [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n    seen = set()\n    repeated_positions = set()\n    for i, ng in enumerate(ngrams):\n        if ng in seen:\n            for j in range(i, i + n):\n                repeated_positions.add(j)\n        seen.add(ng)\n    return len(repeated_positions) / max(len(words), 1)\n\n\nprint(f\"\\u2705 Generation helpers ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Smoke Test ‚Äî Does Suppression Work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: generate a single response and check for <think> tags\n",
    "print(\"\\U0001f9ea Smoke test: single generation with LogitsProcessor...\")\n",
    "\n",
    "test_prompt = build_chat_prompt(\"\\u0bb5\\u0ba3\\u0b95\\u0bcd\\u0b95\\u0bae\\u0bcd\")\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "logits_processors = LogitsProcessorList([think_suppressor])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        logits_processor=logits_processors,\n",
    "    )\n",
    "\n",
    "full = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "response = extract_response(full)\n",
    "\n",
    "# Check for think tags in RAW output (before stripping)\n",
    "raw_resp = full.split(\"<|im_start|>assistant\")[-1].split(\"<|im_end|>\")[0] if \"<|im_start|>assistant\" in full else full\n",
    "has_think_raw = \"<think>\" in raw_resp\n",
    "\n",
    "print(f\"\\n  Prompt: \\u0bb5\\u0ba3\\u0b95\\u0bcd\\u0b95\\u0bae\\u0bcd\")\n",
    "print(f\"  Raw output (first 300 chars): {raw_resp[:300]}\")\n",
    "print(f\"  Clean response: {response[:300]}\")\n",
    "print(f\"  <think> in raw: {has_think_raw}\")\n",
    "print(f\"  Tamil %: {tamil_char_pct(response):.0f}%\")\n",
    "\n",
    "if not has_think_raw:\n",
    "    print(f\"\\n\\u2705 LogitsProcessor suppression WORKS! No <think> tokens generated.\")\n",
    "else:\n",
    "    print(f\"\\n\\u26a0\\ufe0f <think> still present in raw output. Checking if strip_think_tags catches it...\")\n",
    "    if \"<think>\" not in response:\n",
    "        print(f\"   \\u2705 strip_think_tags fallback works \\u2014 cleaned response has no think tags.\")\n",
    "    else:\n",
    "        print(f\"   \\u274c Both methods failed! Need different approach.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Full Evaluation ‚Äî 12 Chat-Templated Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    # Greetings (2)\n",
    "    (\"greeting\", \"\\u0bb5\\u0ba3\\u0b95\\u0bcd\\u0b95\\u0bae\\u0bcd\"),\n",
    "    (\"greeting\", \"\\u0ba8\\u0bc0\\u0b99\\u0bcd\\u0b95\\u0bb3\\u0bcd \\u0baf\\u0bbe\\u0bb0\\u0bcd?\"),\n",
    "    # Factual (3)\n",
    "    (\"factual\", \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\\u0ba8\\u0bbe\\u0b9f\\u0bcd\\u0b9f\\u0bbf\\u0ba9\\u0bcd \\u0ba4\\u0bb2\\u0bc8\\u0ba8\\u0b95\\u0bb0\\u0bae\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0ba9?\"),\n",
    "    (\"factual\", \"\\u0baa\\u0bca\\u0b99\\u0bcd\\u0b95\\u0bb2\\u0bcd \\u0b8e\\u0baa\\u0bcd\\u0baa\\u0bcb\\u0ba4\\u0bc1 \\u0b95\\u0bca\\u0ba3\\u0bcd\\u0b9f\\u0bbe\\u0b9f\\u0baa\\u0bcd\\u0baa\\u0b9f\\u0bc1\\u0b95\\u0bbf\\u0bb1\\u0ba4\\u0bc1?\"),\n",
    "    (\"factual\", \"2+2 \\u0b8e\\u0ba9\\u0bcd\\u0ba9?\"),\n",
    "    # Culture (2)\n",
    "    (\"culture\", \"\\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0bb5\\u0bb3\\u0bcd\\u0bb3\\u0bc1\\u0bb5\\u0bb0\\u0bcd \\u0baf\\u0bbe\\u0bb0\\u0bcd?\"),\n",
    "    (\"culture\", \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd \\u0bae\\u0bca\\u0bb4\\u0bbf\\u0baf\\u0bbf\\u0ba9\\u0bcd \\u0b9a\\u0bbf\\u0bb1\\u0baa\\u0bcd\\u0baa\\u0bc1 \\u0b8e\\u0ba9\\u0bcd\\u0ba9?\"),\n",
    "    # Safety (2)\n",
    "    (\"safety\", \"\\u0b92\\u0bb0\\u0bc1 scam message \\u0bb5\\u0ba8\\u0bcd\\u0ba4\\u0bbe\\u0bb2\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0ba9 \\u0b9a\\u0bc6\\u0baf\\u0bcd\\u0bb5\\u0ba4\\u0bc1?\"),\n",
    "    (\"safety\", \"\\u0bb5\\u0bc0\\u0b9f\\u0bcd\\u0b9f\\u0bbf\\u0bb2\\u0bcd \\u0ba4\\u0bc0 \\u0bb5\\u0bbf\\u0baa\\u0ba4\\u0bcd\\u0ba4\\u0bc1 \\u0b8e\\u0ba9\\u0bcd\\u0ba9 \\u0b9a\\u0bc6\\u0baf\\u0bcd\\u0baf \\u0bb5\\u0bc7\\u0ba3\\u0bcd\\u0b9f\\u0bc1\\u0bae\\u0bcd?\"),\n",
    "    # Refusal (2)\n",
    "    (\"refusal\", \"\\u0ba8\\u0bbe\\u0bb3\\u0bc8 \\u0baa\\u0b99\\u0bcd\\u0b95\\u0bc1 \\u0b9a\\u0ba8\\u0bcd\\u0ba4\\u0bc8 \\u0b8f\\u0bb1\\u0bc1\\u0bae\\u0bbe?\"),\n",
    "    (\"refusal\", \"\\u0b8e\\u0ba9\\u0bcd \\u0b95\\u0ba3\\u0bbf\\u0ba9\\u0bbf\\u0baf\\u0bbf\\u0bb2\\u0bcd \\u0bb5\\u0bc8\\u0bb0\\u0bb8\\u0bcd \\u0b87\\u0bb0\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bbf\\u0bb1\\u0ba4\\u0bbe?\"),\n",
    "    # General (1)\n",
    "    (\"general\", \"\\u0b95\\u0bbe\\u0bb2\\u0bc8\\u0baf\\u0bbf\\u0bb2\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0ba9 \\u0b9a\\u0bbe\\u0baa\\u0bcd\\u0baa\\u0bbf\\u0b9f\\u0bb2\\u0bbe\\u0bae\\u0bcd?\"),\n",
    "]\n",
    "\n",
    "logits_processors = LogitsProcessorList([think_suppressor])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"\\U0001f9ea SFT v4.0 EVAL (with <think> suppression fix)\")\n",
    "print(f\"   Model: {SFT_MODEL}\")\n",
    "print(f\"   Method: LogitsProcessorList + strip_think_tags fallback\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for category, prompt_text in test_prompts:\n",
    "    full_prompt = build_chat_prompt(prompt_text)\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=150,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        logits_processor=logits_processors,\n",
    "        no_repeat_ngram_size=4,\n",
    "    )\n",
    "    if category == \"factual\":\n",
    "        gen_kwargs[\"do_sample\"] = False\n",
    "    else:\n",
    "        gen_kwargs[\"do_sample\"] = True\n",
    "        gen_kwargs[\"temperature\"] = 0.3\n",
    "        gen_kwargs[\"top_p\"] = 0.9\n",
    "        gen_kwargs[\"repetition_penalty\"] = 1.2\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    full = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Get raw response (before think stripping) to check suppression\n",
    "    if \"<|im_start|>assistant\" in full:\n",
    "        raw_resp = full.split(\"<|im_start|>assistant\")[-1].split(\"<|im_end|>\")[0]\n",
    "    else:\n",
    "        raw_resp = full\n",
    "    had_think = \"<think>\" in raw_resp\n",
    "    \n",
    "    # Clean response\n",
    "    response = extract_response(full)\n",
    "\n",
    "    t_pct = tamil_char_pct(response)\n",
    "    repeat_r = compute_repeat_ratio(response)\n",
    "    has_loop = repeat_r > 0.2\n",
    "    has_system = \"system\" in response.lower()[:50]\n",
    "    is_empty = len(response.strip()) < 5\n",
    "    is_code = any(c in response[:100] for c in ['=True', '={\"', 'var ', 'function', '<br'])\n",
    "\n",
    "    status = \"\\u2705\"\n",
    "    if is_code: status = \"\\u274c CODE\"\n",
    "    elif has_loop: status = \"\\u26a0\\ufe0f LOOP\"\n",
    "    elif has_system: status = \"\\u274c SYSTEM\"\n",
    "    elif is_empty: status = \"\\u274c EMPTY\"\n",
    "    elif t_pct < 20 and category not in [\"factual\"]: status = \"\\u26a0\\ufe0f LOW TAMIL\"\n",
    "\n",
    "    think_flag = \" [think leaked]\" if had_think else \"\"\n",
    "    results.append((category, prompt_text, response[:300], status, t_pct, repeat_r, had_think))\n",
    "\n",
    "    print(f\"\\n[{category.upper()}] {status} (Tamil: {t_pct:.0f}%, Rep: {repeat_r:.2f}){think_flag}\")\n",
    "    print(f\"  Q: {prompt_text}\")\n",
    "    print(f\"  A: {response[:300]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVAL SUMMARY ===\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"\\U0001f4ca SFT v4.0 EVAL SUMMARY (with <think> fix)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "pass_count = sum(1 for r in results if r[3] == \"\\u2705\")\n",
    "avg_tamil = np.mean([r[4] for r in results])\n",
    "avg_repeat = np.mean([r[5] for r in results])\n",
    "max_repeat = max(r[5] for r in results)\n",
    "think_leaked = sum(1 for r in results if r[6])\n",
    "\n",
    "print(f\"   Passed:         {pass_count}/{len(results)}\")\n",
    "print(f\"   Avg Tamil:      {avg_tamil:.0f}%\")\n",
    "print(f\"   Avg Repeat:     {avg_repeat:.2f} (>0.2 is bad)\")\n",
    "print(f\"   Max Repeat:     {max_repeat:.2f}\")\n",
    "print(f\"   Think leaked:   {think_leaked}/{len(results)} (0 = suppression works)\")\n",
    "print()\n",
    "\n",
    "for cat, prompt, resp, status, tamil, repeat, think in results:\n",
    "    think_mark = \" [think]\" if think else \"\"\n",
    "    print(f\"   {status} [{cat}] {prompt[:40]}... (Tamil: {tamil:.0f}%, Rep: {repeat:.2f}){think_mark}\")\n",
    "\n",
    "print(f\"\\n\\U0001f4cb Comparison with previous runs:\")\n",
    "print(f\"   v4.0 (broken suppress): 0/12 passed, avg Tamil 45% \\u274c (all THINK)\")\n",
    "print(f\"   v3.8 (SFT-only, no DAPT): 0/12 passed, avg Tamil 52% \\u274c\")\n",
    "print(f\"   v3.6 (merge corruption):  0/12 passed, 0% Tamil \\u274c\")\n",
    "\n",
    "if pass_count >= len(results) * 0.8 and avg_tamil > 30 and avg_repeat < 0.2:\n",
    "    print(f\"\\n\\U0001f389 SFT v4.0 successful! Proceed to GGUF quantization.\")\n",
    "elif pass_count >= len(results) * 0.5:\n",
    "    print(f\"\\n\\u26a0\\ufe0f Partial success ({pass_count}/{len(results)}). Review outputs manually.\")\n",
    "    print(f\"   If content quality is ok but metrics are borderline, proceed to GGUF.\")\n",
    "    print(f\"   If content is gibberish, SFT needs more data or different hyperparameters.\")\n",
    "else:\n",
    "    print(f\"\\n\\u274c SFT eval failed even with <think> fix.\")\n",
    "    print(f\"   Content quality is the issue, not just token suppression.\")\n",
    "    print(f\"   Next steps:\")\n",
    "    print(f\"     1. Try LoRA r=8 (less overfitting with 1,365 samples)\")\n",
    "    print(f\"     2. Try 2 epochs instead of 3\")\n",
    "    print(f\"     3. Try higher LR (5e-5) for stronger instruction signal\")\n",
    "    print(f\"     4. Target fewer LoRA modules (q_proj, v_proj only)\")\n",
    "    print(f\"     5. Add more training data (more IndicAlign samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (Optional) Side-by-Side: SFT vs DAPT-only vs Vanilla\n",
    "\n",
    "Run the same prompts on all 3 models to see if SFT helped, hurt, or had no effect.\n",
    "This helps diagnose whether the issue is SFT quality or just generation config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import gc\n\n# Select a subset for comparison (faster)\ncomparison_prompts = [\n    (\"greeting\", \"\\u0bb5\\u0ba3\\u0b95\\u0bcd\\u0b95\\u0bae\\u0bcd\"),\n    (\"factual\", \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\\u0ba8\\u0bbe\\u0b9f\\u0bcd\\u0b9f\\u0bbf\\u0ba9\\u0bcd \\u0ba4\\u0bb2\\u0bc8\\u0ba8\\u0b95\\u0bb0\\u0bae\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0ba9?\"),\n    (\"safety\", \"\\u0b92\\u0bb0\\u0bc1 scam message \\u0bb5\\u0ba8\\u0bcd\\u0ba4\\u0bbe\\u0bb2\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0ba9 \\u0b9a\\u0bc6\\u0baf\\u0bcd\\u0bb5\\u0ba4\\u0bc1?\"),\n    (\"culture\", \"\\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0bb5\\u0bb3\\u0bcd\\u0bb3\\u0bc1\\u0bb5\\u0bb0\\u0bcd \\u0baf\\u0bbe\\u0bb0\\u0bcd?\"),\n]\n\ndef eval_model_on_prompts(model_obj, tokenizer_obj, prompts, model_name):\n    \"\"\"Run prompts and return results.\"\"\"\n    results = []\n    model_obj.eval()\n\n    # CRITICAL: Clear suppress_tokens from generation_config to prevent\n    # generate() from injecting the buggy built-in SuppressTokensLogitsProcessor\n    # (which has a CPU/CUDA device mismatch in transformers 2.8.0).\n    # Our custom SuppressThinkTokens handles suppression correctly.\n    if hasattr(model_obj, 'generation_config') and hasattr(model_obj.generation_config, 'suppress_tokens'):\n        model_obj.generation_config.suppress_tokens = None\n\n    suppressor = SuppressThinkTokens(THINK_TOKEN_IDS, model_obj.device)\n    procs = LogitsProcessorList([suppressor])\n    \n    for category, prompt_text in prompts:\n        full_prompt = build_chat_prompt(prompt_text)\n        inputs = tokenizer_obj(full_prompt, return_tensors=\"pt\").to(model_obj.device)\n        gen_kwargs = dict(\n            max_new_tokens=100, do_sample=False,\n            eos_token_id=tokenizer_obj.eos_token_id,\n            pad_token_id=tokenizer_obj.eos_token_id,\n            logits_processor=procs,\n        )\n        with torch.no_grad():\n            outputs = model_obj.generate(**inputs, **gen_kwargs)\n        full = tokenizer_obj.decode(outputs[0], skip_special_tokens=False)\n        response = extract_response(full)\n        results.append((category, prompt_text, response[:200], tamil_char_pct(response)))\n    return results\n\n\n# Clear suppress_tokens on SFT model BEFORE running\nif hasattr(model, 'generation_config') and hasattr(model.generation_config, 'suppress_tokens'):\n    model.generation_config.suppress_tokens = None\n    print(\"üîß Cleared suppress_tokens from SFT model generation_config\")\n\nprint(\"üß™ Running comparison prompts on SFT model...\")\nsft_results = eval_model_on_prompts(model, tokenizer, comparison_prompts, \"SFT v4.0\")\n\n# Free SFT model, load DAPT\ndel model; gc.collect(); torch.cuda.empty_cache()\nprint(f\"\\nüì• Loading DAPT model: {DAPT_MODEL}...\")\ndapt_tok = AutoTokenizer.from_pretrained(DAPT_MODEL, trust_remote_code=True)\ndapt_model = AutoModelForCausalLM.from_pretrained(\n    DAPT_MODEL, torch_dtype=torch.float16, device_map={\"\":0}, trust_remote_code=True,\n)\ndapt_model.config.use_cache = True\ndapt_results = eval_model_on_prompts(dapt_model, dapt_tok, comparison_prompts, \"DAPT v1.1\")\n\n# Free DAPT, load vanilla\ndel dapt_model; gc.collect(); torch.cuda.empty_cache()\nprint(f\"\\nüì• Loading vanilla: {VANILLA_MODEL}...\")\nvan_tok = AutoTokenizer.from_pretrained(VANILLA_MODEL, trust_remote_code=True)\nvan_model = AutoModelForCausalLM.from_pretrained(\n    VANILLA_MODEL, torch_dtype=torch.float16, device_map={\"\":0}, trust_remote_code=True,\n)\nvan_model.config.use_cache = True\nvan_results = eval_model_on_prompts(van_model, van_tok, comparison_prompts, \"Vanilla\")\ndel van_model; gc.collect(); torch.cuda.empty_cache()\n\n# Side by side\nprint(f\"\\n{'='*70}\")\nprint(f\"üìä SIDE-BY-SIDE COMPARISON\")\nprint(f\"{'='*70}\")\n\nfor i, (cat, prompt) in enumerate(comparison_prompts):\n    print(f\"\\n‚îå‚îÄ [{cat.upper()}] {prompt[:50]}\")\n    print(f\"‚îÇ\")\n    print(f\"‚îÇ VANILLA (Tamil {van_results[i][3]:.0f}%):\")\n    print(f\"‚îÇ   {van_results[i][2][:200]}\")\n    print(f\"‚îÇ\")\n    print(f\"‚îÇ DAPT (Tamil {dapt_results[i][3]:.0f}%):\")\n    print(f\"‚îÇ   {dapt_results[i][2][:200]}\")\n    print(f\"‚îÇ\")\n    print(f\"‚îÇ SFT (Tamil {sft_results[i][3]:.0f}%):\")\n    print(f\"‚îÇ   {sft_results[i][2][:200]}\")\n    print(f\"‚îî{'‚îÄ' * 69}\")\n\n# Summary\navg_van = np.mean([r[3] for r in van_results])\navg_dapt = np.mean([r[3] for r in dapt_results])\navg_sft = np.mean([r[3] for r in sft_results])\nprint(f\"\\nüìä Average Tamil %: Vanilla {avg_van:.0f}% ‚Üí DAPT {avg_dapt:.0f}% ‚Üí SFT {avg_sft:.0f}%\")\n\nif avg_sft > avg_dapt:\n    print(f\"‚úÖ SFT improved over DAPT!\")\nelif avg_sft > avg_van:\n    print(f\"‚ö†Ô∏è SFT worse than DAPT but better than vanilla.\")\nelse:\n    print(f\"‚ùå SFT degraded below vanilla. Training may have overfit or damaged DAPT gains.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tests the already-trained SFT v4.0 model with proper `<think>` suppression.\n",
    "\n",
    "### If eval passes\n",
    "The original SFT training was fine ‚Äî only the generation config was wrong.\n",
    "Proceed to GGUF quantization.\n",
    "\n",
    "### If eval still fails\n",
    "Content quality is genuinely poor. The side-by-side comparison (Section 6) will show\n",
    "whether SFT helped, hurt, or had no effect compared to DAPT-only. Based on that:\n",
    "\n",
    "| Outcome | Diagnosis | Next Step |\n",
    "|---------|-----------|----------|\n",
    "| SFT > DAPT > Vanilla | SFT working, needs more data | Add more training samples |\n",
    "| DAPT > SFT > Vanilla | SFT overfitting | Reduce LoRA r=8, 2 epochs |\n",
    "| DAPT > Vanilla > SFT | SFT destructive | Major config issue, investigate |\n",
    "| All similar | Instruction-following not learned | Higher LR (5e-5), more epochs |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
