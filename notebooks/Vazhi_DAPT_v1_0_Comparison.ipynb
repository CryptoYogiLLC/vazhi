{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI DAPT v1.0 — Base vs DAPT Comparison\n",
    "\n",
    "Quick comparison: run the same 8 Tamil text continuation prompts on:\n",
    "1. **Vanilla base:** `Qwen/Qwen3-0.6B-Base` (no Tamil training)\n",
    "2. **DAPT'd model:** `CryptoYogi/qwen3-0.6b-tamil` (375 steps on Sangraha Tamil)\n",
    "\n",
    "**Platform:** Colab T4 or Kaggle T4 (1.2GB per model in fp16)\n",
    "\n",
    "**Expected outcome:** Base model should produce English/code/gibberish for Tamil prompts. DAPT'd model should produce coherent Tamil continuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"transformers>=4.45.0,<5.0.0\" \"accelerate>=0.34.2\" \"torch\"\n",
    "print(\"\\u2705 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen3-0.6B-Base\"\n",
    "DAPT_MODEL = \"CryptoYogi/qwen3-0.6b-tamil\"\n",
    "\n",
    "# Same eval prompts from DAPT training notebook\n",
    "eval_prompts = [\n",
    "    (\"prose\", \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\\u0ba8\\u0bbe\\u0b9f\\u0bc1 \\u0b87\\u0ba8\\u0bcd\\u0ba4\\u0bbf\\u0baf\\u0bbe\\u0bb5\\u0bbf\\u0ba9\\u0bcd \\u0ba4\\u0bc6\\u0ba9\\u0bcd \\u0baa\\u0b95\\u0bc1\\u0ba4\\u0bbf\\u0baf\\u0bbf\\u0bb2\\u0bcd \\u0b85\\u0bae\\u0bc8\\u0ba8\\u0bcd\\u0ba4\\u0bc1\\u0bb3\\u0bcd\\u0bb3 \\u0b92\\u0bb0\\u0bc1 \\u0bae\\u0bbe\\u0ba8\\u0bbf\\u0bb2\\u0bae\\u0bcd.\"),\n",
    "    (\"prose\", \"\\u0baa\\u0bca\\u0b99\\u0bcd\\u0b95\\u0bb2\\u0bcd \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bb0\\u0bcd\\u0b95\\u0bb3\\u0bbf\\u0ba9\\u0bcd \\u0bae\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bbf\\u0baf \\u0ba4\\u0bbf\\u0bb0\\u0bc1\\u0ba8\\u0bbe\\u0bb3\\u0bcd.\"),\n",
    "    (\"literature\", \"\\u0bb5\\u0bb3\\u0bcd\\u0bb3\\u0bc1\\u0bb5\\u0bb0\\u0bcd \\u0b95\\u0bc2\\u0bb1\\u0bbf\\u0baf \\u0b85\\u0bb1\\u0bae\\u0bcd, \\u0baa\\u0bca\\u0bb0\\u0bc1\\u0bb3\\u0bcd, \\u0b87\\u0ba9\\u0bcd\\u0baa\\u0bae\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0bb1 \\u0bae\\u0bc2\\u0ba9\\u0bcd\\u0bb1\\u0bc1\"),\n",
    "    (\"knowledge\", \"\\u0b9a\\u0bbf\\u0ba4\\u0bcd\\u0ba4 \\u0bae\\u0bb0\\u0bc1\\u0ba4\\u0bcd\\u0ba4\\u0bc1\\u0bb5\\u0bae\\u0bcd \\u0b8e\\u0ba9\\u0bcd\\u0baa\\u0ba4\\u0bc1 \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd \\u0bae\\u0b95\\u0bcd\\u0b95\\u0bb3\\u0bbf\\u0ba9\\u0bcd \\u0baa\\u0bbe\\u0bb0\\u0bae\\u0bcd\\u0baa\\u0bb0\\u0bbf\\u0baf\"),\n",
    "    (\"daily\", \"\\u0b95\\u0bbe\\u0bb2\\u0bc8\\u0baf\\u0bbf\\u0bb2\\u0bcd \\u0b8e\\u0bb4\\u0bc1\\u0ba8\\u0bcd\\u0ba4\\u0ba4\\u0bc1\\u0bae\\u0bcd \\u0bae\\u0bc1\\u0ba4\\u0bb2\\u0bbf\\u0bb2\\u0bcd\"),\n",
    "    (\"short\", \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\"),\n",
    "    (\"short\", \"\\u0ba8\\u0ba9\\u0bcd\\u0bb1\\u0bbf\"),\n",
    "    (\"mixed\", \"India has many languages. \\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd is one of the\"),\n",
    "]\n",
    "\n",
    "def count_tamil_chars(text):\n",
    "    return sum(1 for c in text if '\\u0B80' <= c <= '\\u0BFF')\n",
    "\n",
    "def tamil_char_pct(text):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    return 100.0 * count_tamil_chars(text) / len(text)\n",
    "\n",
    "def generate_responses(model, tokenizer, device):\n",
    "    \"\"\"Run all eval prompts and return results.\"\"\"\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for category, prompt_text in eval_prompts:\n",
    "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2,\n",
    "                no_repeat_ngram_size=4,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        generated_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        t_pct = tamil_char_pct(response)\n",
    "        words = response.split()\n",
    "        unique_ratio = len(set(words)) / max(len(words), 1)\n",
    "        results.append({\n",
    "            \"category\": category,\n",
    "            \"prompt\": prompt_text,\n",
    "            \"response\": response[:300],\n",
    "            \"tamil_pct\": t_pct,\n",
    "            \"unique_ratio\": unique_ratio,\n",
    "            \"length\": len(response),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "print(f\"\\u2705 Config ready\")\n",
    "print(f\"   Base:  {BASE_MODEL}\")\n",
    "print(f\"   DAPT:  {DAPT_MODEL}\")\n",
    "print(f\"   Prompts: {len(eval_prompts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Base Model (Vanilla Qwen3-0.6B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\U0001f4e5 Loading vanilla base model: {BASE_MODEL}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map={\"\":0} if device == \"cuda\" else None,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "base_model.config.use_cache = True\n",
    "print(f\"\\u2705 Base model loaded on {device}\")\n",
    "\n",
    "print(f\"\\n\\U0001f9ea Running {len(eval_prompts)} eval prompts on BASE model...\")\n",
    "base_results = generate_responses(base_model, tokenizer, device)\n",
    "\n",
    "for r in base_results:\n",
    "    print(f\"\\n[{r['category'].upper()}] Tamil: {r['tamil_pct']:.0f}% | Unique: {r['unique_ratio']:.0%}\")\n",
    "    print(f\"  Prompt: {r['prompt'][:60]}\")\n",
    "    print(f\"  Output: {r['response'][:200]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Free memory\n",
    "del base_model\n",
    "gc.collect()\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "print(f\"\\n\\U0001f5d1\\ufe0f Base model freed from GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DAPT Model (Tamil-adapted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\U0001f4e5 Loading DAPT model: {DAPT_MODEL}\")\n",
    "dapt_model = AutoModelForCausalLM.from_pretrained(\n",
    "    DAPT_MODEL,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map={\"\":0} if device == \"cuda\" else None,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "dapt_model.config.use_cache = True\n",
    "print(f\"\\u2705 DAPT model loaded on {device}\")\n",
    "\n",
    "print(f\"\\n\\U0001f9ea Running {len(eval_prompts)} eval prompts on DAPT model...\")\n",
    "dapt_results = generate_responses(dapt_model, tokenizer, device)\n",
    "\n",
    "for r in dapt_results:\n",
    "    print(f\"\\n[{r['category'].upper()}] Tamil: {r['tamil_pct']:.0f}% | Unique: {r['unique_ratio']:.0%}\")\n",
    "    print(f\"  Prompt: {r['prompt'][:60]}\")\n",
    "    print(f\"  Output: {r['response'][:200]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "del dapt_model\n",
    "gc.collect()\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "print(f\"\\n\\U0001f5d1\\ufe0f DAPT model freed from GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\U0001f4ca SIDE-BY-SIDE COMPARISON: Base vs DAPT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Category':<12} {'Base Tamil%':>12} {'DAPT Tamil%':>12} {'Base Uniq':>10} {'DAPT Uniq':>10} {'Winner':>8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "base_wins = 0\n",
    "dapt_wins = 0\n",
    "\n",
    "for b, d in zip(base_results, dapt_results):\n",
    "    winner = \"DAPT\" if d[\"tamil_pct\"] > b[\"tamil_pct\"] else (\"BASE\" if b[\"tamil_pct\"] > d[\"tamil_pct\"] else \"TIE\")\n",
    "    if winner == \"DAPT\":\n",
    "        dapt_wins += 1\n",
    "    elif winner == \"BASE\":\n",
    "        base_wins += 1\n",
    "    print(f\"{b['category']:<12} {b['tamil_pct']:>10.0f}% {d['tamil_pct']:>10.0f}% {b['unique_ratio']:>9.0%} {d['unique_ratio']:>9.0%} {winner:>8}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "\n",
    "avg_base_tamil = np.mean([r[\"tamil_pct\"] for r in base_results])\n",
    "avg_dapt_tamil = np.mean([r[\"tamil_pct\"] for r in dapt_results])\n",
    "avg_base_uniq = np.mean([r[\"unique_ratio\"] for r in base_results])\n",
    "avg_dapt_uniq = np.mean([r[\"unique_ratio\"] for r in dapt_results])\n",
    "\n",
    "print(f\"{'AVERAGE':<12} {avg_base_tamil:>10.0f}% {avg_dapt_tamil:>10.0f}% {avg_base_uniq:>9.0%} {avg_dapt_uniq:>9.0%}\")\n",
    "print()\n",
    "print(f\"\\U0001f3c6 DAPT wins: {dapt_wins}/{len(eval_prompts)} | Base wins: {base_wins}/{len(eval_prompts)}\")\n",
    "print(f\"   Tamil% improvement: {avg_base_tamil:.0f}% \\u2192 {avg_dapt_tamil:.0f}% ({avg_dapt_tamil - avg_base_tamil:+.0f}%)\")\n",
    "\n",
    "if avg_dapt_tamil > avg_base_tamil + 5:\n",
    "    print(f\"\\n\\U0001f389 DAPT clearly improved Tamil fluency! Proceed to SFT.\")\n",
    "elif avg_dapt_tamil > avg_base_tamil:\n",
    "    print(f\"\\n\\u2705 DAPT shows marginal improvement. Consider more training tokens.\")\n",
    "elif avg_dapt_tamil < avg_base_tamil:\n",
    "    print(f\"\\n\\u26a0\\ufe0f DAPT degraded Tamil output! Check training data quality.\")\n",
    "else:\n",
    "    print(f\"\\n\\U0001f914 No change. Base model may already know Tamil — check outputs manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed side-by-side output for manual inspection\n",
    "print(\"=\" * 70)\n",
    "print(\"\\U0001f50d DETAILED OUTPUT COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for b, d in zip(base_results, dapt_results):\n",
    "    print(f\"\\n\\u250c\\u2500 [{b['category'].upper()}] Prompt: {b['prompt'][:60]}\")\n",
    "    print(f\"\\u2502\")\n",
    "    print(f\"\\u2502 BASE (Tamil {b['tamil_pct']:.0f}%):\")\n",
    "    print(f\"\\u2502   {b['response'][:200]}\")\n",
    "    print(f\"\\u2502\")\n",
    "    print(f\"\\u2502 DAPT (Tamil {d['tamil_pct']:.0f}%):\")\n",
    "    print(f\"\\u2502   {d['response'][:200]}\")\n",
    "    print(f\"\\u2514{'\\u2500' * 69}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
