{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# VAZHI SFT v3.8 ‚Äî v4.0 Dataset + fp16 Merge\n\n**v3.8 = v3.7 merge fix + v4.0 curated dataset (ADR-010)**\n\n**What changed from v3.7:**\n- **Dataset:** `CryptoYogi/vazhi-tamil-sft-v4_0` (1,514 samples, properly composed)\n  - 50% domain packs (security, govt, education, legal, healthcare, culture)\n  - 33% IndicAlign diversity (Dolly_T, WikiHow, Wiki_Conv, OpenAssistant_T)\n  - 6% Kural interpretive (hard-capped, anti-memorization filtered)\n  - 3% handcrafted (guardrails, refusal, brevity, greeting)\n  - 8% general knowledge (dialects, emotions, daily routines)\n- **Output:** `CryptoYogi/vazhi-qwen3-v3_8`\n\n**Carried over from v3.7 (all correct):**\n1. Save LoRA adapter separately (not merged into 4-bit)\n2. Reload base model in fp16 ‚Üí merge in full precision\n3. Disable gradient checkpointing before eval\n4. Text-based loss logging\n5. Pre-merge sanity check on PeftModel\n\n**Target:** Kaggle P100 (16GB)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "**After running this cell, RESTART the session** (Runtime ‚Üí Restart session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies ‚Äî pin TRL to avoid DataCollatorForCompletionOnlyLM removal\n",
    "!pip install -q -U \\\n",
    "  \"transformers>=4.51.0\" \\\n",
    "  \"accelerate>=0.34.2\" \\\n",
    "  \"peft>=0.12.0\" \\\n",
    "  \"trl>=0.12.0,<0.20.0\" \\\n",
    "  \"bitsandbytes>=0.43.3\" \\\n",
    "  \"datasets>=2.21.0\" \\\n",
    "  \"huggingface_hub>=0.24.7\"\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")\n",
    "print(\"‚ö†Ô∏è  RESTART THE SESSION NOW (Runtime ‚Üí Restart session)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Force single GPU BEFORE importing torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport json\nimport re\nimport random\nimport torch\nimport numpy as np\nfrom datasets import load_dataset, Dataset\nfrom huggingface_hub import login, HfApi\n\nfrom transformers import (\n    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n    TrainerCallback,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\n# === KEY CONFIG ===\nDATASET_NAME = \"CryptoYogi/vazhi-tamil-sft-v4_0\"  # ADR-010 curated dataset\nBASE_MODEL = \"Qwen/Qwen3-0.6B\"                    # INSTRUCT model (NOT Base!)\nOUTPUT_MODEL = \"CryptoYogi/vazhi-qwen3-v3_8\"\n\n# Same hyperparameters as v3.6/v3.7 (training config was correct)\nLEARNING_RATE = 2e-5\nNUM_EPOCHS = 3\nMAX_SEQ_LENGTH = 1024\nLORA_R = 16\nLORA_ALPHA = 32\n\nSYSTEM_PROMPT = (\n    \"‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç VAZHI (‡Æµ‡Æ¥‡Æø), ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æ© AI ‡Æâ‡Æ§‡Æµ‡Æø‡ÆØ‡Ææ‡Æ≥‡Æ∞‡Øç. \"\n    \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Æø‡Æ≤‡Øç ‡Æ§‡ØÜ‡Æ≥‡Æø‡Æµ‡Ææ‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç ‡Æâ‡Æ§‡Æµ‡Æø‡ÆØ‡Ææ‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Æ§‡Æø‡Æ≤‡Æ≥‡Æø‡ÆØ‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç. \"\n    '‡Æ§‡ØÜ‡Æ∞‡Æø‡ÆØ‡Ææ‡Æµ‡Æø‡Æü‡Øç‡Æü‡Ææ‡Æ≤‡Øç \"‡Æ§‡ØÜ‡Æ∞‡Æø‡ÆØ‡Æµ‡Æø‡Æ≤‡Øç‡Æ≤‡Øà\" ‡Æé‡Æ©‡Øç‡Æ±‡ØÅ ‡Æö‡Øä‡Æ≤‡Øç‡Æ≤‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç.'\n)\n\nprint(f\"‚úÖ Configuration loaded\")\nprint(f\"   PyTorch: {torch.__version__}\")\nprint(f\"   CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"\")\nprint(f\"üîë v3.8 = v3.7 merge fix + v4.0 dataset:\")\nprint(f\"   Dataset: {DATASET_NAME}\")\nprint(f\"   Output:  {OUTPUT_MODEL}\")\nprint(f\"   FIX: LoRA merge in fp16, NOT 4-bit\")\nprint(f\"   NEW: ADR-010 curated dataset (balanced composition)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "print(\"‚úÖ Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Load v4.0 Curated Dataset\n\n**ADR-010 dataset** built by `Vazhi_Dataset_Factory_v4_0.ipynb`:\n- 1,365 train / 149 validation samples\n- Composition-enforced: domain packs 50%, IndicAlign 33%, kural 6%, handcrafted 3%, general 8%\n- All samples are strict ChatML with Tamil char % >= 30%"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === HELPER FUNCTIONS ===\n\ndef count_tamil_chars(text):\n    \"\"\"Count Tamil Unicode characters.\"\"\"\n    return sum(1 for c in text if '‡ÆÄ' <= c <= '‡Øø')\n\ndef tamil_char_pct(text):\n    \"\"\"Get Tamil character percentage.\"\"\"\n    if not text:\n        return 0.0\n    return 100.0 * count_tamil_chars(text) / len(text)\n\n# === LOAD DATASET ===\nprint(f\"üìö Loading dataset from {DATASET_NAME}...\")\nds = load_dataset(DATASET_NAME)\nbalanced_ds = ds[\"train\"]\neval_ds = ds[\"validation\"]\nprint(f\"   Train: {len(balanced_ds)} samples\")\nprint(f\"   Validation: {len(eval_ds)} samples\")\n\n# Quick stats on train split\nkural_count = sum(1 for item in balanced_ds\n                  if any(k in item['text'] for k in ['‡Æï‡ØÅ‡Æ±‡Æ≥‡Øç', '‡Æ§‡Æø‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡Æ±‡Æ≥‡Øç']))\navg_len = sum(len(item['text']) for item in balanced_ds) / len(balanced_ds)\nshort_count = sum(1 for item in balanced_ds if len(item['text']) < 400)\n\n# Bucket distribution (v4.0 has bucket field)\nfrom collections import Counter\nbucket_dist = Counter(item.get('bucket', 'unknown') for item in balanced_ds)\n\nprint(f\"\\nüìä Train dataset stats:\")\nprint(f\"   Total samples: {len(balanced_ds)}\")\nprint(f\"   Kural: {kural_count} ({100*kural_count/len(balanced_ds):.1f}%)\")\nprint(f\"   Short (<400 chars): {short_count} ({100*short_count/len(balanced_ds):.1f}%)\")\nprint(f\"   Avg length: {avg_len:.0f} chars\")\nprint(f\"   Buckets:\")\nfor bucket, count in sorted(bucket_dist.items(), key=lambda x: -x[1]):\n    print(f\"     {bucket}: {count} ({100*count/len(balanced_ds):.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model + Tokenizer\n",
    "\n",
    "**CRITICAL:** Using `Qwen/Qwen3-0.6B` (INSTRUCT), not Base.\n",
    "The instruct model already has Tamil capability ‚Äî v3.3 proved this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üì• Loading tokenizer from {BASE_MODEL}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"‚úÖ Tokenizer ready: {len(tokenizer)} tokens\")\n",
    "print(f\"   pad_token: {tokenizer.pad_token!r} (ID {tokenizer.pad_token_id})\")\n",
    "print(f\"   eos_token: {tokenizer.eos_token!r} (ID {tokenizer.eos_token_id})\")\n",
    "\n",
    "# Verify ChatML tokens exist\n",
    "for token in [\"<|im_start|>\", \"<|im_end|>\"]:\n",
    "    assert token in tokenizer.get_vocab(), f\"Missing {token} in tokenizer!\"\n",
    "print(\"‚úÖ ChatML tokens present in tokenizer\")\n",
    "\n",
    "# Get <think> token IDs for suppression during generation\n",
    "think_open_ids = tokenizer.encode(\"<think>\", add_special_tokens=False)\n",
    "think_close_ids = tokenizer.encode(\"</think>\", add_special_tokens=False)\n",
    "suppress_ids = list(set(think_open_ids + think_close_ids))\n",
    "print(f\"\\nüß† <think> token IDs to suppress: {suppress_ids}\")\n",
    "print(f\"   Decoded: {[tokenizer.decode([t]) for t in suppress_ids]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization config (for training memory only)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"üì• Loading model {BASE_MODEL} in 4-bit (for training)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\":0},\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.use_cache = False  # Required for gradient checkpointing\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model.num_parameters():,} params\")\n",
    "print(f\"   ‚ö†Ô∏è  4-bit is for training memory ONLY\")\n",
    "print(f\"   ‚ö†Ô∏è  Will merge LoRA in fp16 (NOT 4-bit) after training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Convert any bf16 params to fp16 (safety check for P100)\n",
    "bf16_count = sum(1 for _, p in model.named_parameters() if p.dtype == torch.bfloat16)\n",
    "if bf16_count > 0:\n",
    "    print(f\"‚ö†Ô∏è  Converting {bf16_count} bf16 parameters to fp16\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dtype == torch.bfloat16:\n",
    "            param.data = param.data.to(torch.float16)\n",
    "else:\n",
    "    print(\"‚úÖ No bf16 parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Completion-Only Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template_str = \"<|im_start|>assistant\\n\"\n",
    "response_template_ids = tokenizer.encode(response_template_str, add_special_tokens=False)\n",
    "\n",
    "print(f\"Response template: {response_template_str!r}\")\n",
    "print(f\"Token IDs: {response_template_ids}\")\n",
    "print(f\"Decoded back: {tokenizer.decode(response_template_ids)!r}\")\n",
    "\n",
    "# Fallback: without trailing newline\n",
    "response_template_short = \"<|im_start|>assistant\"\n",
    "response_template_short_ids = tokenizer.encode(response_template_short, add_special_tokens=False)\n",
    "print(f\"\\nShort template: {response_template_short!r}\")\n",
    "print(f\"Short token IDs: {response_template_short_ids}\")\n",
    "\n",
    "# Verify template in actual data\n",
    "sample_text = balanced_ds[0][\"text\"]\n",
    "sample_ids = tokenizer.encode(sample_text, add_special_tokens=False)\n",
    "\n",
    "def find_template(sample_ids, template_ids):\n",
    "    for i in range(len(sample_ids) - len(template_ids) + 1):\n",
    "        if sample_ids[i:i+len(template_ids)] == template_ids:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "pos = find_template(sample_ids, response_template_ids)\n",
    "if pos >= 0:\n",
    "    print(f\"\\n‚úÖ Full template found at token position {pos}\")\n",
    "    use_template_ids = response_template_ids\n",
    "else:\n",
    "    pos = find_template(sample_ids, response_template_short_ids)\n",
    "    if pos >= 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Full template NOT found, using short template at position {pos}\")\n",
    "        use_template_ids = response_template_short_ids\n",
    "    else:\n",
    "        raise RuntimeError(\"STOP: Neither template found in tokenized sample!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collator and run preflight\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=use_template_ids,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Preflight masking verification (20 samples)...\")\n",
    "fail_count = 0\n",
    "total_trainable = 0\n",
    "total_tokens = 0\n",
    "\n",
    "for idx in range(min(20, len(balanced_ds))):\n",
    "    t = tokenizer(\n",
    "        balanced_ds[idx][\"text\"],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "    )\n",
    "    b = collator([{\"input_ids\": t[\"input_ids\"][0], \"attention_mask\": t[\"attention_mask\"][0]}])\n",
    "    n_train = (b[\"labels\"][0] != -100).sum().item()\n",
    "    n_total = len(b[\"labels\"][0])\n",
    "    total_trainable += n_train\n",
    "    total_tokens += n_total\n",
    "\n",
    "    if n_train == 0 or n_train == n_total:\n",
    "        fail_count += 1\n",
    "        status = \"‚ùå ALL MASKED\" if n_train == 0 else \"‚ùå NO MASKING\"\n",
    "        print(f\"   Sample {idx}: {n_train}/{n_total} {status}\")\n",
    "\n",
    "if fail_count == 0:\n",
    "    pct = 100 * total_trainable / total_tokens\n",
    "    print(f\"   All 20 samples passed ‚úÖ\")\n",
    "    print(f\"   Avg trainable: {pct:.1f}% of tokens\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå {fail_count}/20 samples have masking issues!\")\n",
    "    if fail_count > 5:\n",
    "        raise RuntimeError(\"TOO MANY FAILURES ‚Äî DO NOT PROCEED WITH TRAINING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training\n",
    "\n",
    "**Same as v3.6** plus text-based loss logging (v3.6 only had HTML widget, loss wasn't visible in notebook output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === TEXT-BASED LOSS LOGGING ===\n# v3.6 only had HTML widget ‚Äî loss wasn't captured in notebook output.\n# This callback prints loss as plain text so we can verify convergence.\n\nclass LossLoggingCallback(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs and \"loss\" in logs:\n            lr = logs.get(\"learning_rate\", 0)\n            step = state.global_step\n            loss = logs[\"loss\"]\n            print(f\"  Step {step:4d} | Loss: {loss:.4f} | LR: {lr:.2e}\")\n\n\nsft_config = SFTConfig(\n    output_dir=\"/kaggle/working/vazhi-v3_8\",\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=16,\n    learning_rate=LEARNING_RATE,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n    logging_steps=25,\n    save_steps=50,\n    save_total_limit=3,\n    fp16=False,            # FP32 mode for P100\n    bf16=False,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",\n    dataset_text_field=\"text\",\n    max_seq_length=MAX_SEQ_LENGTH,\n    packing=False,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=balanced_ds,\n    args=sft_config,\n    processing_class=tokenizer,\n    data_collator=collator,\n    callbacks=[LossLoggingCallback()],\n)\n\nprint(\"‚úÖ Trainer initialized\")\nprint(f\"   Model: {BASE_MODEL} (INSTRUCT)\")\nprint(f\"   Dataset: {DATASET_NAME} ({len(balanced_ds)} train samples)\")\nprint(f\"   LR: {LEARNING_RATE}\")\nprint(f\"   Epochs: {NUM_EPOCHS}\")\nprint(f\"   LoRA: r={LORA_R}, alpha={LORA_ALPHA}\")\nprint(f\"   Max seq length: {MAX_SEQ_LENGTH}\")\nprint(f\"   Loss logging: TEXT (not just HTML widget)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Starting training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "\n",
    "# Print final metrics as text (so they're captured in notebook output)\n",
    "metrics = train_result.metrics\n",
    "print(f\"\\nüìä Final Training Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"   {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save LoRA Adapter (NOT Merged Model)\n",
    "\n",
    "**CRITICAL FIX from v3.6:** Do NOT call `model.merge_and_unload()` on the 4-bit model.\n",
    "Instead, save the LoRA adapter, then reload base model in fp16 for merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "ADAPTER_PATH = \"/kaggle/working/vazhi-v3_8-lora\"\n\nprint(\"üíæ Saving LoRA adapter (NOT merging into 4-bit!)...\")\ntrainer.save_model(ADAPTER_PATH)\ntokenizer.save_pretrained(ADAPTER_PATH)\nprint(f\"‚úÖ LoRA adapter saved to {ADAPTER_PATH}\")\n\n# Verify adapter files exist\nimport glob\nadapter_files = glob.glob(f\"{ADAPTER_PATH}/*\")\nprint(f\"   Files: {[os.path.basename(f) for f in adapter_files]}\")\nassert any('adapter' in f for f in adapter_files), \"No adapter files found!\"\nprint(\"‚úÖ Adapter files verified\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FREE 4-BIT MODEL FROM GPU ===\n",
    "print(\"üóëÔ∏è  Freeing 4-bit training model from GPU...\")\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "print(f\"‚úÖ GPU memory after cleanup: {gpu_mem:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reload Base Model in FP16 + Merge LoRA\n",
    "\n",
    "**This is THE fix.** Load the base model in fp16 (~1.5GB), apply LoRA adapter, merge in full precision.\n",
    "No 4-bit rounding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üì• Reloading {BASE_MODEL} in FP16 (NOT 4-bit)...\")\n",
    "base_model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\":0},\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "print(f\"‚úÖ Base model loaded in fp16: {base_model_fp16.num_parameters():,} params\")\n",
    "print(f\"   GPU memory: {gpu_mem:.1f} GB\")\n",
    "\n",
    "# Load LoRA adapter onto fp16 model\n",
    "print(f\"\\nüîó Loading LoRA adapter from {ADAPTER_PATH}...\")\n",
    "peft_model = PeftModel.from_pretrained(base_model_fp16, ADAPTER_PATH)\n",
    "print(f\"‚úÖ LoRA adapter loaded\")\n",
    "\n",
    "# Disable gradient checkpointing BEFORE any generation\n",
    "peft_model.gradient_checkpointing_disable()\n",
    "peft_model.config.use_cache = True\n",
    "peft_model.eval()\n",
    "print(\"‚úÖ Gradient checkpointing disabled, use_cache=True, eval mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === PRE-MERGE SANITY CHECK ===\n# Test the PeftModel BEFORE merge to verify training worked\n# Uses apply_chat_template (GPT5.2 suggestion) for exact format matching\n\nprint(\"üß™ Pre-merge sanity check (PeftModel, before merge_and_unload)...\")\n\n# Check if tokenizer supports enable_thinking parameter\ntry:\n    test_tmpl = tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": \"test\"}],\n        tokenize=False, add_generation_prompt=True, enable_thinking=False,\n    )\n    USE_THINKING_FLAG = True\n    print(\"‚úÖ Tokenizer supports enable_thinking=False\")\nexcept TypeError:\n    USE_THINKING_FLAG = False\n    print(\"‚ö†Ô∏è  Tokenizer doesn't support enable_thinking, using manual template\")\n\ndef build_prompt(prompt_text):\n    \"\"\"Build prompt using apply_chat_template when possible.\"\"\"\n    msgs = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": prompt_text},\n    ]\n    if USE_THINKING_FLAG:\n        return tokenizer.apply_chat_template(\n            msgs, tokenize=False, add_generation_prompt=True, enable_thinking=False,\n        )\n    else:\n        return (\n            f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n\"\n            f\"<|im_start|>user\\n{prompt_text}<|im_end|>\\n\"\n            f\"<|im_start|>assistant\\n\"\n        )\n\nsanity_prompts = [\n    \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç\",\n    \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ©‡Øç ‡Æ§‡Æ≤‡Øà‡Æ®‡Æï‡Æ∞‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\",\n    \"‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡ÆØ‡Ææ‡Æ∞‡Øç?\",\n]\n\nfor prompt_text in sanity_prompts:\n    full_prompt = build_prompt(prompt_text)\n    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(peft_model.device)\n\n    with torch.no_grad():\n        outputs = peft_model.generate(\n            **inputs,\n            max_new_tokens=100,\n            do_sample=False,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.eos_token_id,\n            suppress_tokens=suppress_ids,\n        )\n\n    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    if \"<|im_start|>assistant\" in response:\n        response = response.split(\"<|im_start|>assistant\")[-1]\n        response = response.split(\"<|im_end|>\")[0].strip()\n        if response.startswith(\"\\n\"):\n            response = response[1:]\n\n    tamil_pct = tamil_char_pct(response)\n    print(f\"\\n  Q: {prompt_text}\")\n    print(f\"  A: {response[:200]}\")\n    print(f\"  Tamil: {tamil_pct:.0f}%\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"If the above responses are Tamil (not garbage), training worked!\")\nprint(\"Proceeding with merge...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MERGE IN FP16 ‚Äî THE FIX ===\n",
    "print(\"üîÄ Merging LoRA weights in FP16 (NO rounding errors)...\")\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Verify no 4-bit warning appeared\n",
    "print(\"‚úÖ LoRA merged in fp16 ‚Äî no 4-bit rounding errors\")\n",
    "print(f\"   Model params: {merged_model.num_parameters():,}\")\n",
    "\n",
    "gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "print(f\"   GPU memory: {gpu_mem:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "merged_model.eval()\nmerged_model.config.use_cache = True\n\ntest_prompts = [\n    # Greetings (2)\n    (\"greeting\", \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç\"),\n    (\"greeting\", \"‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡ÆØ‡Ææ‡Æ∞‡Øç?\"),\n    # Factual (3) ‚Äî greedy\n    (\"factual\", \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ©‡Øç ‡Æ§‡Æ≤‡Øà‡Æ®‡Æï‡Æ∞‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\"),\n    (\"factual\", \"2+2 ‡Æé‡Æ©‡Øç‡Æ©?\"),\n    (\"factual\", \"‡Æ™‡Øä‡Æô‡Øç‡Æï‡Æ≤‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Øã‡Æ§‡ØÅ ‡Æï‡Øä‡Æ£‡Øç‡Æü‡Ææ‡Æü‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ?\"),\n    # Culture (2)\n    (\"culture\", \"‡Æ§‡Æø‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡Æ±‡Æ≥‡Æø‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Øç ‡Æï‡ØÅ‡Æ±‡Æ≥‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\"),\n    (\"culture\", \"‡Æ§‡Æø‡Æ∞‡ØÅ‡Æµ‡Æ≥‡Øç‡Æ≥‡ØÅ‡Æµ‡Æ∞‡Øç ‡ÆØ‡Ææ‡Æ∞‡Øç?\"),\n    # Safety (2)\n    (\"safety\", \"‡Æí‡Æ∞‡ØÅ scam message ‡Æµ‡Æ®‡Øç‡Æ§‡Ææ‡Æ≤‡Øç ‡Æé‡Æ©‡Øç‡Æ© ‡Æö‡ØÜ‡ÆØ‡Øç‡Æµ‡Æ§‡ØÅ?\"),\n    (\"safety\", \"‡Æµ‡ØÄ‡Æü‡Øç‡Æü‡Æø‡Æ≤‡Øç ‡Æ§‡ØÄ ‡Æµ‡Æø‡Æ™‡Æ§‡Øç‡Æ§‡ØÅ ‡Æè‡Æ±‡Øç‡Æ™‡Æü‡Øç‡Æü‡Ææ‡Æ≤‡Øç ‡Æé‡Æ©‡Øç‡Æ© ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç?\"),\n    # Refusal (2)\n    (\"refusal\", \"‡Æ®‡Ææ‡Æ≥‡Øà ‡Æ™‡Æô‡Øç‡Æï‡ØÅ ‡Æö‡Æ®‡Øç‡Æ§‡Øà ‡Æè‡Æ±‡ØÅ‡ÆÆ‡Ææ?\"),\n    (\"refusal\", \"‡Æé‡Æ©‡Øç ‡Æï‡Æ£‡Æø‡Æ©‡Æø‡ÆØ‡Æø‡Æ≤‡Øç ‡Æµ‡Øà‡Æ∞‡Æ∏‡Øç ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æ§‡Ææ?\"),\n    # General (1)\n    (\"general\", \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡ÆÆ‡Øä‡Æ¥‡Æø‡ÆØ‡Æø‡Æ©‡Øç ‡Æö‡Æø‡Æ±‡Æ™‡Øç‡Æ™‡ØÅ ‡Æé‡Æ©‡Øç‡Æ©?\"),\n]\n\nprint(f\"\\n{'='*60}\")\nprint(f\"üß™ EVALUATION: {len(test_prompts)} prompts (on fp16 merged model)\")\nprint(f\"   Using: {'apply_chat_template(enable_thinking=False)' if USE_THINKING_FLAG else 'manual ChatML'}\")\nprint(f\"{'='*60}\")\n\nresults = []\n\nfor category, prompt_text in test_prompts:\n    # Use apply_chat_template when available (GPT5.2 suggestion)\n    full_prompt = build_prompt(prompt_text)\n    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(merged_model.device)\n\n    gen_kwargs = dict(\n        max_new_tokens=150,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.eos_token_id,\n        suppress_tokens=suppress_ids,\n        no_repeat_ngram_size=4,  # GPT5.2 suggestion: repetition control\n    )\n\n    if category == \"factual\":\n        gen_kwargs[\"do_sample\"] = False\n    else:\n        gen_kwargs[\"do_sample\"] = True\n        gen_kwargs[\"temperature\"] = 0.3\n        gen_kwargs[\"top_p\"] = 0.9\n        gen_kwargs[\"repetition_penalty\"] = 1.2\n\n    with torch.no_grad():\n        outputs = merged_model.generate(**inputs, **gen_kwargs)\n\n    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    if \"<|im_start|>assistant\" in response:\n        response = response.split(\"<|im_start|>assistant\")[-1]\n        response = response.split(\"<|im_end|>\")[0].strip()\n        if response.startswith(\"\\n\"):\n            response = response[1:]\n\n    # Quality checks\n    tamil_pct = tamil_char_pct(response)\n    has_loop = len(set(response.split())) < max(3, len(response.split()) * 0.3) if response.split() else True\n    has_system = \"system\" in response.lower()[:50]\n    has_think = \"<think>\" in response\n    is_empty = len(response.strip()) < 5\n    is_code = any(c in response[:100] for c in ['=True', '=\"', 'var ', 'function', '{\"type', '<br'])\n\n    status = \"\\u2705\"\n    if is_code: status = \"\\u274c CODE\"\n    elif has_loop: status = \"\\u26a0\\ufe0f LOOP\"\n    elif has_system: status = \"\\u274c SYSTEM LEAK\"\n    elif has_think: status = \"\\u274c THINK LEAK\"\n    elif is_empty: status = \"\\u274c EMPTY\"\n    elif tamil_pct < 20 and category not in [\"factual\"]:\n        status = \"\\u26a0\\ufe0f LOW TAMIL\"\n\n    results.append((category, prompt_text, response[:200], status, tamil_pct))\n\n    print(f\"\\n[{category.upper()}] {status} (Tamil: {tamil_pct:.0f}%)\")\n    print(f\"Q: {prompt_text}\")\n    print(f\"A: {response[:300]}\")\n    print(\"-\" * 50)\n\n# Summary\nprint(f\"\\n{'='*60}\")\nprint(f\"\\ud83d\\udcca EVALUATION SUMMARY\")\nprint(f\"{'='*60}\")\npass_count = sum(1 for r in results if r[3] == \"\\u2705\")\navg_tamil = sum(r[4] for r in results) / len(results)\nprint(f\"   Passed: {pass_count}/{len(results)}\")\nprint(f\"   Avg Tamil: {avg_tamil:.0f}%\")\nfor cat, prompt, resp, status, tamil in results:\n    print(f\"   {status} [{cat}] {prompt[:40]}... (Tamil: {tamil:.0f}%)\")\n\nif pass_count >= len(results) * 0.8 and avg_tamil > 30:\n    print(f\"\\n\\ud83c\\udf89 Model looks good!\")\nelif pass_count >= len(results) * 0.5:\n    print(f\"\\n\\u26a0\\ufe0f  Partially working. Review failures above.\")\nelse:\n    print(f\"\\n\\u274c Too many failures. Check:\")\n    print(f\"   1. Loss curve above \\u2014 did training converge?\")\n    print(f\"   2. Pre-merge sanity check \\u2014 did PeftModel work?\")\n    print(f\"   3. Consider DAPT stage before SFT\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Push to HuggingFace\n\nAlways upload ‚Äî eval may have false negatives. Better to upload and discard than miss a good model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "api = HfApi()\napi.create_repo(OUTPUT_MODEL, exist_ok=True)\n\nprint(f\"üì§ Pushing merged fp16 model to {OUTPUT_MODEL}...\")\nmerged_model.push_to_hub(OUTPUT_MODEL, private=False)\ntokenizer.push_to_hub(OUTPUT_MODEL, private=False)\n\nprint(f\"\\n‚úÖ Model uploaded: https://huggingface.co/{OUTPUT_MODEL}\")\nprint(f\"   Eval passed: {pass_count}/{len(results)}, Avg Tamil: {avg_tamil:.0f}%\")\nprint(f\"   Review eval results above to decide if model is usable\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n### v3.8 Changes from v3.7\n\n| What | v3.7 | v3.8 (this notebook) |\n|------|------|---------------------|\n| **Dataset** | `vazhi-tamil-sft-v3_6` (3,667 samples) | **`vazhi-tamil-sft-v4_0` (1,514 samples, ADR-010)** |\n| **Composition** | Uncontrolled (72% Kural) | **Enforced: 50% domain, 33% IndicAlign, 6% Kural, 3% handcrafted, 8% general** |\n| **Training** | Same as v3.6 | Same (LR 2e-5, 3 epochs, LoRA r=16) |\n| **LoRA merge** | fp16 (fixed from v3.6) | Same (fp16) |\n| **Everything else** | Same | Same |\n\n### Key difference: Dataset quality over quantity\n\nv3.6/v3.7 used 3,667 samples with 72% Thirukkural (memorization risk).\nv3.8 uses 1,514 samples with proper diversity ‚Äî domain knowledge, IndicAlign general Tamil, guardrails, and capped Kural interpretations.\n\n### If this succeeds:\n1. Convert to GGUF (Q4_K_M ~462MB, Q5_K_M ~526MB)\n2. Test on mobile via Flutter app\n3. Ship hybrid retrieval + LLM reasoning\n\n### If output quality is low:\n1. Check loss curve ‚Äî did it converge with fewer samples?\n2. If training converged but output is weak: increase epochs to 5, or add more data to Dataset Factory\n3. If training didn't converge: dataset may be too small, consider combining v3.6 + v4.0 data"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
