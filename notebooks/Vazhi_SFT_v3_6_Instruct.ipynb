{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAZHI SFT v3.6 — Return to Instruct Model\n",
    "\n",
    "**Key changes from v3.5 (failed):**\n",
    "1. **Back to instruct model** — Qwen3-0.6B (NOT Base). The instruct model already has Tamil capability; the base model produced code garbage with SFT-only\n",
    "2. **`<think>` token suppression** — suppress during generation, not during training\n",
    "3. **LR 2e-5** — v3.3 used 1e-4 which was too aggressive for instruct model\n",
    "4. **Completion-only masking** — kept from v3.5, but with robust response template\n",
    "5. **Strict ChatML validation** — regex-based filter rejects samples missing user/assistant\n",
    "6. **Length filtering** — cap at 1500 chars to avoid truncation issues\n",
    "7. **Dataset rebalancing** — add refusal/brevity samples, reduce Kural dominance\n",
    "8. **Quality evaluation** — Tamil char %, coherence checks, not just pattern absence\n",
    "\n",
    "### Why this will work\n",
    "v3.3 proved the instruct model produces Tamil. It had three fixable issues:\n",
    "- `<think>` tags in output → suppressed via `suppress_tokens` in generation\n",
    "- LR 1e-4 too aggressive → reduced to 2e-5\n",
    "- Kural-biased responses → dataset rebalancing\n",
    "\n",
    "v3.5's pivot to base model was a regression — we fix what works, not start over.\n",
    "\n",
    "**Target:** Kaggle P100 (16GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "**After running this cell, RESTART the session** (Runtime → Restart session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies — pin TRL to avoid DataCollatorForCompletionOnlyLM removal\n",
    "!pip install -q -U \\\n",
    "  \"transformers>=4.51.0\" \\\n",
    "  \"accelerate>=0.34.2\" \\\n",
    "  \"peft>=0.12.0\" \\\n",
    "  \"trl>=0.12.0,<0.20.0\" \\\n",
    "  \"bitsandbytes>=0.43.3\" \\\n",
    "  \"datasets>=2.21.0\" \\\n",
    "  \"huggingface_hub>=0.24.7\"\n",
    "\n",
    "print(\"\\u2705 Dependencies installed\")\n",
    "print(\"\\u26a0\\ufe0f  RESTART THE SESSION NOW (Runtime \\u2192 Restart session)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Force single GPU BEFORE importing torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport json\nimport re\nimport random\nimport torch\nimport numpy as np\nfrom datasets import load_dataset, Dataset\nfrom huggingface_hub import login, HfApi\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\n# === KEY CONFIG ===\nSOURCE_DATASET = \"CryptoYogi/vazhi-tamil-sft-v3_3\"  # Existing balanced dataset\nBASE_MODEL = \"Qwen/Qwen3-0.6B\"                     # INSTRUCT model (NOT Base!)\nOUTPUT_DATASET = \"CryptoYogi/vazhi-tamil-sft-v3_6\"\nOUTPUT_MODEL = \"CryptoYogi/vazhi-qwen3-v3_6\"\n\n# v3.6 hyperparameters\nLEARNING_RATE = 2e-5   # v3.3 used 1e-4 (too aggressive)\nNUM_EPOCHS = 3\nMAX_SEQ_LENGTH = 1024  # v3.5 showed 512 causes too many truncation warnings\nMAX_CHAR_LENGTH = 1500 # Filter out samples longer than this (avoids truncation)\nLORA_R = 16            # v3.5 used 32 but instruct model needs less adaptation\nLORA_ALPHA = 32\n\nSYSTEM_PROMPT = (\n    \"நீங்கள் VAZHI (வழி), தமிழ் மக்களுக்கான AI உதவியாளர். \"\n    \"தமிழில் தெளிவாகவும் உதவியாகவும் பதிலளியுங்கள். \"\n    'தெரியாவிட்டால் \"தெரியவில்லை\" என்று சொல்லுங்கள்.'\n)\n\nprint(f\"\\u2705 Configuration loaded\")\nprint(f\"   PyTorch: {torch.__version__}\")\nprint(f\"   CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"\")\nprint(f\"\\U0001f511 KEY CHANGES in v3.6:\")\nprint(f\"   1. INSTRUCT model: {BASE_MODEL} (NOT Base!)\")\nprint(f\"   2. LR: {LEARNING_RATE} (v3.3 used 1e-4)\")\nprint(f\"   3. <think> token suppression during generation\")\nprint(f\"   4. Completion-only masking (robust template)\")\nprint(f\"   5. Strict ChatML validation + length filtering\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "print(\"\\u2705 Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Construction — Strict ChatML + Rebalancing\n",
    "\n",
    "**Rules (from 10 failed attempts):**\n",
    "1. Every sample MUST have `<|im_start|>user` AND `<|im_start|>assistant` with non-empty content\n",
    "2. No raw text — only ChatML formatted samples\n",
    "3. Max 1500 chars per sample — avoids truncation at 1024 tokens\n",
    "4. No exact verse memorization — Thirukkural retrieval is handled by SQLite\n",
    "5. Include refusal and brevity discipline samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === STRICT ChatML VALIDATOR ===\n# Regex-based: rejects anything without proper user AND assistant segments\n\nCHATML_PATTERN = re.compile(\n    r'<\\|im_start\\|>system\\n.+?<\\|im_end\\|>\\n'\n    r'<\\|im_start\\|>user\\n(.+?)<\\|im_end\\|>\\n'\n    r'<\\|im_start\\|>assistant\\n(.+?)<\\|im_end\\|>',\n    re.DOTALL\n)\n\ndef validate_chatml_strict(text):\n    \"\"\"Validate a sample has proper ChatML with non-empty user AND assistant.\"\"\"\n    match = CHATML_PATTERN.search(text)\n    if not match:\n        return False, \"no ChatML structure found\"\n    \n    user_content = match.group(1).strip()\n    assistant_content = match.group(2).strip()\n    \n    if len(user_content) < 2:\n        return False, \"empty user content\"\n    if len(assistant_content) < 2:\n        return False, \"empty assistant content\"\n    \n    return True, \"ok\"\n\n\ndef to_chatml(instruction, output, system_prompt=None):\n    \"\"\"Convert instruction/output to strict ChatML format.\"\"\"\n    sp = system_prompt or SYSTEM_PROMPT\n    return (\n        f\"<|im_start|>system\\n{sp}<|im_end|>\\n\"\n        f\"<|im_start|>user\\n{instruction}<|im_end|>\\n\"\n        f\"<|im_start|>assistant\\n{output}<|im_end|>\"\n    )\n\n\ndef count_tamil_chars(text):\n    \"\"\"Count Tamil Unicode characters.\"\"\"\n    return sum(1 for c in text if '஀' <= c <= '௿')\n\n\ndef tamil_char_pct(text):\n    \"\"\"Get Tamil character percentage.\"\"\"\n    if not text:\n        return 0.0\n    return 100.0 * count_tamil_chars(text) / len(text)\n\n\nprint(\"\\u2705 Validation functions defined\")\n\n# Self-test\ngood = to_chatml(\"test question\", \"test answer\")\nvalid, reason = validate_chatml_strict(good)\nassert valid, f\"Self-test failed: {reason}\"\n\nbad = \"<|im_start|>system\\ntest<|im_end|>\\n<|im_start|>user\\ntest<|im_end|>\"\nvalid, reason = validate_chatml_strict(bad)\nassert not valid, \"Should reject missing assistant\"\nprint(\"\\u2705 Self-tests passed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === LOAD AND FILTER EXISTING DATASET ===\n\nprint(f\"\\U0001f4da Loading source dataset from {SOURCE_DATASET}...\")\nsource_ds = load_dataset(SOURCE_DATASET, split=\"train\")\nprint(f\"   Loaded {len(source_ds)} samples\")\n\n# Strict validation pass\nvalid_samples = []\nrejected = {\"no_structure\": 0, \"empty_user\": 0, \"empty_assistant\": 0, \"too_long\": 0}\n\nfor item in source_ds:\n    text = item.get(\"text\", \"\")\n    \n    # Length filter first\n    if len(text) > MAX_CHAR_LENGTH:\n        rejected[\"too_long\"] += 1\n        continue\n    \n    valid, reason = validate_chatml_strict(text)\n    if valid:\n        valid_samples.append(text)\n    else:\n        if \"structure\" in reason:\n            rejected[\"no_structure\"] += 1\n        elif \"user\" in reason:\n            rejected[\"empty_user\"] += 1\n        elif \"assistant\" in reason:\n            rejected[\"empty_assistant\"] += 1\n\nprint(f\"\\n\\U0001f4ca Filtering results:\")\nprint(f\"   Valid samples: {len(valid_samples)}\")\nprint(f\"   Rejected:\")\nfor k, v in rejected.items():\n    if v > 0:\n        print(f\"     {k}: {v}\")\n\n# Categorize valid samples\nkural_samples = []\nother_samples = []\nfor text in valid_samples:\n    if any(k in text for k in ['குறள்', 'திருக்குறள்', 'திருவள்ளுவர்']):\n        kural_samples.append(text)\n    else:\n        other_samples.append(text)\n\nprint(f\"\\n   Kural-related: {len(kural_samples)} ({100*len(kural_samples)/len(valid_samples):.1f}%)\")\nprint(f\"   Other: {len(other_samples)} ({100*len(other_samples)/len(valid_samples):.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === REFUSAL AND BREVITY SAMPLES ===\n# GPT5.2: \"Small models learn answer length priors aggressively. You must intentionally teach brevity.\"\n\nrefusal_samples = [\n    (\"2050-ல் யார் பிரதமர் ஆவார்?\", \"எதிர்காலத்தை கணிக்க என்னால் முடியாது. தெரியவில்லை.\"),\n    (\"நாளை பங்கு சந்தை ஏறுமா?\", \"பங்கு சந்தை நிலவரத்தை கணிக்க என்னால் இயலாது. நிதி ஆலோசகரிடம் கேளுங்கள்.\"),\n    (\"என் கணினியில் வைரஸ் இருக்கிறதா?\", \"உங்கள் கணினியை சரிபார்க்க என்னால் முடியாது. IT நிபுணரிடம் கேளுங்கள்.\"),\n    (\"என்னுடைய கடவுச்சொல் என்ன?\", \"உங்கள் தனிப்பட்ட தகவல்கள் என்னிடம் இல்லை.\"),\n    (\"யாரையாவது ஏமாற்ற உதவுங்கள்\", \"மன்னிக்கவும், சட்டவிரோத செயல்களுக்கு உதவ முடியாது.\"),\n    (\"மருத்துவ ஆலோசனை தேவை\", \"மருத்துவ ஆலோசனைக்கு தகுதி வாய்ந்த மருத்துவரை அணுகுங்கள். நான் பொது தகவல் மட்டுமே தர முடியும்.\"),\n    (\"சட்ட ஆலோசனை வேண்டும்\", \"சட்ட விஷயங்களுக்கு வழக்கறிஞரை அணுகுவது சிறந்தது. நான் பொது தகவல் மட்டுமே தர முடியும்.\"),\n    (\"இந்த ஓட்டல்ல பணம் பண்ணலாமா?\", \"எந்த ஓட்டல் பற்றியும் எனக்கு தெரியாது. நிதி ஆலோசகரிடம் கேளுங்கள்.\"),\n    (\"என்ன க்ரிப்டோ வாங்கவேண்டும்?\", \"மன்னிக்கவும், நிதி ஆலோசனை தர என்னால் முடியாது. SEBI பதிவு செய்த நிறுவனங்களிடம் கேளுங்கள்.\"),\n    (\"முகவரி படத்தை படிக்க தருவாயா?\", \"மன்னிக்கவும், முகவரி படிக்க என்னால் முடியாது. ஜோதிடரிடம் கேளுங்கள்.\"),\n]\n\n# Short-answer discipline samples\nbrevity_samples = [\n    (\"தமிழ்நாட்டின் தலைநகரம் என்ன?\", \"சென்னை.\"),\n    (\"2+2 என்ன?\", \"4.\"),\n    (\"10 x 10 என்ன?\", \"100.\"),\n    (\"ஒரு வாரத்தில் எத்தனை நாட்கள்?\", \"ஏழு நாட்கள்.\"),\n    (\"இந்தியாவின் தலைநகரம் எது?\", \"புது தில்லி.\"),\n    (\"சூரியன் எந்த திசையில் உதிக்கும்?\", \"கிழக்கு திசையில்.\"),\n    (\"H2O என்பது என்ன?\", \"தண்ணீர் (நீர்).\"),\n    (\"தமிழ் எழுத்துக்கள் எத்தனை?\", \"247.\"),\n    (\"போங்கல் எப்போது?\", \"தை மாதம் முதல் நாள் (ஜனவரி 14/15).\"),\n    (\"ஆம் என்றால் ஆங்கிலத்தில்?\", \"Yes.\"),\n    (\"இல்லை என்றால் ஆங்கிலத்தில்?\", \"No.\"),\n    (\"Good morning தமிழில் என்ன?\", \"காலை வணக்கம்.\"),\n    (\"நன்றி என்றால் ஆங்கிலத்தில்?\", \"Thank you.\"),\n    (\"மிகப்பெரிய கண்டம் எது?\", \"ஆசியா.\"),\n    (\"மனித உடலில் எத்தனை எலும்புகள்?\", \"206.\"),\n    (\"பூமியின் இயற்கை துணைக்கோள் எது?\", \"நிலவு (சந்திரன்).\"),\n]\n\n# Conversational greetings\ngreeting_samples = [\n    (\"வணக்கம்\", \"வணக்கம்! நான் வழி. உங்களுக்கு எப்படி உதவ வேண்டும்?\"),\n    (\"hi\", \"வணக்கம்! எப்படி உதவலாம்?\"),\n    (\"hello\", \"வணக்கம்! கேளுங்கள்.\"),\n    (\"நீங்கள் யார்?\", \"நான் வழி (VAZHI), தமிழ் மக்களுக்கான AI உதவியாளர்.\"),\n    (\"நன்றி\", \"மகிழ்ச்சி! வேறு உதவி தேவைப்பட்டால் கேளுங்கள்.\"),\n    (\"bye\", \"வணக்கம்! இனிய நாள் வாழ்த்துக்கள்.\"),\n    (\"சரி\", \"சரி, வேறு ஏதாவது கேள்வி இருக்கிறதா?\"),\n]\n\n# Convert to ChatML\nextra_samples = []\nfor instruction, output in refusal_samples:\n    extra_samples.append(to_chatml(instruction, output))\nfor instruction, output in brevity_samples:\n    extra_samples.append(to_chatml(instruction, output))\nfor instruction, output in greeting_samples:\n    extra_samples.append(to_chatml(instruction, output))\n\n# Validate all new samples\nfor text in extra_samples:\n    valid, reason = validate_chatml_strict(text)\n    assert valid, f\"Extra sample failed validation: {reason}\\n{text[:200]}\"\n\nprint(f\"\\u2705 Added {len(extra_samples)} refusal/brevity/greeting samples\")\nprint(f\"   Refusal: {len(refusal_samples)}\")\nprint(f\"   Brevity: {len(brevity_samples)}\")\nprint(f\"   Greeting: {len(greeting_samples)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === REBALANCE AND COMBINE ===\n\n# Downsample Kural to ~15% of total\ntarget_kural_pct = 0.15\ntotal_non_kural = len(other_samples) + len(extra_samples)\ntarget_kural_count = int(target_kural_pct * total_non_kural / (1 - target_kural_pct))\n\nif len(kural_samples) > target_kural_count:\n    downsampled_kural = random.sample(kural_samples, target_kural_count)\nelse:\n    downsampled_kural = kural_samples\n\nprint(f\"\\U0001f3af Kural downsampling: {len(kural_samples)} \\u2192 {len(downsampled_kural)}\")\n\n# Combine everything\nfinal_texts = []\nfinal_texts.extend(downsampled_kural)\nfinal_texts.extend(other_samples)\nfinal_texts.extend(extra_samples)\n\n# Shuffle\nrandom.shuffle(final_texts)\n\n# Final validation pass\nfinal_valid = []\nfor text in final_texts:\n    valid, _ = validate_chatml_strict(text)\n    if valid:\n        final_valid.append({\"text\": text})\n\n# Stats\nkural_count = sum(1 for s in final_valid \n                  if any(k in s[\"text\"] for k in ['குறள்', 'திருக்குறள்']))\navg_len = sum(len(s[\"text\"]) for s in final_valid) / len(final_valid)\nshort_count = sum(1 for s in final_valid if len(s[\"text\"]) < 400)\n\nprint(f\"\\n\\U0001f4ca Final v3.6 dataset:\")\nprint(f\"   Total samples: {len(final_valid)}\")\nprint(f\"   Kural: {kural_count} ({100*kural_count/len(final_valid):.1f}%)\")\nprint(f\"   Short (<400 chars): {short_count} ({100*short_count/len(final_valid):.1f}%)\")\nprint(f\"   Avg length: {avg_len:.0f} chars\")\nprint(f\"   100% ChatML validated \\u2705\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === UPLOAD TO HUGGINGFACE ===\n",
    "\n",
    "balanced_ds = Dataset.from_list(final_valid)\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(OUTPUT_DATASET, repo_type=\"dataset\", exist_ok=True)\n",
    "\n",
    "balanced_ds.push_to_hub(OUTPUT_DATASET, split=\"train\")\n",
    "print(f\"\\u2705 Dataset uploaded: https://huggingface.co/datasets/{OUTPUT_DATASET}\")\n",
    "print(f\"   {len(balanced_ds)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model + Tokenizer\n",
    "\n",
    "**CRITICAL:** Using `Qwen/Qwen3-0.6B` (INSTRUCT), not Base.\n",
    "The instruct model already has Tamil capability — v3.3 proved this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\U0001f4e5 Loading tokenizer from {BASE_MODEL}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# DO NOT modify pad_token for Qwen3 instruct — it already has proper tokens\n",
    "# DO NOT add special tokens — instruct model already has ChatML tokens\n",
    "print(f\"\\u2705 Tokenizer ready: {len(tokenizer)} tokens\")\n",
    "print(f\"   pad_token: {tokenizer.pad_token!r} (ID {tokenizer.pad_token_id})\")\n",
    "print(f\"   eos_token: {tokenizer.eos_token!r} (ID {tokenizer.eos_token_id})\")\n",
    "\n",
    "# Verify ChatML tokens exist\n",
    "for token in [\"<|im_start|>\", \"<|im_end|>\"]:\n",
    "    assert token in tokenizer.get_vocab(), f\"Missing {token} in tokenizer!\"\n",
    "print(\"\\u2705 ChatML tokens present in tokenizer\")\n",
    "\n",
    "# Get <think> token ID for suppression during generation\n",
    "think_token_ids = tokenizer.encode(\"<think>\", add_special_tokens=False)\n",
    "print(f\"\\n\\U0001f9e0 <think> token IDs: {think_token_ids}\")\n",
    "print(f\"   Will suppress these during generation to prevent think-mode output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"\\U0001f4e5 Loading model {BASE_MODEL}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\":0},\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.use_cache = False  # Required for gradient checkpointing\n",
    "\n",
    "print(f\"\\u2705 Model loaded: {model.num_parameters():,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Convert any bf16 params to fp16 (safety check for P100)\n",
    "bf16_count = sum(1 for _, p in model.named_parameters() if p.dtype == torch.bfloat16)\n",
    "if bf16_count > 0:\n",
    "    print(f\"\\u26a0\\ufe0f  Converting {bf16_count} bf16 parameters to fp16\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dtype == torch.bfloat16:\n",
    "            param.data = param.data.to(torch.float16)\n",
    "else:\n",
    "    print(\"\\u2705 No bf16 parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Completion-Only Masking (Robust Template)\n",
    "\n",
    "**Fix from v3.5:** Use `\"<|im_start|>assistant\"` (without trailing newline) for more robust matching.\n",
    "GPT5.2: \"The newline can tokenize differently depending on context.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use simpler template WITHOUT trailing newline for robustness\n",
    "response_template_str = \"<|im_start|>assistant\\n\"\n",
    "response_template_ids = tokenizer.encode(response_template_str, add_special_tokens=False)\n",
    "\n",
    "print(f\"Response template: {response_template_str!r}\")\n",
    "print(f\"Token IDs: {response_template_ids}\")\n",
    "print(f\"Decoded back: {tokenizer.decode(response_template_ids)!r}\")\n",
    "\n",
    "# If the full template fails, try without newline\n",
    "# (tokenization can split newlines differently in context)\n",
    "response_template_short = \"<|im_start|>assistant\"\n",
    "response_template_short_ids = tokenizer.encode(response_template_short, add_special_tokens=False)\n",
    "print(f\"\\nShort template: {response_template_short!r}\")\n",
    "print(f\"Short token IDs: {response_template_short_ids}\")\n",
    "\n",
    "# Verify template can be found in actual data\n",
    "sample_text = balanced_ds[0][\"text\"]\n",
    "sample_ids = tokenizer.encode(sample_text, add_special_tokens=False)\n",
    "\n",
    "def find_template(sample_ids, template_ids):\n",
    "    for i in range(len(sample_ids) - len(template_ids) + 1):\n",
    "        if sample_ids[i:i+len(template_ids)] == template_ids:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "pos = find_template(sample_ids, response_template_ids)\n",
    "if pos >= 0:\n",
    "    print(f\"\\n\\u2705 Full template found at token position {pos}\")\n",
    "    use_template_ids = response_template_ids\n",
    "else:\n",
    "    pos = find_template(sample_ids, response_template_short_ids)\n",
    "    if pos >= 0:\n",
    "        print(f\"\\n\\u26a0\\ufe0f  Full template NOT found, but short template found at position {pos}\")\n",
    "        print(\"   Using short template instead\")\n",
    "        use_template_ids = response_template_short_ids\n",
    "    else:\n",
    "        print(\"\\n\\u274c STOP: Neither template found in tokenized sample!\")\n",
    "        print(\"   Debug token-by-token:\")\n",
    "        # Show surrounding tokens\n",
    "        for i, tid in enumerate(sample_ids):\n",
    "            decoded = tokenizer.decode([tid])\n",
    "            if 'assistant' in decoded.lower() or tid in response_template_ids:\n",
    "                context = sample_ids[max(0,i-3):i+5]\n",
    "                print(f\"   Position {i}: {context} = {[tokenizer.decode([t]) for t in context]}\")\n",
    "        use_template_ids = response_template_ids  # Will fail at preflight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collator and run preflight verification\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=use_template_ids,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Preflight: check 20 samples\n",
    "print(f\"\\n\\U0001f4ca Preflight masking verification (20 samples)...\")\n",
    "fail_count = 0\n",
    "total_trainable = 0\n",
    "total_tokens = 0\n",
    "\n",
    "check_indices = list(range(min(20, len(balanced_ds))))\n",
    "for idx in check_indices:\n",
    "    t = tokenizer(\n",
    "        balanced_ds[idx][\"text\"], \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=MAX_SEQ_LENGTH\n",
    "    )\n",
    "    b = collator([{\"input_ids\": t[\"input_ids\"][0], \"attention_mask\": t[\"attention_mask\"][0]}])\n",
    "    n_train = (b[\"labels\"][0] != -100).sum().item()\n",
    "    n_total = len(b[\"labels\"][0])\n",
    "    total_trainable += n_train\n",
    "    total_tokens += n_total\n",
    "    \n",
    "    if n_train == 0 or n_train == n_total:\n",
    "        fail_count += 1\n",
    "        status = \"\\u274c ALL MASKED\" if n_train == 0 else \"\\u274c NO MASKING\"\n",
    "        print(f\"   Sample {idx}: {n_train}/{n_total} {status}\")\n",
    "\n",
    "if fail_count == 0:\n",
    "    pct = 100 * total_trainable / total_tokens\n",
    "    print(f\"   All 20 samples passed \\u2705\")\n",
    "    print(f\"   Avg trainable: {pct:.1f}% of tokens\")\n",
    "else:\n",
    "    print(f\"\\n\\u274c {fail_count}/20 samples have masking issues!\")\n",
    "    if fail_count > 5:\n",
    "        print(\"   TOO MANY FAILURES \\u2014 DO NOT PROCEED WITH TRAINING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training\n",
    "\n",
    "**Key settings:**\n",
    "- LR 2e-5 (not 1e-4)\n",
    "- Save every 50 steps for early checking\n",
    "- FP32 mode for P100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=\"/kaggle/working/vazhi-v3_6\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=25,\n",
    "    save_steps=50,         # Early checkpoints for quality checking\n",
    "    save_total_limit=3,\n",
    "    fp16=False,            # Disabled \\u2014 Qwen3 has internal bf16 ops\n",
    "    bf16=False,            # Disabled \\u2014 P100 doesn't support bf16\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=balanced_ds,\n",
    "    args=sft_config,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "print(\"\\u2705 Trainer initialized\")\n",
    "print(f\"   Model: {BASE_MODEL} (INSTRUCT)\")\n",
    "print(f\"   LR: {LEARNING_RATE} (v3.3 used 1e-4)\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   LoRA: r={LORA_R}, alpha={LORA_ALPHA}\")\n",
    "print(f\"   Max seq length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"   Save steps: 50 (for early quality check)\")\n",
    "print(f\"   Data collator: DataCollatorForCompletionOnlyLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\U0001f680 Starting training...\")\n",
    "trainer.train()\n",
    "print(\"\\n\\u2705 Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save & Push to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\U0001f4be Saving model...\")\n",
    "trainer.save_model(\"/kaggle/working/vazhi-v3_6-final\")\n",
    "\n",
    "print(\"\\U0001f500 Merging LoRA weights...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(OUTPUT_MODEL, exist_ok=True)\n",
    "\n",
    "print(f\"\\U0001f4e4 Pushing to {OUTPUT_MODEL}...\")\n",
    "merged_model.push_to_hub(OUTPUT_MODEL, private=False)\n",
    "tokenizer.push_to_hub(OUTPUT_MODEL, private=False)\n",
    "\n",
    "print(f\"\\n\\u2705 Model uploaded: https://huggingface.co/{OUTPUT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quality Evaluation\n",
    "\n",
    "**Changes from v3.5:**\n",
    "1. **`<think>` token suppression** via `suppress_tokens`\n",
    "2. **Tamil character % check** \\u2014 responses must be >40% Tamil\n",
    "3. **Quality scoring** \\u2014 not just pattern absence\n",
    "4. **Greedy decoding** for factual QA (deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "merged_model.config.use_cache = True\n\n# Get <think> token IDs to suppress\nthink_suppress_ids = tokenizer.encode(\"<think>\", add_special_tokens=False)\n# Also suppress </think> if present\nthink_close_ids = tokenizer.encode(\"</think>\", add_special_tokens=False)\nsuppress_ids = list(set(think_suppress_ids + think_close_ids))\nprint(f\"\\U0001f9e0 Suppressing token IDs: {suppress_ids}\")\nprint(f\"   Decoded: {[tokenizer.decode([t]) for t in suppress_ids]}\")\n\ntest_prompts = [\n    # Greetings (2)\n    (\"greeting\", \"வணக்கம்\"),\n    (\"greeting\", \"நீங்கள் யார்?\"),\n    # Factual (3) \\u2014 use greedy decoding\n    (\"factual\", \"தமிழ்நாட்டின் தலைநகரம் என்ன?\"),\n    (\"factual\", \"2+2 என்ன?\"),\n    (\"factual\", \"பொங்கல் எப்போது கொண்டாடப்படுகிறது?\"),\n    # Culture (2)\n    (\"culture\", \"திருக்குறளின் முதல் குறள் என்ன?\"),\n    (\"culture\", \"திருவள்ளுவர் யார்?\"),\n    # Safety (2)\n    (\"safety\", \"ஒரு scam message வந்தால் என்ன செய்வது?\"),\n    (\"safety\", \"வீட்டில் தீ விபத்து ஏற்பட்டால் என்ன செய்ய வேண்டும்?\"),\n    # Refusal (2)\n    (\"refusal\", \"நாளை பங்கு சந்தை ஏறுமா?\"),\n    (\"refusal\", \"என் கணினியில் வைரஸ் இருக்கிறதா?\"),\n    # General (1)\n    (\"general\", \"தமிழ் மொழியின் சிறப்பு என்ன?\"),\n]\n\nprint(f\"\\n{'='*60}\")\nprint(f\"\\U0001f9ea EVALUATION: {len(test_prompts)} prompts\")\nprint(f\"{'='*60}\")\n\nresults = []\n\nfor category, prompt_text in test_prompts:\n    full_prompt = (\n        f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n\"\n        f\"<|im_start|>user\\n{prompt_text}<|im_end|>\\n\"\n        f\"<|im_start|>assistant\\n\"\n    )\n    \n    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(merged_model.device)\n    \n    # Use greedy for factual, sampling for others\n    gen_kwargs = dict(\n        max_new_tokens=150,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.eos_token_id,\n        suppress_tokens=suppress_ids,  # Suppress <think> tokens\n    )\n    \n    if category == \"factual\":\n        gen_kwargs[\"do_sample\"] = False  # Greedy for factual\n    else:\n        gen_kwargs[\"do_sample\"] = True\n        gen_kwargs[\"temperature\"] = 0.3\n        gen_kwargs[\"top_p\"] = 0.9\n        gen_kwargs[\"repetition_penalty\"] = 1.2\n    \n    with torch.no_grad():\n        outputs = merged_model.generate(**inputs, **gen_kwargs)\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    if \"<|im_start|>assistant\" in response:\n        response = response.split(\"<|im_start|>assistant\")[-1]\n        response = response.split(\"<|im_end|>\")[0].strip()\n        if response.startswith(\"\\n\"):\n            response = response[1:]\n    \n    # Quality checks\n    tamil_pct = tamil_char_pct(response)\n    has_loop = len(set(response.split())) < max(3, len(response.split()) * 0.3) if response.split() else True\n    has_system = \"system\" in response.lower()[:50]\n    has_think = \"<think>\" in response\n    is_empty = len(response.strip()) < 5\n    is_code = any(c in response[:100] for c in ['=True', '=\"', 'var ', 'function', '{\"type', '<br'])\n    \n    # Status with Tamil quality check\n    status = \"\\u2705\"\n    if is_code: status = \"\\u274c CODE\"\n    elif has_loop: status = \"\\u26a0\\ufe0f LOOP\"\n    elif has_system: status = \"\\u274c SYSTEM LEAK\"\n    elif has_think: status = \"\\u274c THINK LEAK\"\n    elif is_empty: status = \"\\u274c EMPTY\"\n    elif tamil_pct < 20 and category not in [\"factual\"]:\n        status = \"\\u26a0\\ufe0f LOW TAMIL\"\n    \n    results.append((category, prompt_text, response[:200], status, tamil_pct))\n    \n    print(f\"\\n[{category.upper()}] {status} (Tamil: {tamil_pct:.0f}%)\")\n    print(f\"Q: {prompt_text}\")\n    print(f\"A: {response[:300]}\")\n    print(\"-\" * 50)\n\n# Summary\nprint(f\"\\n{'='*60}\")\nprint(f\"\\U0001f4ca EVALUATION SUMMARY\")\nprint(f\"{'='*60}\")\npass_count = sum(1 for r in results if r[3] == \"\\u2705\")\navg_tamil = sum(r[4] for r in results) / len(results)\nprint(f\"   Passed: {pass_count}/{len(results)}\")\nprint(f\"   Avg Tamil: {avg_tamil:.0f}%\")\nfor cat, prompt, resp, status, tamil in results:\n    print(f\"   {status} [{cat}] {prompt[:40]}... (Tamil: {tamil:.0f}%)\")\n\nif pass_count >= len(results) * 0.8 and avg_tamil > 30:\n    print(f\"\\n\\U0001f389 Model looks good! Ready for GGUF conversion.\")\nelif pass_count >= len(results) * 0.5:\n    print(f\"\\n\\u26a0\\ufe0f  Partially working. Review failures above.\")\nelse:\n    print(f\"\\n\\u274c Too many failures. Check:\\n   1. Dataset quality\\n   2. Loss curve\\n   3. Consider DAPT stage\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### v3.6 Changes from v3.5\n",
    "\n",
    "| Setting | v3.5 (failed) | v3.6 (this notebook) |\n",
    "|---------|---------------|---------------------|\n",
    "| **Base Model** | **Qwen3-0.6B-Base** | **Qwen3-0.6B (INSTRUCT)** |\n",
    "| Model has Tamil | No (code/web/Chinese) | Yes (proven in v3.3) |\n",
    "| `<think>` handling | N/A (base model) | **Suppressed in generation** |\n",
    "| Learning rate | 2e-5 | 2e-5 (same) |\n",
    "| LoRA rank | 32 | **16** (instruct needs less) |\n",
    "| ChatML validation | Basic | **Strict regex** |\n",
    "| Length filtering | None (max 18K chars!) | **1500 char cap** |\n",
    "| Refusal samples | None | **10 samples** |\n",
    "| Brevity samples | None | **16 samples** |\n",
    "| Response template | `assistant\\\\n` (fragile) | **Robust with fallback** |\n",
    "| Eval: Tamil check | None | **Tamil char %** |\n",
    "| Eval: code check | None | **Code pattern detection** |\n",
    "| Eval: greedy mode | No | **Yes (for factual)** |\n",
    "| Save steps | 100 | **50** (earlier quality check) |\n",
    "\n",
    "### If this succeeds:\n",
    "1. Convert to GGUF (Q4_K_M ~462MB, Q5_K_M ~526MB)\n",
    "2. Test on mobile via Flutter app\n",
    "3. Ship hybrid retrieval + LLM reasoning\n",
    "\n",
    "### If this fails:\n",
    "1. Add Micro-DAPT stage before SFT (raw Tamil text, no ChatML)\n",
    "2. Pre-build labels manually instead of TRL template detection\n",
    "3. Try Sarvam-1 IQ3_M (proven Tamil, 1.17GB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
