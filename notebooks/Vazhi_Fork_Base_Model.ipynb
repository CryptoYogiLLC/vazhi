{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fork Gemma-2B Tamil Base Model\n",
    "\n",
    "**Purpose:** Copy the base model to our HuggingFace account so we're not dependent on the original author's alpha release.\n",
    "\n",
    "**Run this once, then use the forked model for all future training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q huggingface_hub transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download, HfApi, login\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "SOURCE_MODEL = \"abhinand/gemma-2b-it-tamil-v0.1-alpha\"\n",
    "TARGET_MODEL = \"CryptoYogi/gemma-2b-tamil-base\"\n",
    "LOCAL_DIR = \"./gemma-2b-tamil-base\"\n",
    "\n",
    "print(f\"Source: {SOURCE_MODEL}\")\n",
    "print(f\"Target: {TARGET_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Download the original model\nprint(\"=\" * 60)\nprint(\"STEP 1: Downloading original model (~5GB)...\")\nprint(\"=\" * 60)\n\nlocal_path = snapshot_download(\n    repo_id=SOURCE_MODEL,\n    local_dir=LOCAL_DIR  # Removed deprecated local_dir_use_symlinks parameter\n)\n\nprint(f\"\\n✅ Downloaded to: {local_path}\")\n\n# Show what was downloaded\nprint(\"\\nFiles downloaded:\")\nfor f in os.listdir(local_path):\n    size = os.path.getsize(os.path.join(local_path, f)) / 1e6\n    print(f\"  {f}: {size:.1f} MB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download the original model\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Downloading original model (~5GB)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "local_path = snapshot_download(\n",
    "    repo_id=SOURCE_MODEL,\n",
    "    local_dir=LOCAL_DIR,\n",
    "    local_dir_use_symlinks=False  # Actually download, don't symlink\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Downloaded to: {local_path}\")\n",
    "\n",
    "# Show what was downloaded\n",
    "print(\"\\nFiles downloaded:\")\n",
    "for f in os.listdir(local_path):\n",
    "    size = os.path.getsize(os.path.join(local_path, f)) / 1e6\n",
    "    print(f\"  {f}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create repo and upload to your account\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2: Uploading to your HuggingFace account...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Create the repository\n",
    "api.create_repo(\n",
    "    repo_id=TARGET_MODEL,\n",
    "    exist_ok=True,\n",
    "    private=False  # Make it public so Kaggle can access\n",
    ")\n",
    "print(f\"Repository created: {TARGET_MODEL}\")\n",
    "\n",
    "# Upload all files\n",
    "print(\"\\nUploading files (this may take a few minutes)...\")\n",
    "api.upload_folder(\n",
    "    folder_path=local_path,\n",
    "    repo_id=TARGET_MODEL,\n",
    "    commit_message=\"Fork of abhinand/gemma-2b-it-tamil-v0.1-alpha for VAZHI project\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Model uploaded successfully!\")\n",
    "print(f\"\\nView at: https://huggingface.co/{TARGET_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Verify the upload by loading the model\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 3: Verifying upload...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load tokenizer from our fork\n",
    "tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
    "print(f\"✅ Tokenizer loaded from {TARGET_MODEL}\")\n",
    "print(f\"   Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"   PAD token: {tokenizer.pad_token}\")\n",
    "print(f\"   EOS token: {tokenizer.eos_token}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ MODEL FORK COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nYou can now use: {TARGET_MODEL}\")\n",
    "print(\"\\nIn training notebook, set:\")\n",
    "print('  USE_FORKED = True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - remove local copy to save space\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(LOCAL_DIR):\n",
    "    shutil.rmtree(LOCAL_DIR)\n",
    "    print(f\"Cleaned up local copy: {LOCAL_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}