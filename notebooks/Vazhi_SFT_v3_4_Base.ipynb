{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# VAZHI SFT v3.4 - Using Base Model\n",
    "\n",
    "**Critical Fix:** Use `Qwen3-0.6B-Base` (non-instruct) instead of `Qwen3-0.6B` (instruct)\n",
    "\n",
    "**Why v3.3 failed:**\n",
    "- Qwen3-0.6B is instruction-tuned with `<think>` reasoning tokens\n",
    "- Our ChatML format conflicted with its native format\n",
    "- Learning rate 1e-4 was too aggressive, causing catastrophic forgetting\n",
    "\n",
    "**v3.4 Fixes:**\n",
    "1. **Use Qwen3-0.6B-Base** - clean slate, no instruction tuning\n",
    "2. **Lower LR: 2e-5** - safer for fine-tuning\n",
    "3. **3 epochs** - more training to learn Tamil properly\n",
    "4. FP32 mode for P100 compatibility\n",
    "\n",
    "**Target:** Kaggle P100 (16GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "**IMPORTANT:** After running this cell, **RESTART the session** (Runtime ‚Üí Restart session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -U \\\n",
    "  \"transformers>=4.51.0\" \\\n",
    "  \"accelerate>=0.34.2\" \\\n",
    "  \"peft>=0.12.0\" \\\n",
    "  \"trl>=0.12.0\" \\\n",
    "  \"bitsandbytes>=0.43.3\" \\\n",
    "  \"datasets>=2.21.0\" \\\n",
    "  \"huggingface_hub>=0.24.7\"\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")\n",
    "print(\"‚ö†Ô∏è RESTART THE SESSION NOW (Runtime ‚Üí Restart session)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force single GPU BEFORE importing torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from huggingface_hub import login, HfApi, dataset_info\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Config\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Repos - USING BASE MODEL!\n",
    "BALANCED_DATASET = \"CryptoYogi/vazhi-tamil-sft-v3_3\"  # Reuse existing dataset\n",
    "BASE_MODEL = \"Qwen/Qwen3-0.6B-Base\"  # BASE model, not instruct!\n",
    "OUTPUT_MODEL = \"CryptoYogi/vazhi-qwen3-v3_4\"\n",
    "\n",
    "# System prompt\n",
    "SYSTEM_PROMPT = \"‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç VAZHI (‡Æµ‡Æ¥‡Æø), ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æ© AI ‡Æâ‡Æ§‡Æµ‡Æø‡ÆØ‡Ææ‡Æ≥‡Æ∞‡Øç. ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Æø‡Æ≤‡Øç ‡Æ§‡ØÜ‡Æ≥‡Æø‡Æµ‡Ææ‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç ‡Æâ‡Æ§‡Æµ‡Æø‡ÆØ‡Ææ‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Æ§‡Æø‡Æ≤‡Æ≥‡Æø‡ÆØ‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç. ‡Æ§‡ØÜ‡Æ∞‡Æø‡ÆØ‡Ææ‡Æµ‡Æø‡Æü‡Øç‡Æü‡Ææ‡Æ≤‡Øç \\\"‡Æ§‡ØÜ‡Æ∞‡Æø‡ÆØ‡Æµ‡Æø‡Æ≤‡Øç‡Æ≤‡Øà\\\" ‡Æé‡Æ©‡Øç‡Æ±‡ØÅ ‡Æö‡Øä‡Æ≤‡Øç‡Æ≤‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç.\"\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"\\nüî• KEY FIX: Using BASE model (not instruct)\")\n",
    "print(f\"   Base model: {BASE_MODEL}\")\n",
    "print(f\"   Dataset: {BALANCED_DATASET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "print(\"‚úÖ Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Load Dataset (Reusing v3.3 dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìö Loading balanced dataset...\")\n",
    "balanced_ds = load_dataset(BALANCED_DATASET, split=\"train\")\n",
    "print(f\"‚úÖ Loaded {len(balanced_ds)} samples\")\n",
    "\n",
    "# Verify ChatML format\n",
    "sample = balanced_ds[0]['text'][:300]\n",
    "print(f\"\\nüìù Sample: {sample}...\")\n",
    "if \"<|im_start|>\" in sample:\n",
    "    print(\"‚úÖ ChatML format verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Load BASE Model with 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüì• Loading BASE model and tokenizer...\")\n",
    "print(f\"   Model: {BASE_MODEL}\")\n",
    "\n",
    "# Tokenizer from BASE model\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Ensure ChatML special tokens exist\n",
    "special_tokens = [\"<|im_start|>\", \"<|im_end|>\"]\n",
    "tokens_to_add = [t for t in special_tokens if t not in tokenizer.get_vocab()]\n",
    "if tokens_to_add:\n",
    "    print(f\"   Adding special tokens: {tokens_to_add}\")\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': tokens_to_add})\n",
    "\n",
    "# Set pad token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"   Set pad_token = eos_token\")\n",
    "\n",
    "print(f\"‚úÖ Tokenizer ready: {len(tokenizer)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization - use float16 compute dtype\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model - MUST specify torch_dtype to avoid bf16 default\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\":0},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Resize embeddings if we added special tokens\n",
    "if len(tokenizer) > model.config.vocab_size:\n",
    "    print(f\"   Resizing embeddings: {model.config.vocab_size} ‚Üí {len(tokenizer)}\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model.num_parameters():,} params\")\n",
    "print(f\"   torch_dtype: float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config - slightly higher rank for base model\n",
    "lora_config = LoraConfig(\n",
    "    r=32,  # Increased from 16 for better learning on base model\n",
    "    lora_alpha=64,  # 2x r\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Verify no bf16 parameters\n",
    "bf16_count = sum(1 for _, p in model.named_parameters() if p.dtype == torch.bfloat16)\n",
    "if bf16_count > 0:\n",
    "    print(f\"‚ö†Ô∏è Found {bf16_count} bf16 parameters - converting to fp16\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dtype == torch.bfloat16:\n",
    "            param.data = param.data.to(torch.float16)\n",
    "else:\n",
    "    print(\"‚úÖ No bf16 parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Training (Lower LR, More Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP32 training with SAFER learning rate\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"/kaggle/working/vazhi-v3_4\",\n",
    "    num_train_epochs=3,  # Increased from 2\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-5,  # MUCH lower: 2e-5 vs 1e-4 (5x reduction)\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=25,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,  # DISABLED - Qwen3 has internal bf16\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=512,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=balanced_ds,\n",
    "    args=sft_config,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized (FP32 mode)\")\n",
    "print(f\"   Epochs: 3 (was 2)\")\n",
    "print(f\"   Learning rate: 2e-5 (was 1e-4)\")\n",
    "print(f\"   LoRA rank: 32 (was 16)\")\n",
    "print(f\"   Batch size: 1 x 16 = 16 effective\")\n",
    "print(f\"   Mode: FP32 (P100 compatible)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "trainer.train()\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Save and Push to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Saving model...\")\n",
    "trainer.save_model(\"/kaggle/working/vazhi-v3_4-final\")\n",
    "\n",
    "print(\"üîÄ Merging LoRA weights...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Push to HuggingFace\n",
    "api = HfApi()\n",
    "api.create_repo(OUTPUT_MODEL, exist_ok=True)\n",
    "\n",
    "print(f\"üì§ Pushing to {OUTPUT_MODEL}...\")\n",
    "merged_model.push_to_hub(OUTPUT_MODEL, private=False)\n",
    "tokenizer.push_to_hub(OUTPUT_MODEL, private=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Model uploaded: https://huggingface.co/{OUTPUT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.config.use_cache = True\n",
    "\n",
    "test_prompts = [\n",
    "    \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç\",\n",
    "    \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ©‡Øç ‡Æ§‡Æ≤‡Øà‡Æ®‡Æï‡Æ∞‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\",\n",
    "    \"2+2 ‡Æé‡Æ©‡Øç‡Æ©?\",\n",
    "    \"‡Æ™‡Øä‡Æô‡Øç‡Æï‡Æ≤‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Øã‡Æ§‡ØÅ ‡Æï‡Øä‡Æ£‡Øç‡Æü‡Ææ‡Æü‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ?\",\n",
    "    \"‡Æ§‡Æø‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡Æ±‡Æ≥‡Æø‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Øç ‡Æï‡ØÅ‡Æ±‡Æ≥‡Øç ‡Æé‡Æ©‡Øç‡Æ©?\",\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Testing model...\\n\")\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    full_prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(merged_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = merged_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    if \"<|im_start|>assistant\" in response:\n",
    "        response = response.split(\"<|im_start|>assistant\")[-1]\n",
    "        response = response.split(\"<|im_end|>\")[0].strip()\n",
    "    \n",
    "    print(f\"Q: {prompt}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**v3.4 Key Changes from v3.3:**\n",
    "\n",
    "| Setting | v3.3 (failed) | v3.4 |\n",
    "|---------|--------------|------|\n",
    "| Base Model | Qwen3-0.6B (instruct) | Qwen3-0.6B-Base |\n",
    "| Learning Rate | 1e-4 | 2e-5 |\n",
    "| Epochs | 2 | 3 |\n",
    "| LoRA Rank | 16 | 32 |\n",
    "\n",
    "**Why v3.3 failed:**\n",
    "- Qwen3-0.6B is instruction-tuned with `<think>` reasoning\n",
    "- Our ChatML format conflicted with its native format\n",
    "- High LR caused catastrophic forgetting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
